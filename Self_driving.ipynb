{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_driving.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/07012018.zip\" -d \"/content/\" "
      ],
      "metadata": {
        "id": "ButSKOpKfMnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/data.txt\")"
      ],
      "metadata": {
        "id": "_OaW8UVYgdHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = file.readlines()"
      ],
      "metadata": {
        "id": "j9g4rshFiF_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [x.split(\" \")[1] for x in data]"
      ],
      "metadata": {
        "id": "mwecyCpTizwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [x.split(\",\")[0] for x in data]"
      ],
      "metadata": {
        "id": "O-kJ0-9Zi28o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = list(map(float,data))"
      ],
      "metadata": {
        "id": "rjKAItEEkBec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "nqs5V7f7kYDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "BwLtooV0koc5",
        "outputId": "0e72d342-05ce-4db0-fe76-b0420da11096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3db49ba490>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5TddX3n8ef7/pwfSSY/GEhIgESIaBRRNoVWrbunVERbjVZY0FppZRe7ldPdenq6tO6ylLrnFPesrnvEY9mFPYgiWKw2drEUitqlQmQQFANEhgD5QcjvTGYyM/fne//4fu/kzuU7MzeZ73fmfievxzn35Hu/3++99/NN5uY1n59fc3dERERaZea7ACIi0pkUECIiEkkBISIikRQQIiISSQEhIiKRcvNdgLicdtppvnbt2vkuhohIqjzxxBMH3L0/6tiCCYi1a9cyMDAw38UQEUkVM3t5qmNqYhIRkUgKCBERiaSAEBGRSAoIERGJpIAQEZFICggREYmkgBARkUgKCBERiaSAEEmpT941wP946BfzXQxZwBbMTGqRU83Pdx+d7yLIAqcahEhKlap1xir1+S6GLGAKCJGUKlVrjJdr810MWcAUECIpVarWGa8qICQ5CgiRFHJ3ytU6Y6pBSIIUECIpVKoGfQ+qQUiSFBAiKdQIiLGyOqklOQoIkRQqhTWHUkU1CEmOAkIkhUrh8NYxBYQkSAEhkkKNJqZq3anU1MwkyVBAiKRQqalzely1CElIogFhZpeb2TYzGzSzGyKOF83s3vD4FjNbG+5fa2ZjZvZU+PhKkuUUSZtGDQLUzCTJSWwtJjPLArcC7wZ2AY+b2WZ3f6bptGuBw+5+npldDdwCXBUee8Hd35pU+UTSrNS0xEZJy21IQpKsQVwMDLr7dncvA/cAm1rO2QTcGW7fB1xqZpZgmUQWhOYmJtUgJClJBsRqYGfT813hvshz3L0KDAErwmPrzOxJM/uhmf1q1AeY2XVmNmBmA/v374+39CIdrLmJSX0QkpRO7aTeA5zt7m8DPg3cbWZLWk9y99vcfaO7b+zv75/zQorMl0l9EFpuQxKSZEDsBs5qer4m3Bd5jpnlgD7goLuX3P0ggLs/AbwAvD7BsoqkSvMEufGq+iAkGUkGxOPAejNbZ2YF4Gpgc8s5m4Frwu0rgIfd3c2sP+zkxsxeB6wHtidYVpFUUQ1C5kJio5jcvWpm1wMPAFngDnffamY3AwPuvhm4HbjLzAaBQwQhAvAu4GYzqwB14Pfd/VBSZRVJm+aAKGnBPklIorccdff7gftb9t3YtD0OXBnxum8B30qybCJpNmkUk2oQkpBO7aQWkWk0z33QKCZJigJCJIUmz6RWJ7UkQwEhkkLlap1FxaCFWBPlJCmJ9kGISDJK1Rpd+SzVel33hJDEKCBEUqhUrVPMZajWs6pBSGIUECIpVKrWKeYz1D2rTmpJjAJCJIVKlRrFXBZ3dVJLchQQIinUaGICDXOV5CggRFKoVK1RzGUwU0BIcjTMVSSFgj6ILN159UFIchQQIilUqgRNTN15jWKS5KiJSSSFGk1M7lqLSZKjgBBJoaCTOgvAuEYxSUIUECIp1JgHYaiTWpKjgBBJoWAeRIasmQJCEqOAEEmhRhNTPmuMVWq4O2Y238WSBUajmERSxt0nJsp15bPUHSo1n+9iyQKkgBBJmXIt6JQu5oOAAC35LclQQIikTONmQcVclq588BXWkt+SBAWESMo0bjfamCgHqkFIMhQQIilTqgZh0OiDAAWEJEOjmERS4u4tOwDYP1wCYODlwxSywe94lao6qSV+qkGIpEy1HjQx5TNGNhMMba3UNZta4qeAEEmZajikNZfNHA+IqgJC4qcmJpGUadQWcs01CM2DkAQoIERSprkG0Zg7XampBiHxU0CIpMxEQGSOL61RVkBIAhQQIinT6KTOZQ0PW5aqamKSBCggRFLmeA0ig4cJoSYmSUKio5jM7HIz22Zmg2Z2Q8TxopndGx7fYmZrW46fbWYjZvbHSZZTJE2q9eNNTI1OajUxSRISCwgzywK3Au8FNgAfMbMNLaddCxx29/OALwC3tBz/PPC9pMookkbVplFMmYlRTAoIiV+SNYiLgUF33+7uZeAeYFPLOZuAO8Pt+4BLLVzU3sw+CLwIbE2wjCKpUwtrENmMkTPNg5DkJBkQq4GdTc93hfsiz3H3KjAErDCzRcB/BP58ug8ws+vMbMDMBvbv3x9bwUU62URAZDUPQpLVqTOpbwK+4O4j053k7re5+0Z339jf3z83JROZZ40+iKxpqQ1JVpKjmHYDZzU9XxPuizpnl5nlgD7gIHAJcIWZfQ5YCtTNbNzdv5RgeUVSoVZ3shnDzMiEv+JpsT5JQpIB8Tiw3szWEQTB1cBHW87ZDFwDPApcATzswbi9X22cYGY3ASMKB5FAIyAAMmEtQp3UkoTEAsLdq2Z2PfAAkAXucPetZnYzMODum4HbgbvMbBA4RBAiIjKNar0+aRZ1PquAkGQkOlHO3e8H7m/Zd2PT9jhw5QzvcVMihRNJqeYaBEA+m9E8CElEp3ZSi8gUqjWfVIMoZDNaakMSoYAQSZmaT65B5NTEJAlRQIikTFCDOP7VVROTJEUBIZIyrX0QhWxGE+UkEQoIkZSJ6qTWUhuSBAWESMpUWwMiZxML+InESQEhkjK1lnkQuUyGspqYJAEKCJGUaa1BFNTEJAlRQIikTK0+eR5EPqdhrpIMBYRIyrymDyKbUUBIIhQQIikT1CAmz4PQMFdJggJCJGWqdSeb1WJ9kjwFhEjK1Op1NTHJnFBAiKTMazqp1cQkCVFAiKSMlvuWuaKAEEmRujt1p2W5b6OqgJAEKCBEUqRWD5qSsk2jmHJqYpKEKCBEUqRxY6DWPgg1MUkSFBAiKVLzRg1ichNTpVbHXbUIiZcCQiRFGn0NrTUI9+PNTyJxUUCIpMjxPojmW44GX2P1Q0jcFBAiKVKNCIh8OKu6ontCSMwUECIp0qhBTBrmmgtrEFryW2KmgBBJkahhrnk1MUlC2goIM/sbM/sNM1OgiMyj6CamRkCoBiHxavc//C8DHwWeN7O/NLPzEyyTiEwhqomp0QehuRASt7YCwt0fcvffBi4CXgIeMrMfmdnvmVk+yQKKyHHVsCM6qgZRVROTxKztJiMzWwH8LvBvgCeBLxIExoOJlExEXmOiBpFVE5MkL9fOSWb2beB84C7g/e6+Jzx0r5kNJFU4EZlsog/C1MQkyWsrIID/5e73N+8ws6K7l9x9YwLlEpEIx2sQxyv/hayGuUoy2m1i+mzEvkdnepGZXW5m28xs0MxuiDheNLN7w+NbzGxtuP9iM3sqfPzUzD7UZjlFFrRaTTOpZe5MW4Mws5XAaqDbzN4GNH4qlwA9M7w2C9wKvBvYBTxuZpvd/Zmm064FDrv7eWZ2NXALcBXwc2Cju1fNbBXwUzP7rrtXT/wSRRaOasRifZpJLUmZqYnpPQQd02uAzzftHwb+bIbXXgwMuvt2ADO7B9gENAfEJuCmcPs+4EtmZu4+2nROF6BfjUSA2hSL9YGamCR+0waEu98J3GlmH3b3b53ge68GdjY93wVcMtU5YW1hCFgBHDCzS4A7gHOA34mqPZjZdcB1AGefffYJFk8kfaIW65tYakNNTBKzmZqYPubuXwPWmtmnW4+7++cjXhYLd98CvMnM3kgQUt9z9/GWc24DbgPYuHGjvh2y4GkmtcylmTqpe8M/FwGLIx7T2Q2c1fR8Tbgv8hwzywF9wMHmE9z9WWAEePMMnyey4NXqTsYg0zTMtdHcpGGuEreZmpj+Kvzzz0/ivR8H1pvZOoIguJpguY5mm4FrCEZEXQE87O4evmZn2Ox0DvAGghncIqe0at0n1R7geBOTZlJL3NpdrO9zZrbEzPJm9o9mtt/MPjbda8I+g+uBB4BngW+6+1Yzu9nMPhCedjuwwswGgU8DjaGw7yQYufQU8G3gD9z9wIlfnsjCUq07uczkr62amCQp7U6Uu8zd/yScj/AS8FvAPwFfm+5F4eS6+1v23di0PQ5cGfG6uwhmbYtIk1pEDWJimKsCQmLW7kS5RpD8BvDX7j6UUHlEZBq1en3SEFc4XoNQH4TErd0axN+Z2XPAGPDvzKwfGJ/hNSISs6g+iOPzINQHIfFqd7nvG4C3E8xurgDHCCa5icgcimpiymaMjB1fClwkLu3WICAYSbQ2HI7a8NWYyyMi06jV/TVNTBDUItTEJHFrd7nvu4BzgaeAWrjbUUCIzKmoJiYIVnRVE5PErd0axEZgg7vrJ1BkHgVNTK9tGc5lTaOYJHbtjmL6ObAyyYKIyMyqtfqku8k15LMZBYTErt0axGnAM2b2Y6DU2OnuH5j6JSISt5qrD0LmTrsBcVOShRCR9lRrU/RB5DJaakNi11ZAuPsPwzWR1rv7Q2bWA2STLZqItIoa5grBbGo1MUnc2l2L6d8S3NDnr8Jdq4HvJFUoEYk21TDXXEZ9EBK/djupPwW8AzgK4O7PA6cnVSgRiTbVMNdiPsN4RQEh8Wo3IEruXm48CSfLqcFTZI5VanUK2dd+bRcVcxwr65btEq92A+KHZvZnQLeZvRv4a+C7yRVLRFq5O+VqfeL+D816CllGS7WIV4mcvHYD4gZgP/A08EmCJbz/U1KFEpHXqtYdh8gaRG9BNQiJX7ujmOpm9h3gO+6+P+EyiUiEUjXoY4iqQfQWc4yWVYOQeE1bg7DATWZ2ANgGbAvvJnfjdK8TkfhVpgmInmKWkZJqEBKvmZqY/ohg9NIvuftyd18OXAK8w8z+KPHSiciEUq0REK+dgtRbyFGu1jXUVWI1U0D8DvARd3+xscPdtwMfAz6eZMFEZLJyowYR1QdRDFqL1cwkcZopIPLufqB1Z9gPkU+mSCISpTxdH0QhqFUcUzOTxGimgCif5DERidl0AdEzUYNQQEh8ZhrFdKGZHY3Yb0BXAuURkSk0VmuNnijXqEGoiUniM21AuLsW5BPpENPWIArBV1lNTBKndifKicg8K1eD2kExsg8iDAh1UkuMFBAiKdFoYspHjmIKKvvqg5A4KSBEUqJcrZPLWORqro1hrposJ3FSQIikRLkWvVAfBIv1AVqwT2KlgBBJiXI1eqlvaOqkVhOTxEgBIZISpSmW+gbIZozufFajmCRWiQaEmV1uZtvMbNDMbog4XjSze8PjW8xsbbj/3Wb2hJk9Hf75a0mWUyQNKtM0MUHQUa1RTBKnxALCzLLArcB7gQ3AR8xsQ8tp1wKH3f084AvALeH+A8D73f0C4BrgrqTKKZIWpWmamCBoZhpVDUJilGQN4mJg0N23h7crvQfY1HLOJuDOcPs+4FIzM3d/0t1fCfdvJbiTXTHBsop0vMo0TUwQjGQaUSe1xCjJgFgN7Gx6vivcF3mOu1eBIWBFyzkfBn7i7qWEyimSCtP1QUCwYJ/mQUic2rqj3HwxszcRNDtdNsXx64DrAM4+++w5LJnI3KvUZmhiKuYYGqvMYYlkoUuyBrEbOKvp+ZpwX+Q5ZpYD+oCD4fM1wLeBj7v7C1Ef4O63uftGd9/Y398fc/FFOstMNYhFRY1iknglGRCPA+vNbJ2ZFYCrgc0t52wm6IQGuAJ42N3dzJYC/xe4wd3/OcEyiqSCuwfzIKYJCHVSS9wSC4iwT+F64AHgWeCb7r7VzG42sw+Ep90OrDCzQeDTQGMo7PXAecCNZvZU+Dg9qbKKdLpStY4DxWmamHoLGuYq8Uq0D8Ld7wfub9l3Y9P2OHBlxOs+C3w2ybKJpEnjVqL5GUYxqZNa4qSZ1CIp0PiPP2qp74beYo5KzSlVVYuQeCggRFJgogYx7UQ5Ldgn8VJAiKRAY3TSTDUI0IJ9Ep+OngchIoGxsAZRyL32LsB3b9kBwNO7hwD45sAuVi7p4qOXaG6QzI5qECIp0BidNN1EucaxckVNTBIPBYRICjQ6qaefBxH2QSggJCYKCJEUGJ1oYppuJnV429Fx9UFIPBQQIilwQp3Umk0tMVFAiKTAkdEKGZs+IAq5DIVchhEFhMREASGSAkfGynTls5jZtOctKuYYVkBITBQQIilweLRCT2HmUemLijk1MUlsFBAiKTA0WpkYpTSdRcWcmpgkNgoIkRQ4PFpuPyA0ikliooAQSYEjoxW6820ERFeO0XKNWt3noFSy0CkgRFLgSJs1iN5iDgct+y2xUECIdLhytc6xco3uNjupAfVDSCwUECId7shYGaDtPghQQEg8FBAiHW5otAKcWEBoqKvEQQEh0uEOTwTECTQxaSSTxEABIdLhjowGTUzdbdQguvIZshlTE5PEQgEh0uGOnEATk5mFk+W05LfMngJCpMMdDmsQPW3Mg4DGbOpKkkWSU4QCQqTDHRmrkM/atPeCaKbZ1BIXBYRIhzsyWqavuzDjSq4Ni7tyDCsgJAYKCJEOd2S0wrKefNvnL+nOM1KqUq3VEyyVnAoUECId7vBomaUnEBCLu4LlNg6MlJMrlJwSFBAiHe7IaIWlPYW2z1/SFYTJvuHxpIokpwgFhEiHOzJaYWn3idUgAPYeLSVVJDlFKCBEOpi7c3i0zLLe9msQi8MaxN6jqkHI7CggRDrY/pESpWqd1Uu7237NomIOA/YNqwYhs5NoQJjZ5Wa2zcwGzeyGiONFM7s3PL7FzNaG+1eY2ffNbMTMvpRkGUU62Y6DowCcvaKn7ddkM8Fs6n2qQcgsJRYQZpYFbgXeC2wAPmJmG1pOuxY47O7nAV8Abgn3jwP/GfjjpMonkgY7DoUBsbz9gICgH0JNTDJbSdYgLgYG3X27u5eBe4BNLedsAu4Mt+8DLjUzc/dj7v4IQVCInLJePjiKGaxZ1n4TEwT9EGpiktlKMiBWAzubnu8K90We4+5VYAhY0e4HmNl1ZjZgZgP79++fZXFFOs/OQ6OsWtJFMdfeOkwNS7pzGsUks5bqTmp3v83dN7r7xv7+/vkujkjsXj40ekL9Dw2Lu/IcPFbSbGqZlSQDYjdwVtPzNeG+yHPMLAf0AQcTLJNIquw4NHrC/Q8QzqZ2zaaW2UkyIB4H1pvZOjMrAFcDm1vO2QxcE25fATzs7p5gmURSY7RcZf9wiXNW9J7wa5doLoTEYOZ7GJ4kd6+a2fXAA0AWuMPdt5rZzcCAu28GbgfuMrNB4BBBiABgZi8BS4CCmX0QuMzdn0mqvCKdZuehMQDOOskaBMCrR8e5MNZSyakksYAAcPf7gftb9t3YtD0OXDnFa9cmWTaRTvfywWPAiQ9xBSbWbnrlyFisZZJTS6IBISIn5+4tO3jk+WBk3sCLh3jmlaMn9PreQpaufIbdhxUQcvJSPYpJZCHbN1yiO5+lu417UbcyM85c2s0rQwoIOXkKCJEOtffoOCv7utq+k1yr1Uu7VYOQWVFAiHSgujt7h0ucsaTrpN9j9dJudqsPQmZBASHSgY6MVihX66ycZUAcGCkzXqnFWDI5lSggRDpQY/7CyiXFk36P1eH6TRrJJCdLASHSgRoBcfosahBnhveQUDOTnCwFhEgHevXoOEt78nTlT3wEU0PjJkOqQcjJUkCIdKBXh8Zn1f8AsLKvi4yhkUxy0hQQIh2mXK1zYGR2I5gA8tkMZyzpYpdqEHKSFBAiHWb7gRHqzqwDAjQXQmZHASHSYba9Ogww6yYmgHNW9PLC/hG0SLKcDAWESId57tVhMganLS7M+r3eevZSDoyU2aVahJwEBYRIh9n26jD9i4vkMrP/er7trKUAPLnzyKzfS049CgiRDrPt1eFY+h8Azl+5mK58hqd2KCDkxCkgRDrI8HiF3UfGYul/gGAk01tWL+XJnYdjeT85teh+ECId5Bd7gw7qOGoQd2/ZAUAxl+GJ7Yf56o9eIpfN8NFLzp71e8upQTUIkQ7yXIwjmBrOWt5Dre68MqT7U8uJUUCIdJCtrxxlcTHH0p58bO+59rRe8lnje0/voVqvx/a+svApIEQ6yGPbD7Jx7bKTvklQlEXFHB++aA0vHxrl2z/ZTbWmkJD2KCBEOsTeo+Ns33+MXzl3Rezv/ZY1S7n0jafz5M4jfOLOAUZK1dg/QxYeBYRIh3hs+0EAfuV1pyXy/pe+4Qw+9NbVPPL8fr740C8S+QxZWBQQIh3i0RcOsrgrx4YzlyT2Gb+0bjnvu2AV3/jxTo6OVxL7HFkYFBAiHeLR7Qe5ZN1yspn4+h+ifPJd5zJSqvKNcBisyFQUECId4OldQ7x8cJS3n5tM81KzC9b08fZzV3D7Iy8yWlZfhExNASHSAW75++dY1pPnio1rEv+su7fs4ILVfewbLvGpr/+Eu7fsmJhUJ9JMASEyzx58Zi+PDB7g+l9bz5Ku+OY/TOecFb1cuKaP//f8AXYdHp2Tz5T00VIbInPoxQPH+OJDv2DrK0epu2MYg/tHOG1RgXzG5vQ3+cvfvIpte4f58g9eYFlPnv/zzy+SMaOrkOX8Mxbx/gvP5FfX989ZeaTz2EK5kcjGjRt9YGBgvoshp5BytU6t7hRzGTIzdCxXa3W+9tjL3PL328hljEteF8x1eGrnYd50Zh//8vX9dOWzc1HsSUZKVX668wg7D49SrzsOjFVq7DkyzlilxvlnLObLH7uIc/sXzXnZZG6Y2RPuvjHymAJCZGbD4xUG940wNFZh26vDbHnxEI8MHqBcrdNbyHLZm1byjvNOY1VfF3uGxtlxaJT9wyVW9BYYKVX5u5/t4cBIifWnL+K3LlpDX/fcNCWdrGqtzo9eOMj3t+2jVnd+8y2r+PUNZ3D64i6q9TpHxyrkMhkKuQy9xRwXrO6jkFOLdRrNW0CY2eXAF4Es8L/d/S9bjheBrwL/AjgIXOXuL4XH/hS4FqgBf+juD0z3WQoIidPRMBCeeOkwDz+3j8dfOkS1fvy7sry3wPkrF7OkmOPAsTJbXxlivHJ8CQsDegpZxio1cpkMq/q6eNfr+3nDysWxLqORtOHxCt/fto+f7hxirFKb8rzlvQU+cOGZfPBtq3nDysVkzNh7dJw9Q+OMV2qsXtbNuhW9M9a0ZO7NS0CYWRb4BfBuYBfwOPARd3+m6Zw/AN7i7r9vZlcDH3L3q8xsA/AN4GLgTOAh4PXuPuVPaCcFhLvT+Gv18HljOzgOHj5r/uuP2t/6+onzJ45P/VnedE5j51RlOL5/5nK1nlurO+VanXK1Tt2djBkZM7IZwwyyGZvYl2l6fqxc5dBImUOjZQ4Mlxgaq9JTyLKoK8eiYo5FXTkWF3P0FnPU3anWnGq9TqUWbFfqdWrN+8I/a+F/5NmMkTWjVndK1RrjlTqlao1KzenKZ+kpZOnKZ6jV4fBomR0HR3n50DFePHCMvUdLE9d/xpIi55+xhHNW9NBTyLJiUZFFxcndd7W6c+hYmaPjFZZ25+nryZPLZKiHf0eZFIVClFrd2TM0xlilRsaM7nyWugd/18PjVZ7ePcSze45OCtFWpy0q8s7zVnBu/yLOWNLF8t4Cy3oL9HXn6Cnk6ClkyWUzkd+X5h1R35HgefTr6u6MlmuMlquUq3W68ll6Czm6C9nwM+34e3n0d6r1qhr/mo2wr4Y//6WJR41yNfiFIZfJkM0Y+WzwnchlMuRzRlcuS3chSy5jk35paP5Jaf6xSeoXi+kCIslO6ouBQXffHhbiHmAT8EzTOZuAm8Lt+4AvWfC3sAm4x91LwItmNhi+36NxF/LpXUNcddujU/7n3M5/xM2vkfRaXMyxrLfAmX3dvHXNUk5f0sWqvi6W9sx8b+hsxuhfXKR/cXHS/rQHQ0M2Y6xZ1jPl8Tev7mOsXGPb3qMcGa1Qqzt93Xn6uvPkshkOHSvx/L4RHn5uH9956pU5LPnClGn5xet9F6ziv//rC2P/nCQDYjWws+n5LuCSqc5x96qZDQErwv2Ptbx2desHmNl1wHXh0xEz2xZP0WN3GnBgvgsRA11HZ9F1dJZ5u47ngM9fddIvP2eqA6ke5urutwG3zXc5ZmJmA1NV4dJE19FZdB2dZaFcR7Mkhx3sBs5qer4m3Bd5jpnlgD6Czup2XisiIglKMiAeB9ab2TozKwBXA5tbztkMXBNuXwE87EHj/mbgajMrmtk6YD3w4wTLKiIiLRJrYgr7FK4HHiAY5nqHu281s5uBAXffDNwO3BV2Qh8iCBHC875J0KFdBT413QimFOj4ZrA26To6i66jsyyU65iwYCbKiYhIvDT1UUREIikgREQkkgIiZmb2F2b2MzN7ysz+wczODPebmf1PMxsMj1/U9JprzOz58HHN1O8+d8zsv5nZc2FZv21mS5uO/Wl4HdvM7D1N+y8P9w2a2Q3zU/LJzOxKM9tqZnUz29hyLDXX0SoNZWwwszvMbJ+Z/bxp33IzezD8mX/QzJaF+6f8nsw3MzvLzL5vZs+EP1P/PtyfumtpW7AshB5xPYAlTdt/CHwl3H4f8D2CmfS/DGwJ9y8Htod/Lgu3l3XAdVwG5MLtW4Bbwu0NwE+BIrAOeIFgEEI23H4dUAjP2dAB1/FG4HzgB8DGpv2puo6Wa+r4MraU913ARcDPm/Z9Drgh3L6h6ecr8nvSCQ9gFXBRuL2YYCmhDWm8lnYfqkHEzN2PNj3t5fhqHJuAr3rgMWCpma0C3gM86O6H3P0w8CBw+ZwWOoK7/4O7N+5H+RjBXBRoWgbF3V8EGsugTCyt4u5loLG0yrxy92fdPWqGfaquo0UayjjB3f+JYJRis03AneH2ncAHm/ZHfU/mnbvvcfefhNvDwLMEKzyk7lrapYBIgJn9VzPbCfw2cGO4O2rpkdXT7O8knyD4TQjSfR3N0nwdaSjjTM5w9z3h9qvAGeF2Kq7NzNYCbwO2kPJrmU6ql9qYL2b2ELAy4tBn3P1v3f0zwGfCJcuvB/7LnBawTTNdR3jOZwjmonx9Lst2Itq5Dulc7u5mlprx9ma2CPgW8B/c/WjzKqtpu5aZKCBOgrv/epunfh24nyAgplo+ZDfwr1r2/2DWhWzDTNdhZr8L/CZwqYeNqky/DMq8LI9yAv8ezTruOk7AQliKZq+ZrXL3PWGzy75wf0dfm5nlCcLh6+7+N+HuVF5LO9TEFDMzW9/0dBPBQosQLB/y8XBkwy8DQ/MH5DUAAAEESURBVGG19AHgMjNbFo5+uCzcN68suNnTnwAfcPfmu9pPtQxKO0urdJI0X0cayjiT5mV2rgH+tml/1Pdk3llQVbgdeNbdP990KHXX0rb57iVfaA+C3y5+DvwM+C6wOtxvwK0Eo0+eZvKImk8QdJIOAr8339cQlmmQoP30qfDxlaZjnwmvYxvw3qb97yMY2fECQfNOJ1zHhwjafkvAXuCBNF5HxHV1fBmbyvoNYA9QCf8triVY1v8fgecJbgi2PDx3yu/JfD+AdxIMOvlZ0/fifWm8lnYfWmpDREQiqYlJREQiKSBERCSSAkJERCIpIEREJJICQkREIikgREQkkgJCREQi/X+i8TcLDBY4WwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ZYx5kpCJkruG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns = [\"File_name\",\"angle\"])"
      ],
      "metadata": {
        "id": "i4FxN_b3lOQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/data.txt\")\n",
        "data = file.readlines()"
      ],
      "metadata": {
        "id": "yR3VXPPglVsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HZAA9_ylfdJ",
        "outputId": "8268fb28-dc48-494f-e5b9-1c916f3e2f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.jpg 0.000000,2018-07-01 17:09:44:912\\n',\n",
              " '1.jpg 0.000000,2018-07-01 17:09:44:972\\n',\n",
              " '2.jpg 0.000000,2018-07-01 17:09:45:11\\n',\n",
              " '3.jpg 0.000000,2018-07-01 17:09:45:76\\n',\n",
              " '4.jpg 0.000000,2018-07-01 17:09:45:105\\n',\n",
              " '5.jpg 0.000000,2018-07-01 17:09:45:145\\n',\n",
              " '6.jpg 0.000000,2018-07-01 17:09:45:205\\n',\n",
              " '7.jpg 0.000000,2018-07-01 17:09:45:246\\n',\n",
              " '8.jpg 0.000000,2018-07-01 17:09:45:301\\n',\n",
              " '9.jpg 0.000000,2018-07-01 17:09:45:341\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = [x.split(\" \")[0] for x in data] "
      ],
      "metadata": {
        "id": "2Z785jxzlhF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angles = [x.split(\" \")[1] for x in data]"
      ],
      "metadata": {
        "id": "U-IJ23MTltqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angles = [x.split(\",\")[0] for x in angles]"
      ],
      "metadata": {
        "id": "jZRSb19BlzQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angles = list(map(float,angles))"
      ],
      "metadata": {
        "id": "J_PzgK6bl6vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.File_name = file_names"
      ],
      "metadata": {
        "id": "RfUATCIHmI9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.angle = angles"
      ],
      "metadata": {
        "id": "JiiPeUihmO3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MK_gdF-MmSET",
        "outputId": "1a205681-022e-49de-c9d6-a43ff126f6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-542236c4-6394-4550-84df-39a7365baa0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_name</th>\n",
              "      <th>angle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-542236c4-6394-4550-84df-39a7365baa0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-542236c4-6394-4550-84df-39a7365baa0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-542236c4-6394-4550-84df-39a7365baa0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  File_name  angle\n",
              "0     0.jpg    0.0\n",
              "1     1.jpg    0.0\n",
              "2     2.jpg    0.0\n",
              "3     3.jpg    0.0\n",
              "4     4.jpg    0.0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi = 22/7\n",
        "def degrees_rads(x):\n",
        "  x = float((x)*pi/180)\n",
        "  return x"
      ],
      "metadata": {
        "id": "9umqgeO_mYq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.angle = df.angle.map(degrees_rads)"
      ],
      "metadata": {
        "id": "kuI-977Nm0FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"data_file.csv\")"
      ],
      "metadata": {
        "id": "66nncQw6ntnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple mean model\n",
        "\n",
        "split = 0.8\n",
        "\n",
        "X =[]\n",
        "y =[]"
      ],
      "metadata": {
        "id": "sZq-5KXmn0us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3CZeAmfdoEvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(df.angle)"
      ],
      "metadata": {
        "id": "2PO-OmSloGWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYW8IpSSXs64",
        "outputId": "2a1b5791-68bb-41c0-cc1b-1390648ddc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.        ,  0.        ,  0.        , ..., -0.19712698,\n",
              "       -0.19712698, -0.19712698])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(len(y)*0.8)"
      ],
      "metadata": {
        "id": "FAxDIbWZoTEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y =y[:split_index]\n",
        "test_y = y[split_index:]"
      ],
      "metadata": {
        "id": "6ajsjOe2oXfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mean model\n",
        "y_test_pred = np.mean(train_y)"
      ],
      "metadata": {
        "id": "VwMWdHRLoeUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjNTEGNiopg4",
        "outputId": "9b2f4f8c-ee56-4969-e1fd-1899b082d9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.015270005220344565"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LDTUDfHAortA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(train_y,bins=50,color=\"green\",histtype=\"step\")\n",
        "plt.hist(test_y,bins=50,color=\"red\",histtype=\"step\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "mY0akD04o1zg",
        "outputId": "e0f0674d-4363-44c9-f824-df33a7676165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MSE (MEAN MODEL) is \" + str(np.mean(np.square(test_y-y_test_pred))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-VtelaOpDlK",
        "outputId": "492c2af2-b8ef-450e-9e10-5315e9902533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE (MEAN MODEL) is 0.0905898860902429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MSE (ZERO CONSTANT MODEL) is \" + str(np.mean(np.square(test_y-0))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qx7AVBxpqdd",
        "outputId": "6a636b1d-7ffa-4ea6-d8d9-8701a3c07038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE (ZERO CONSTANT MODEL) is 0.09184150655669693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs=[]\n",
        "ys=[]"
      ],
      "metadata": {
        "id": "KHRJy3kQp1en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#points to the end of the last batch\n",
        "train_batch_pointer =0\n",
        "val_batch_pointer = 0"
      ],
      "metadata": {
        "id": "TwNWsrK23VRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read data\n",
        "\n",
        "for x in range(len(df)):\n",
        "  xs.append(\"/content/data/\" + str(df.File_name.iloc[x]))\n",
        "  ys.append(df.angle[x])\n"
      ],
      "metadata": {
        "id": "3-7XemIn3dWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = len(xs)"
      ],
      "metadata": {
        "id": "2CEsLS7v4C5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_xs = xs[:int(len(xs)*0.8)]\n",
        "\n",
        "train_ys = ys[:int(len(xs)*0.8)]"
      ],
      "metadata": {
        "id": "hcO2HkGG4E3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_xs = xs[-int(len(xs)*0.2):]\n",
        "\n",
        "val_ys = ys[-int(len(xs)*0.2):]"
      ],
      "metadata": {
        "id": "XrESziIM4GxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_images=len(train_xs)"
      ],
      "metadata": {
        "id": "3dFuB96b4gSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_images = len(val_xs)"
      ],
      "metadata": {
        "id": "o5HyBVhE4k-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "iOgTS-pT5hDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadTrainBatch(batch_size):\n",
        "  global train_batch_pointer\n",
        "  x_out =[]\n",
        "  y_out =[]\n",
        "  for i in range(0,batch_size):\n",
        "    x_out.append(cv2.resize(cv2.imread(train_xs[(train_batch_pointer + i) % num_train_images])[-150:], (200, 66)) / 255.0)\n",
        "    y_out.append([train_ys[(train_batch_pointer + i) % num_train_images]])\n",
        "  train_batch_pointer += batch_size\n",
        "  return x_out,y_out"
      ],
      "metadata": {
        "id": "Qn6zU9iN4of3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadValBatch(batch_size):\n",
        "  global val_batch_pointer\n",
        "  x_out =[]\n",
        "  y_out =[]\n",
        "  for i in range(0,batch_size):\n",
        "    x_out.append(cv2.resize(cv2.imread(val_xs[(val_batch_pointer + i) % num_val_images])[-150:], (200, 66)) / 255.0)\n",
        "    y_out.append([val_ys[(val_batch_pointer + i) % num_val_images]])\n",
        "  val_batch_pointer += batch_size\n",
        "  return x_out,y_out"
      ],
      "metadata": {
        "id": "Ur9Y38r35yis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "1yS8qVSK6Fcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model architecture\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(BatchNormalization(epsilon=0.001,trainable=False, axis=3,input_shape=(66,200,3)))\n",
        "\n",
        "model.add(Conv2D(24,(5,5),padding='valid', activation='relu', strides=(2,2)))\n",
        "model.add(Conv2D(36,(5,5),padding='valid', activation='relu', strides=(2,2)))\n",
        "model.add(Conv2D(48,(5,5),padding='valid', activation='relu', strides=(2,2)))\n",
        "model.add(Conv2D(64,(3,3),padding='valid', activation='relu', strides=(1,1)))\n",
        "model.add(Conv2D(64,(3,3),padding='valid', activation='relu', strides=(1,1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1164, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='tanh'))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga-HW_Z0DTGB",
        "outputId": "09d8d259-d11d-45c6-eae2-46536927a19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (BatchN  (None, 66, 200, 3)       12        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 31, 98, 24)        1824      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 47, 36)        21636     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 5, 22, 48)         43248     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 20, 64)         27712     \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 1, 18, 64)         36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1152)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1164)              1342092   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               116500    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,595,523\n",
            "Trainable params: 1,595,511\n",
            "Non-trainable params: 12\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='mse',\n",
        "              optimizer=adam\n",
        "              )"
      ],
      "metadata": {
        "id": "YzdTo8TvHBjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30"
      ],
      "metadata": {
        "id": "LZ7r9lrKBaQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100"
      ],
      "metadata": {
        "id": "BRGPrG_bBaTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  print(str(epoch+1)+\" Epoch\")\n",
        "  for i in range(num_train_images//100):\n",
        "    train_x,train_y = LoadTrainBatch(batch_size)\n",
        "    train_x = np.array(train_x)\n",
        "    train_y = np.array(train_y)\n",
        "    model.fit(train_x,train_y,verbose=0)\n",
        "    print(str(model.history.history[\"loss\"][0])+ \" : Loss for Batch-> \" + str(i+1))   \n",
        "    if i%10==0:\n",
        "      val_x,val_y = LoadValBatch(batch_size)\n",
        "      val_x = np.array(val_x)\n",
        "      val_y = np.array(val_y)\n",
        "      val_loss = model.evaluate(val_x,val_y,verbose=0)\n",
        "      print(str(val_loss) + \" : Val Loss after training \" + str(i+1)+\" batche(s)\")\n",
        "    model.save(\"saved_model.h5\")\n",
        "\n",
        "      "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md-r5bFiBaWo",
        "outputId": "be257b09-fc5d-479b-eca6-09de6bb6572f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.004520346410572529 : Loss for Batch-> 52\n",
            "0.018866682425141335 : Loss for Batch-> 53\n",
            "0.018003204837441444 : Loss for Batch-> 54\n",
            "0.004694061353802681 : Loss for Batch-> 55\n",
            "0.0003892533131875098 : Loss for Batch-> 56\n",
            "0.009947068057954311 : Loss for Batch-> 57\n",
            "0.0747578889131546 : Loss for Batch-> 58\n",
            "0.22905398905277252 : Loss for Batch-> 59\n",
            "0.23879310488700867 : Loss for Batch-> 60\n",
            "0.03340647369623184 : Loss for Batch-> 61\n",
            "0.011912508867681026 : Val Loss after training 61 batche(s)\n",
            "0.0852825865149498 : Loss for Batch-> 62\n",
            "0.053461551666259766 : Loss for Batch-> 63\n",
            "0.10095419734716415 : Loss for Batch-> 64\n",
            "0.06141970679163933 : Loss for Batch-> 65\n",
            "0.08882547169923782 : Loss for Batch-> 66\n",
            "0.029049301519989967 : Loss for Batch-> 67\n",
            "0.011811021715402603 : Loss for Batch-> 68\n",
            "0.011768016964197159 : Loss for Batch-> 69\n",
            "0.01035241037607193 : Loss for Batch-> 70\n",
            "0.010822744108736515 : Loss for Batch-> 71\n",
            "0.00835160817950964 : Val Loss after training 71 batche(s)\n",
            "0.010907289572060108 : Loss for Batch-> 72\n",
            "0.022057538852095604 : Loss for Batch-> 73\n",
            "0.28926143050193787 : Loss for Batch-> 74\n",
            "2.2845637798309326 : Loss for Batch-> 75\n",
            "0.023127369582653046 : Loss for Batch-> 76\n",
            "0.08655192703008652 : Loss for Batch-> 77\n",
            "0.09079018235206604 : Loss for Batch-> 78\n",
            "0.028722429648041725 : Loss for Batch-> 79\n",
            "0.0011525633744895458 : Loss for Batch-> 80\n",
            "0.027091646566987038 : Loss for Batch-> 81\n",
            "0.00791696086525917 : Val Loss after training 81 batche(s)\n",
            "0.08714818954467773 : Loss for Batch-> 82\n",
            "0.08100578188896179 : Loss for Batch-> 83\n",
            "0.0916435495018959 : Loss for Batch-> 84\n",
            "0.024368761107325554 : Loss for Batch-> 85\n",
            "0.013408166356384754 : Loss for Batch-> 86\n",
            "0.037656959146261215 : Loss for Batch-> 87\n",
            "0.03588755428791046 : Loss for Batch-> 88\n",
            "0.0021117946598678827 : Loss for Batch-> 89\n",
            "0.03547889366745949 : Loss for Batch-> 90\n",
            "0.03763429820537567 : Loss for Batch-> 91\n",
            "0.007764420006424189 : Val Loss after training 91 batche(s)\n",
            "0.009451211430132389 : Loss for Batch-> 92\n",
            "0.014411362819373608 : Loss for Batch-> 93\n",
            "0.006040459498763084 : Loss for Batch-> 94\n",
            "0.0016913588624447584 : Loss for Batch-> 95\n",
            "0.0011199170257896185 : Loss for Batch-> 96\n",
            "0.0069559733383357525 : Loss for Batch-> 97\n",
            "0.001802082173526287 : Loss for Batch-> 98\n",
            "0.0031681654509156942 : Loss for Batch-> 99\n",
            "0.002076271688565612 : Loss for Batch-> 100\n",
            "0.004710243549197912 : Loss for Batch-> 101\n",
            "0.00581576582044363 : Val Loss after training 101 batche(s)\n",
            "0.021714990958571434 : Loss for Batch-> 102\n",
            "0.017301660031080246 : Loss for Batch-> 103\n",
            "0.0162460058927536 : Loss for Batch-> 104\n",
            "0.010336753912270069 : Loss for Batch-> 105\n",
            "0.0014877701178193092 : Loss for Batch-> 106\n",
            "0.009056294336915016 : Loss for Batch-> 107\n",
            "0.02878764644265175 : Loss for Batch-> 108\n",
            "0.0029179579578340054 : Loss for Batch-> 109\n",
            "0.0054702116176486015 : Loss for Batch-> 110\n",
            "0.0008891627076081932 : Loss for Batch-> 111\n",
            "0.014596117660403252 : Val Loss after training 111 batche(s)\n",
            "0.038406193256378174 : Loss for Batch-> 112\n",
            "0.03633852303028107 : Loss for Batch-> 113\n",
            "0.05170615017414093 : Loss for Batch-> 114\n",
            "0.004850404337048531 : Loss for Batch-> 115\n",
            "0.05767619609832764 : Loss for Batch-> 116\n",
            "0.024742627516388893 : Loss for Batch-> 117\n",
            "0.00789306964725256 : Loss for Batch-> 118\n",
            "0.014780918136239052 : Loss for Batch-> 119\n",
            "0.010308091528713703 : Loss for Batch-> 120\n",
            "0.113839291036129 : Loss for Batch-> 121\n",
            "0.02012651413679123 : Val Loss after training 121 batche(s)\n",
            "0.062003906816244125 : Loss for Batch-> 122\n",
            "0.009201012551784515 : Loss for Batch-> 123\n",
            "0.03433752432465553 : Loss for Batch-> 124\n",
            "0.007163274101912975 : Loss for Batch-> 125\n",
            "0.00207525328733027 : Loss for Batch-> 126\n",
            "0.022072747349739075 : Loss for Batch-> 127\n",
            "0.03690929710865021 : Loss for Batch-> 128\n",
            "0.13805842399597168 : Loss for Batch-> 129\n",
            "0.006138473283499479 : Loss for Batch-> 130\n",
            "0.048720721155405045 : Loss for Batch-> 131\n",
            "0.004591875243932009 : Val Loss after training 131 batche(s)\n",
            "0.02715480513870716 : Loss for Batch-> 132\n",
            "0.04078008234500885 : Loss for Batch-> 133\n",
            "0.08563776314258575 : Loss for Batch-> 134\n",
            "0.2276676595211029 : Loss for Batch-> 135\n",
            "0.029834609478712082 : Loss for Batch-> 136\n",
            "0.005518557503819466 : Loss for Batch-> 137\n",
            "0.00044066619011573493 : Loss for Batch-> 138\n",
            "0.0006698044599033892 : Loss for Batch-> 139\n",
            "0.000824536953587085 : Loss for Batch-> 140\n",
            "0.009151597507297993 : Loss for Batch-> 141\n",
            "0.0006153923459351063 : Val Loss after training 141 batche(s)\n",
            "0.016107935458421707 : Loss for Batch-> 142\n",
            "0.01243081595748663 : Loss for Batch-> 143\n",
            "0.012439637444913387 : Loss for Batch-> 144\n",
            "0.004344362765550613 : Loss for Batch-> 145\n",
            "0.0017575130332261324 : Loss for Batch-> 146\n",
            "0.0028645694255828857 : Loss for Batch-> 147\n",
            "0.017654886469244957 : Loss for Batch-> 148\n",
            "0.06253760308027267 : Loss for Batch-> 149\n",
            "0.03398359566926956 : Loss for Batch-> 150\n",
            "0.011708752252161503 : Loss for Batch-> 151\n",
            "0.00869156327098608 : Val Loss after training 151 batche(s)\n",
            "0.04374535381793976 : Loss for Batch-> 152\n",
            "0.036715857684612274 : Loss for Batch-> 153\n",
            "0.004303963389247656 : Loss for Batch-> 154\n",
            "0.03895861655473709 : Loss for Batch-> 155\n",
            "0.02654772251844406 : Loss for Batch-> 156\n",
            "0.010027886368334293 : Loss for Batch-> 157\n",
            "0.002069259760901332 : Loss for Batch-> 158\n",
            "0.05136728286743164 : Loss for Batch-> 159\n",
            "0.06385631859302521 : Loss for Batch-> 160\n",
            "0.015824472531676292 : Loss for Batch-> 161\n",
            "0.08648889511823654 : Val Loss after training 161 batche(s)\n",
            "0.03173379600048065 : Loss for Batch-> 162\n",
            "0.045639410614967346 : Loss for Batch-> 163\n",
            "0.05095888674259186 : Loss for Batch-> 164\n",
            "0.03289009630680084 : Loss for Batch-> 165\n",
            "0.01540578342974186 : Loss for Batch-> 166\n",
            "0.025511346757411957 : Loss for Batch-> 167\n",
            "0.00043741759145632386 : Loss for Batch-> 168\n",
            "0.00084752804832533 : Loss for Batch-> 169\n",
            "0.0015002407599240541 : Loss for Batch-> 170\n",
            "0.00037971639540046453 : Loss for Batch-> 171\n",
            "0.09116439521312714 : Val Loss after training 171 batche(s)\n",
            "0.0003486577479634434 : Loss for Batch-> 172\n",
            "0.0006573835271410644 : Loss for Batch-> 173\n",
            "0.012118917889893055 : Loss for Batch-> 174\n",
            "0.019667068496346474 : Loss for Batch-> 175\n",
            "0.001152697834186256 : Loss for Batch-> 176\n",
            "0.0004237285756971687 : Loss for Batch-> 177\n",
            "0.0002593683893792331 : Loss for Batch-> 178\n",
            "1.8472976080374792e-05 : Loss for Batch-> 179\n",
            "7.362159522017464e-06 : Loss for Batch-> 180\n",
            "0.0006204441306181252 : Loss for Batch-> 181\n",
            "0.03250371664762497 : Val Loss after training 181 batche(s)\n",
            "0.00019783273455686867 : Loss for Batch-> 182\n",
            "0.00029776449082419276 : Loss for Batch-> 183\n",
            "0.0007842079503461719 : Loss for Batch-> 184\n",
            "0.00034914686693809927 : Loss for Batch-> 185\n",
            "0.00044312604586593807 : Loss for Batch-> 186\n",
            "0.0010866537922993302 : Loss for Batch-> 187\n",
            "0.01075016614049673 : Loss for Batch-> 188\n",
            "0.024646759033203125 : Loss for Batch-> 189\n",
            "0.012177825905382633 : Loss for Batch-> 190\n",
            "0.005938667804002762 : Loss for Batch-> 191\n",
            "0.0666024312376976 : Val Loss after training 191 batche(s)\n",
            "0.0008454655180685222 : Loss for Batch-> 192\n",
            "0.0001864654041128233 : Loss for Batch-> 193\n",
            "0.00020542707352433354 : Loss for Batch-> 194\n",
            "0.016998760402202606 : Loss for Batch-> 195\n",
            "3.1070175170898438 : Loss for Batch-> 196\n",
            "0.01953260973095894 : Loss for Batch-> 197\n",
            "0.022733520716428757 : Loss for Batch-> 198\n",
            "0.0700283870100975 : Loss for Batch-> 199\n",
            "0.04066385701298714 : Loss for Batch-> 200\n",
            "0.01619379222393036 : Loss for Batch-> 201\n",
            "0.06396866589784622 : Val Loss after training 201 batche(s)\n",
            "0.02313149720430374 : Loss for Batch-> 202\n",
            "0.00907784141600132 : Loss for Batch-> 203\n",
            "0.008193151094019413 : Loss for Batch-> 204\n",
            "0.005867636296898127 : Loss for Batch-> 205\n",
            "0.006150128785520792 : Loss for Batch-> 206\n",
            "0.018898043781518936 : Loss for Batch-> 207\n",
            "0.03292103856801987 : Loss for Batch-> 208\n",
            "0.002082657301798463 : Loss for Batch-> 209\n",
            "0.03313913196325302 : Loss for Batch-> 210\n",
            "0.027082407847046852 : Loss for Batch-> 211\n",
            "0.004253734834492207 : Val Loss after training 211 batche(s)\n",
            "0.019781064242124557 : Loss for Batch-> 212\n",
            "0.038069948554039 : Loss for Batch-> 213\n",
            "0.0005490599432960153 : Loss for Batch-> 214\n",
            "0.00039879948599264026 : Loss for Batch-> 215\n",
            "0.005719730630517006 : Loss for Batch-> 216\n",
            "0.07144591957330704 : Loss for Batch-> 217\n",
            "0.03782198950648308 : Loss for Batch-> 218\n",
            "0.008395266719162464 : Loss for Batch-> 219\n",
            "0.17130693793296814 : Loss for Batch-> 220\n",
            "0.002096431562677026 : Loss for Batch-> 221\n",
            "0.021101584658026695 : Val Loss after training 221 batche(s)\n",
            "0.0004125393461436033 : Loss for Batch-> 222\n",
            "0.00043438098509795964 : Loss for Batch-> 223\n",
            "0.0007779241423122585 : Loss for Batch-> 224\n",
            "1.35614812374115 : Loss for Batch-> 225\n",
            "1.1654525995254517 : Loss for Batch-> 226\n",
            "9.862455368041992 : Loss for Batch-> 227\n",
            "1.6116126775741577 : Loss for Batch-> 228\n",
            "0.2082531601190567 : Loss for Batch-> 229\n",
            "0.12145665287971497 : Loss for Batch-> 230\n",
            "0.003038646187633276 : Loss for Batch-> 231\n",
            "0.006138107739388943 : Val Loss after training 231 batche(s)\n",
            "0.04126454517245293 : Loss for Batch-> 232\n",
            "0.02820298820734024 : Loss for Batch-> 233\n",
            "0.0017440448282286525 : Loss for Batch-> 234\n",
            "0.0011821645312011242 : Loss for Batch-> 235\n",
            "0.0053059314377605915 : Loss for Batch-> 236\n",
            "0.10376864671707153 : Loss for Batch-> 237\n",
            "0.10403455048799515 : Loss for Batch-> 238\n",
            "0.13571174442768097 : Loss for Batch-> 239\n",
            "0.5017794370651245 : Loss for Batch-> 240\n",
            "0.5664751529693604 : Loss for Batch-> 241\n",
            "0.022963890805840492 : Val Loss after training 241 batche(s)\n",
            "4.73250150680542 : Loss for Batch-> 242\n",
            "0.010497484356164932 : Loss for Batch-> 243\n",
            "0.030149951577186584 : Loss for Batch-> 244\n",
            "4.267411708831787 : Loss for Batch-> 245\n",
            "0.16916334629058838 : Loss for Batch-> 246\n",
            "1.1278507709503174 : Loss for Batch-> 247\n",
            "0.09418220818042755 : Loss for Batch-> 248\n",
            "0.12137246131896973 : Loss for Batch-> 249\n",
            "0.41316014528274536 : Loss for Batch-> 250\n",
            "0.5058606266975403 : Loss for Batch-> 251\n",
            "0.009863452054560184 : Val Loss after training 251 batche(s)\n",
            "1.3668928146362305 : Loss for Batch-> 252\n",
            "6.560921669006348 : Loss for Batch-> 253\n",
            "1.7462886571884155 : Loss for Batch-> 254\n",
            "0.002520535374060273 : Loss for Batch-> 255\n",
            "0.3432469964027405 : Loss for Batch-> 256\n",
            "0.3457154333591461 : Loss for Batch-> 257\n",
            "4.407797813415527 : Loss for Batch-> 258\n",
            "1.8896102905273438 : Loss for Batch-> 259\n",
            "0.17338331043720245 : Loss for Batch-> 260\n",
            "0.001821165787987411 : Loss for Batch-> 261\n",
            "0.0016764035681262612 : Val Loss after training 261 batche(s)\n",
            "1.1486880779266357 : Loss for Batch-> 262\n",
            "0.9788532853126526 : Loss for Batch-> 263\n",
            "0.1385439932346344 : Loss for Batch-> 264\n",
            "0.008794547989964485 : Loss for Batch-> 265\n",
            "0.06607744097709656 : Loss for Batch-> 266\n",
            "4.951671123504639 : Loss for Batch-> 267\n",
            "0.9829719066619873 : Loss for Batch-> 268\n",
            "0.002318876562640071 : Loss for Batch-> 269\n",
            "0.0005334962625056505 : Loss for Batch-> 270\n",
            "0.00198437855578959 : Loss for Batch-> 271\n",
            "0.1260225474834442 : Val Loss after training 271 batche(s)\n",
            "0.0008262602495960891 : Loss for Batch-> 272\n",
            "0.0020149073097854853 : Loss for Batch-> 273\n",
            "0.003481709398329258 : Loss for Batch-> 274\n",
            "0.0006942169275134802 : Loss for Batch-> 275\n",
            "0.017953870818018913 : Loss for Batch-> 276\n",
            "0.004765757825225592 : Loss for Batch-> 277\n",
            "0.049148473888635635 : Loss for Batch-> 278\n",
            "0.07045523077249527 : Loss for Batch-> 279\n",
            "0.06725894659757614 : Loss for Batch-> 280\n",
            "0.06331475079059601 : Loss for Batch-> 281\n",
            "0.12506361305713654 : Val Loss after training 281 batche(s)\n",
            "0.015098728239536285 : Loss for Batch-> 282\n",
            "1.4981483221054077 : Loss for Batch-> 283\n",
            "12.595433235168457 : Loss for Batch-> 284\n",
            "0.29442283511161804 : Loss for Batch-> 285\n",
            "0.011357955634593964 : Loss for Batch-> 286\n",
            "0.0021767443977296352 : Loss for Batch-> 287\n",
            "0.09994472563266754 : Loss for Batch-> 288\n",
            "0.0798841267824173 : Loss for Batch-> 289\n",
            "0.00424589216709137 : Loss for Batch-> 290\n",
            "0.00069191784132272 : Loss for Batch-> 291\n",
            "0.04877099394798279 : Val Loss after training 291 batche(s)\n",
            "0.0015995774883776903 : Loss for Batch-> 292\n",
            "0.036279574036598206 : Loss for Batch-> 293\n",
            "0.035722386091947556 : Loss for Batch-> 294\n",
            "0.0005729245604015887 : Loss for Batch-> 295\n",
            "0.04252241179347038 : Loss for Batch-> 296\n",
            "0.07998760044574738 : Loss for Batch-> 297\n",
            "1.200244426727295 : Loss for Batch-> 298\n",
            "1.0294891595840454 : Loss for Batch-> 299\n",
            "0.3680192232131958 : Loss for Batch-> 300\n",
            "0.09620615094900131 : Loss for Batch-> 301\n",
            "0.11830656975507736 : Val Loss after training 301 batche(s)\n",
            "0.03317326679825783 : Loss for Batch-> 302\n",
            "1.619157075881958 : Loss for Batch-> 303\n",
            "1.7500126361846924 : Loss for Batch-> 304\n",
            "0.5707414150238037 : Loss for Batch-> 305\n",
            "0.05765119940042496 : Loss for Batch-> 306\n",
            "0.032076407223939896 : Loss for Batch-> 307\n",
            "0.6723465919494629 : Loss for Batch-> 308\n",
            "1.5698436498641968 : Loss for Batch-> 309\n",
            "0.7478811740875244 : Loss for Batch-> 310\n",
            "0.01790487766265869 : Loss for Batch-> 311\n",
            "0.03393392637372017 : Val Loss after training 311 batche(s)\n",
            "0.004968335386365652 : Loss for Batch-> 312\n",
            "0.22942586243152618 : Loss for Batch-> 313\n",
            "0.40820398926734924 : Loss for Batch-> 314\n",
            "0.016369730234146118 : Loss for Batch-> 315\n",
            "0.025321047753095627 : Loss for Batch-> 316\n",
            "0.18639296293258667 : Loss for Batch-> 317\n",
            "0.15538033843040466 : Loss for Batch-> 318\n",
            "0.23358120024204254 : Loss for Batch-> 319\n",
            "0.2647567391395569 : Loss for Batch-> 320\n",
            "0.04621654376387596 : Loss for Batch-> 321\n",
            "0.03939170762896538 : Val Loss after training 321 batche(s)\n",
            "0.11123380810022354 : Loss for Batch-> 322\n",
            "0.2533005177974701 : Loss for Batch-> 323\n",
            "0.4007168710231781 : Loss for Batch-> 324\n",
            "0.46482494473457336 : Loss for Batch-> 325\n",
            "0.19121910631656647 : Loss for Batch-> 326\n",
            "0.07436499744653702 : Loss for Batch-> 327\n",
            "0.08312603831291199 : Loss for Batch-> 328\n",
            "0.030298175290226936 : Loss for Batch-> 329\n",
            "0.007677709683775902 : Loss for Batch-> 330\n",
            "0.007187327370047569 : Loss for Batch-> 331\n",
            "0.0508270300924778 : Val Loss after training 331 batche(s)\n",
            "0.00028840062441304326 : Loss for Batch-> 332\n",
            "0.0006132559501565993 : Loss for Batch-> 333\n",
            "0.001283008954487741 : Loss for Batch-> 334\n",
            "0.002680085599422455 : Loss for Batch-> 335\n",
            "0.01401490718126297 : Loss for Batch-> 336\n",
            "0.05795080587267876 : Loss for Batch-> 337\n",
            "0.06854862719774246 : Loss for Batch-> 338\n",
            "0.043661776930093765 : Loss for Batch-> 339\n",
            "0.2374875694513321 : Loss for Batch-> 340\n",
            "0.3489917814731598 : Loss for Batch-> 341\n",
            "0.04241292178630829 : Val Loss after training 341 batche(s)\n",
            "0.008743965066969395 : Loss for Batch-> 342\n",
            "0.01887848787009716 : Loss for Batch-> 343\n",
            "0.15575754642486572 : Loss for Batch-> 344\n",
            "0.415335476398468 : Loss for Batch-> 345\n",
            "0.19592514634132385 : Loss for Batch-> 346\n",
            "0.00218083499930799 : Loss for Batch-> 347\n",
            "0.5473498702049255 : Loss for Batch-> 348\n",
            "0.8728601932525635 : Loss for Batch-> 349\n",
            "0.08932958543300629 : Loss for Batch-> 350\n",
            "0.025317028164863586 : Loss for Batch-> 351\n",
            "0.039379142224788666 : Val Loss after training 351 batche(s)\n",
            "0.931139349937439 : Loss for Batch-> 352\n",
            "0.9872387051582336 : Loss for Batch-> 353\n",
            "0.004758544731885195 : Loss for Batch-> 354\n",
            "0.008647696115076542 : Loss for Batch-> 355\n",
            "0.0028299414552748203 : Loss for Batch-> 356\n",
            "0.5527592897415161 : Loss for Batch-> 357\n",
            "0.04504439979791641 : Loss for Batch-> 358\n",
            "0.18973888456821442 : Loss for Batch-> 359\n",
            "0.05620396509766579 : Loss for Batch-> 360\n",
            "0.11990999430418015 : Loss for Batch-> 361\n",
            "0.038947321474552155 : Val Loss after training 361 batche(s)\n",
            "0.283234566450119 : Loss for Batch-> 362\n",
            "0.23036934435367584 : Loss for Batch-> 363\n",
            "0.15010027587413788 : Loss for Batch-> 364\n",
            "0.2462746500968933 : Loss for Batch-> 365\n",
            "0.01426771841943264 : Loss for Batch-> 366\n",
            "0.014281164854764938 : Loss for Batch-> 367\n",
            "0.061402931809425354 : Loss for Batch-> 368\n",
            "0.15363863110542297 : Loss for Batch-> 369\n",
            "0.7740619778633118 : Loss for Batch-> 370\n",
            "0.10350637137889862 : Loss for Batch-> 371\n",
            "0.038039520382881165 : Val Loss after training 371 batche(s)\n",
            "0.8924960494041443 : Loss for Batch-> 372\n",
            "1.0479412078857422 : Loss for Batch-> 373\n",
            "0.1640092134475708 : Loss for Batch-> 374\n",
            "0.499276727437973 : Loss for Batch-> 375\n",
            "0.09062820672988892 : Loss for Batch-> 376\n",
            "0.02212410233914852 : Loss for Batch-> 377\n",
            "0.049252282828092575 : Loss for Batch-> 378\n",
            "0.19329923391342163 : Loss for Batch-> 379\n",
            "0.44299983978271484 : Loss for Batch-> 380\n",
            "0.47664129734039307 : Loss for Batch-> 381\n",
            "0.02943943254649639 : Val Loss after training 381 batche(s)\n",
            "1.6292861700057983 : Loss for Batch-> 382\n",
            "0.9637396931648254 : Loss for Batch-> 383\n",
            "0.9100749492645264 : Loss for Batch-> 384\n",
            "0.18902665376663208 : Loss for Batch-> 385\n",
            "0.004818764980882406 : Loss for Batch-> 386\n",
            "0.09766722470521927 : Loss for Batch-> 387\n",
            "1.0419350862503052 : Loss for Batch-> 388\n",
            "1.3540421724319458 : Loss for Batch-> 389\n",
            "0.23809897899627686 : Loss for Batch-> 390\n",
            "0.0903504267334938 : Loss for Batch-> 391\n",
            "0.033202819526195526 : Val Loss after training 391 batche(s)\n",
            "0.03872794285416603 : Loss for Batch-> 392\n",
            "0.05641983449459076 : Loss for Batch-> 393\n",
            "0.4096265137195587 : Loss for Batch-> 394\n",
            "0.1954212635755539 : Loss for Batch-> 395\n",
            "1.0589179992675781 : Loss for Batch-> 396\n",
            "0.384424090385437 : Loss for Batch-> 397\n",
            "0.00697542866691947 : Loss for Batch-> 398\n",
            "0.00860726460814476 : Loss for Batch-> 399\n",
            "0.018060510978102684 : Loss for Batch-> 400\n",
            "0.01107662171125412 : Loss for Batch-> 401\n",
            "0.0384625680744648 : Val Loss after training 401 batche(s)\n",
            "0.42305439710617065 : Loss for Batch-> 402\n",
            "0.5478675365447998 : Loss for Batch-> 403\n",
            "0.2017688900232315 : Loss for Batch-> 404\n",
            "0.07226511836051941 : Loss for Batch-> 405\n",
            "0.04102485254406929 : Loss for Batch-> 406\n",
            "0.009383019991219044 : Loss for Batch-> 407\n",
            "0.08562316745519638 : Loss for Batch-> 408\n",
            "0.8896796703338623 : Loss for Batch-> 409\n",
            "1.086796522140503 : Loss for Batch-> 410\n",
            "0.0750339925289154 : Loss for Batch-> 411\n",
            "0.02895345911383629 : Val Loss after training 411 batche(s)\n",
            "0.11847517639398575 : Loss for Batch-> 412\n",
            "0.0014478437369689345 : Loss for Batch-> 413\n",
            "0.0017813832964748144 : Loss for Batch-> 414\n",
            "0.0027219951152801514 : Loss for Batch-> 415\n",
            "0.025026679039001465 : Loss for Batch-> 416\n",
            "0.023703683167696 : Loss for Batch-> 417\n",
            "0.007445684168487787 : Loss for Batch-> 418\n",
            "0.20009945333003998 : Loss for Batch-> 419\n",
            "0.06618859618902206 : Loss for Batch-> 420\n",
            "0.025006113573908806 : Loss for Batch-> 421\n",
            "0.0003430001379456371 : Val Loss after training 421 batche(s)\n",
            "0.02297574281692505 : Loss for Batch-> 422\n",
            "0.022238442674279213 : Loss for Batch-> 423\n",
            "0.022724227979779243 : Loss for Batch-> 424\n",
            "0.024069515988230705 : Loss for Batch-> 425\n",
            "0.02416740357875824 : Loss for Batch-> 426\n",
            "0.023839080706238747 : Loss for Batch-> 427\n",
            "0.023557117208838463 : Loss for Batch-> 428\n",
            "0.022764461115002632 : Loss for Batch-> 429\n",
            "0.1049821600317955 : Loss for Batch-> 430\n",
            "0.07262559235095978 : Loss for Batch-> 431\n",
            "0.06345545500516891 : Val Loss after training 431 batche(s)\n",
            "0.014449141919612885 : Loss for Batch-> 432\n",
            "0.004229357931762934 : Loss for Batch-> 433\n",
            "0.0014597555855289102 : Loss for Batch-> 434\n",
            "0.0005402767565101385 : Loss for Batch-> 435\n",
            "0.0025016791187226772 : Loss for Batch-> 436\n",
            "0.002178465947508812 : Loss for Batch-> 437\n",
            "0.056595105677843094 : Loss for Batch-> 438\n",
            "0.05574355274438858 : Loss for Batch-> 439\n",
            "0.008531003259122372 : Loss for Batch-> 440\n",
            "0.024829532951116562 : Loss for Batch-> 441\n",
            "0.08634499460458755 : Val Loss after training 441 batche(s)\n",
            "0.012141295708715916 : Loss for Batch-> 442\n",
            "0.046526625752449036 : Loss for Batch-> 443\n",
            "0.006690661422908306 : Loss for Batch-> 444\n",
            "0.21020273864269257 : Loss for Batch-> 445\n",
            "0.32799190282821655 : Loss for Batch-> 446\n",
            "0.5401824712753296 : Loss for Batch-> 447\n",
            "0.0007866779924370348 : Loss for Batch-> 448\n",
            "0.0014643945032730699 : Loss for Batch-> 449\n",
            "0.000516944273840636 : Loss for Batch-> 450\n",
            "0.0009120281902141869 : Loss for Batch-> 451\n",
            "0.02120700106024742 : Val Loss after training 451 batche(s)\n",
            "0.0012552061816677451 : Loss for Batch-> 452\n",
            "0.00015537226863671094 : Loss for Batch-> 453\n",
            "0.0011935271322727203 : Loss for Batch-> 454\n",
            "0.00015111941320355982 : Loss for Batch-> 455\n",
            "0.00015193730359897017 : Loss for Batch-> 456\n",
            "0.00015014922246336937 : Loss for Batch-> 457\n",
            "0.00015044980682432652 : Loss for Batch-> 458\n",
            "0.0001502304949099198 : Loss for Batch-> 459\n",
            "0.00016597038484178483 : Loss for Batch-> 460\n",
            "0.00011570136848604307 : Loss for Batch-> 461\n",
            "0.037783559411764145 : Val Loss after training 461 batche(s)\n",
            "0.00011829390859929845 : Loss for Batch-> 462\n",
            "0.00011646927305264398 : Loss for Batch-> 463\n",
            "0.0010651007760316133 : Loss for Batch-> 464\n",
            "0.0010597256477922201 : Loss for Batch-> 465\n",
            "0.000529616023413837 : Loss for Batch-> 466\n",
            "0.0010901357745751739 : Loss for Batch-> 467\n",
            "0.0005532156792469323 : Loss for Batch-> 468\n",
            "0.00034163001691922545 : Loss for Batch-> 469\n",
            "0.00041514463373459876 : Loss for Batch-> 470\n",
            "0.0008753992733545601 : Loss for Batch-> 471\n",
            "0.12000728398561478 : Val Loss after training 471 batche(s)\n",
            "0.0006111048860475421 : Loss for Batch-> 472\n",
            "0.0010519885690882802 : Loss for Batch-> 473\n",
            "0.0006007124902680516 : Loss for Batch-> 474\n",
            "0.0010717835975810885 : Loss for Batch-> 475\n",
            "0.0006441663717851043 : Loss for Batch-> 476\n",
            "0.0004834110732190311 : Loss for Batch-> 477\n",
            "0.00046586041571572423 : Loss for Batch-> 478\n",
            "0.0019289484480395913 : Loss for Batch-> 479\n",
            "0.02341303601861 : Loss for Batch-> 480\n",
            "0.00885157659649849 : Loss for Batch-> 481\n",
            "5.336085319519043 : Val Loss after training 481 batche(s)\n",
            "0.003878545481711626 : Loss for Batch-> 482\n",
            "0.0035660520661622286 : Loss for Batch-> 483\n",
            "0.003669064724817872 : Loss for Batch-> 484\n",
            "0.003699744353070855 : Loss for Batch-> 485\n",
            "0.003587318118661642 : Loss for Batch-> 486\n",
            "0.0035815993323922157 : Loss for Batch-> 487\n",
            "0.0037174499593675137 : Loss for Batch-> 488\n",
            "0.0023359202314168215 : Loss for Batch-> 489\n",
            "0.0021705415565520525 : Loss for Batch-> 490\n",
            "0.002106421161442995 : Loss for Batch-> 491\n",
            "0.04216692969202995 : Val Loss after training 491 batche(s)\n",
            "0.03414809703826904 : Loss for Batch-> 492\n",
            "1.723677158355713 : Loss for Batch-> 493\n",
            "0.004989911802113056 : Loss for Batch-> 494\n",
            "0.018525665625929832 : Loss for Batch-> 495\n",
            "0.0035298827569931746 : Loss for Batch-> 496\n",
            "0.0010050866985693574 : Loss for Batch-> 497\n",
            "0.0011596287367865443 : Loss for Batch-> 498\n",
            "0.002379101002588868 : Loss for Batch-> 499\n",
            "0.0035349996760487556 : Loss for Batch-> 500\n",
            "0.0329090841114521 : Loss for Batch-> 501\n",
            "0.08357353508472443 : Val Loss after training 501 batche(s)\n",
            "0.024694539606571198 : Loss for Batch-> 502\n",
            "0.00891755148768425 : Loss for Batch-> 503\n",
            "0.02002895064651966 : Loss for Batch-> 504\n",
            "0.0196150541305542 : Loss for Batch-> 505\n",
            "0.01872122474014759 : Loss for Batch-> 506\n",
            "0.015257242135703564 : Loss for Batch-> 507\n",
            "0.008414116688072681 : Loss for Batch-> 508\n",
            "0.008652831427752972 : Loss for Batch-> 509\n",
            "0.008793146349489689 : Loss for Batch-> 510\n",
            "23 Epoch\n",
            "0.008602160960435867 : Loss for Batch-> 1\n",
            "0.00207375455647707 : Val Loss after training 1 batche(s)\n",
            "0.008328345604240894 : Loss for Batch-> 2\n",
            "0.00810309313237667 : Loss for Batch-> 3\n",
            "0.007702118717133999 : Loss for Batch-> 4\n",
            "0.007610098924487829 : Loss for Batch-> 5\n",
            "0.011246872134506702 : Loss for Batch-> 6\n",
            "0.01766264997422695 : Loss for Batch-> 7\n",
            "0.017538027837872505 : Loss for Batch-> 8\n",
            "0.01825600117444992 : Loss for Batch-> 9\n",
            "0.9627426266670227 : Loss for Batch-> 10\n",
            "5.239408016204834 : Loss for Batch-> 11\n",
            "4.97410110256169e-05 : Val Loss after training 11 batche(s)\n",
            "0.001919719623401761 : Loss for Batch-> 12\n",
            "0.001724550616927445 : Loss for Batch-> 13\n",
            "0.0012993136188015342 : Loss for Batch-> 14\n",
            "0.0015302452957257628 : Loss for Batch-> 15\n",
            "0.0052088419906795025 : Loss for Batch-> 16\n",
            "0.010878745466470718 : Loss for Batch-> 17\n",
            "0.0010754867689684033 : Loss for Batch-> 18\n",
            "0.011108146980404854 : Loss for Batch-> 19\n",
            "0.021787211298942566 : Loss for Batch-> 20\n",
            "0.020590731874108315 : Loss for Batch-> 21\n",
            "0.003102390095591545 : Val Loss after training 21 batche(s)\n",
            "0.018279490992426872 : Loss for Batch-> 22\n",
            "0.019715555012226105 : Loss for Batch-> 23\n",
            "0.013857264071702957 : Loss for Batch-> 24\n",
            "0.0008320041233673692 : Loss for Batch-> 25\n",
            "0.0004388031375128776 : Loss for Batch-> 26\n",
            "0.0010281238937750459 : Loss for Batch-> 27\n",
            "0.004090441856533289 : Loss for Batch-> 28\n",
            "0.0025579442735761404 : Loss for Batch-> 29\n",
            "0.0013580411905422807 : Loss for Batch-> 30\n",
            "9.461781883146614e-05 : Loss for Batch-> 31\n",
            "0.0007376063731499016 : Val Loss after training 31 batche(s)\n",
            "5.8920526498695835e-05 : Loss for Batch-> 32\n",
            "3.141156048513949e-05 : Loss for Batch-> 33\n",
            "0.0007332332897931337 : Loss for Batch-> 34\n",
            "0.0005316847818903625 : Loss for Batch-> 35\n",
            "0.0002623052569106221 : Loss for Batch-> 36\n",
            "0.012326058931648731 : Loss for Batch-> 37\n",
            "0.002748357132077217 : Loss for Batch-> 38\n",
            "0.00019870512187480927 : Loss for Batch-> 39\n",
            "0.0006963259656913579 : Loss for Batch-> 40\n",
            "0.00048648801748640835 : Loss for Batch-> 41\n",
            "0.0011022118851542473 : Val Loss after training 41 batche(s)\n",
            "0.0005844926927238703 : Loss for Batch-> 42\n",
            "0.05969667062163353 : Loss for Batch-> 43\n",
            "0.0628916323184967 : Loss for Batch-> 44\n",
            "0.06838104128837585 : Loss for Batch-> 45\n",
            "0.014853197149932384 : Loss for Batch-> 46\n",
            "0.018310844898223877 : Loss for Batch-> 47\n",
            "0.22744129598140717 : Loss for Batch-> 48\n",
            "0.2544429898262024 : Loss for Batch-> 49\n",
            "0.12744085490703583 : Loss for Batch-> 50\n",
            "0.0008947441237978637 : Loss for Batch-> 51\n",
            "0.0020568480249494314 : Val Loss after training 51 batche(s)\n",
            "0.0008022702531889081 : Loss for Batch-> 52\n",
            "0.012563342228531837 : Loss for Batch-> 53\n",
            "0.016694914549589157 : Loss for Batch-> 54\n",
            "0.015530489385128021 : Loss for Batch-> 55\n",
            "0.0018274975009262562 : Loss for Batch-> 56\n",
            "0.0017737004673108459 : Loss for Batch-> 57\n",
            "0.014562820084393024 : Loss for Batch-> 58\n",
            "0.16103266179561615 : Loss for Batch-> 59\n",
            "0.22946475446224213 : Loss for Batch-> 60\n",
            "0.15962478518486023 : Loss for Batch-> 61\n",
            "0.02329927124083042 : Val Loss after training 61 batche(s)\n",
            "0.0543677844107151 : Loss for Batch-> 62\n",
            "0.09304248541593552 : Loss for Batch-> 63\n",
            "0.013081032782793045 : Loss for Batch-> 64\n",
            "0.13966278731822968 : Loss for Batch-> 65\n",
            "0.04854750260710716 : Loss for Batch-> 66\n",
            "0.08321097493171692 : Loss for Batch-> 67\n",
            "0.01126107294112444 : Loss for Batch-> 68\n",
            "0.011554685421288013 : Loss for Batch-> 69\n",
            "0.011238115839660168 : Loss for Batch-> 70\n",
            "0.01029932964593172 : Loss for Batch-> 71\n",
            "0.00019231536134611815 : Val Loss after training 71 batche(s)\n",
            "0.010800271295011044 : Loss for Batch-> 72\n",
            "0.010503080673515797 : Loss for Batch-> 73\n",
            "0.024338966235518456 : Loss for Batch-> 74\n",
            "1.1456375122070312 : Loss for Batch-> 75\n",
            "1.4204756021499634 : Loss for Batch-> 76\n",
            "0.06605102121829987 : Loss for Batch-> 77\n",
            "0.07389339804649353 : Loss for Batch-> 78\n",
            "0.08562414348125458 : Loss for Batch-> 79\n",
            "0.0035569635219872 : Loss for Batch-> 80\n",
            "0.0020237138960510492 : Loss for Batch-> 81\n",
            "0.0029773178976029158 : Val Loss after training 81 batche(s)\n",
            "0.06364813446998596 : Loss for Batch-> 82\n",
            "0.08398141711950302 : Loss for Batch-> 83\n",
            "0.08773107826709747 : Loss for Batch-> 84\n",
            "0.06028251349925995 : Loss for Batch-> 85\n",
            "0.02235580049455166 : Loss for Batch-> 86\n",
            "0.01334395445883274 : Loss for Batch-> 87\n",
            "0.046823881566524506 : Loss for Batch-> 88\n",
            "0.020226506516337395 : Loss for Batch-> 89\n",
            "0.00585074070841074 : Loss for Batch-> 90\n",
            "0.04801562428474426 : Loss for Batch-> 91\n",
            "0.0015308603178709745 : Val Loss after training 91 batche(s)\n",
            "0.028670435771346092 : Loss for Batch-> 92\n",
            "0.0017528843600302935 : Loss for Batch-> 93\n",
            "0.018844669684767723 : Loss for Batch-> 94\n",
            "0.0013843629276379943 : Loss for Batch-> 95\n",
            "0.0007118075154721737 : Loss for Batch-> 96\n",
            "0.0018873033113777637 : Loss for Batch-> 97\n",
            "0.007212625816464424 : Loss for Batch-> 98\n",
            "0.0014269925886765122 : Loss for Batch-> 99\n",
            "0.0030103663448244333 : Loss for Batch-> 100\n",
            "0.002107546664774418 : Loss for Batch-> 101\n",
            "0.04970414936542511 : Val Loss after training 101 batche(s)\n",
            "0.01365721970796585 : Loss for Batch-> 102\n",
            "0.01841386966407299 : Loss for Batch-> 103\n",
            "0.01807563565671444 : Loss for Batch-> 104\n",
            "0.019443226978182793 : Loss for Batch-> 105\n",
            "0.0008233735570684075 : Loss for Batch-> 106\n",
            "0.0020425142720341682 : Loss for Batch-> 107\n",
            "0.02110959216952324 : Loss for Batch-> 108\n",
            "0.018520675599575043 : Loss for Batch-> 109\n",
            "0.0046854387037456036 : Loss for Batch-> 110\n",
            "0.0018331242026761174 : Loss for Batch-> 111\n",
            "0.025950288400053978 : Val Loss after training 111 batche(s)\n",
            "0.002526977565139532 : Loss for Batch-> 112\n",
            "0.046919263899326324 : Loss for Batch-> 113\n",
            "0.0466138981282711 : Loss for Batch-> 114\n",
            "0.03399075195193291 : Loss for Batch-> 115\n",
            "0.00443458603695035 : Loss for Batch-> 116\n",
            "0.0747629925608635 : Loss for Batch-> 117\n",
            "0.005474178120493889 : Loss for Batch-> 118\n",
            "0.011574553325772285 : Loss for Batch-> 119\n",
            "0.012670806609094143 : Loss for Batch-> 120\n",
            "0.07138571888208389 : Loss for Batch-> 121\n",
            "0.001323097851127386 : Val Loss after training 121 batche(s)\n",
            "0.0847906768321991 : Loss for Batch-> 122\n",
            "0.03263692930340767 : Loss for Batch-> 123\n",
            "0.015651760622859 : Loss for Batch-> 124\n",
            "0.02790232002735138 : Loss for Batch-> 125\n",
            "0.002329410519450903 : Loss for Batch-> 126\n",
            "0.0029816380701959133 : Loss for Batch-> 127\n",
            "0.03416771814227104 : Loss for Batch-> 128\n",
            "0.051409777253866196 : Loss for Batch-> 129\n",
            "0.11129631102085114 : Loss for Batch-> 130\n",
            "0.0368291474878788 : Loss for Batch-> 131\n",
            "0.00048456128570251167 : Val Loss after training 131 batche(s)\n",
            "0.017812056466937065 : Loss for Batch-> 132\n",
            "0.05722184479236603 : Loss for Batch-> 133\n",
            "0.012753310613334179 : Loss for Batch-> 134\n",
            "0.279097318649292 : Loss for Batch-> 135\n",
            "0.03856687247753143 : Loss for Batch-> 136\n",
            "0.02864532545208931 : Loss for Batch-> 137\n",
            "0.0004288858617655933 : Loss for Batch-> 138\n",
            "0.000949468114413321 : Loss for Batch-> 139\n",
            "0.00046674133045598865 : Loss for Batch-> 140\n",
            "0.002724279183894396 : Loss for Batch-> 141\n",
            "0.09795556217432022 : Val Loss after training 141 batche(s)\n",
            "0.010993074625730515 : Loss for Batch-> 142\n",
            "0.01835106499493122 : Loss for Batch-> 143\n",
            "0.01127552054822445 : Loss for Batch-> 144\n",
            "0.010617940686643124 : Loss for Batch-> 145\n",
            "0.002217909786850214 : Loss for Batch-> 146\n",
            "0.001846560975536704 : Loss for Batch-> 147\n",
            "0.0013140967348590493 : Loss for Batch-> 148\n",
            "0.03651966154575348 : Loss for Batch-> 149\n",
            "0.06940333545207977 : Loss for Batch-> 150\n",
            "0.008554072119295597 : Loss for Batch-> 151\n",
            "0.2539525330066681 : Val Loss after training 151 batche(s)\n",
            "0.02689143270254135 : Loss for Batch-> 152\n",
            "0.04367648437619209 : Loss for Batch-> 153\n",
            "0.024812160059809685 : Loss for Batch-> 154\n",
            "0.0035078832879662514 : Loss for Batch-> 155\n",
            "0.060054488480091095 : Loss for Batch-> 156\n",
            "0.0029567398596554995 : Loss for Batch-> 157\n",
            "0.011504130437970161 : Loss for Batch-> 158\n",
            "0.00785109307616949 : Loss for Batch-> 159\n",
            "0.07198674231767654 : Loss for Batch-> 160\n",
            "0.050437431782484055 : Loss for Batch-> 161\n",
            "0.11821327358484268 : Val Loss after training 161 batche(s)\n",
            "0.007841435261070728 : Loss for Batch-> 162\n",
            "0.0453321598470211 : Loss for Batch-> 163\n",
            "0.04559556394815445 : Loss for Batch-> 164\n",
            "0.050286032259464264 : Loss for Batch-> 165\n",
            "0.015577808953821659 : Loss for Batch-> 166\n",
            "0.028396038338541985 : Loss for Batch-> 167\n",
            "0.010647743940353394 : Loss for Batch-> 168\n",
            "0.000243983551627025 : Loss for Batch-> 169\n",
            "0.0010271023493260145 : Loss for Batch-> 170\n",
            "0.0014319908805191517 : Loss for Batch-> 171\n",
            "0.0018794465577229857 : Val Loss after training 171 batche(s)\n",
            "0.00031201689853332937 : Loss for Batch-> 172\n",
            "0.00033095505204983056 : Loss for Batch-> 173\n",
            "0.0013056311290711164 : Loss for Batch-> 174\n",
            "0.02054760977625847 : Loss for Batch-> 175\n",
            "0.011455124244093895 : Loss for Batch-> 176\n",
            "0.000523930590134114 : Loss for Batch-> 177\n",
            "0.00048752551083453 : Loss for Batch-> 178\n",
            "5.060218245489523e-05 : Loss for Batch-> 179\n",
            "7.53216227167286e-06 : Loss for Batch-> 180\n",
            "0.00017202146409545094 : Loss for Batch-> 181\n",
            "0.005961242131888866 : Val Loss after training 181 batche(s)\n",
            "0.0005254367715679109 : Loss for Batch-> 182\n",
            "0.00016237636737059802 : Loss for Batch-> 183\n",
            "0.0010076933540403843 : Loss for Batch-> 184\n",
            "0.00012455609976314008 : Loss for Batch-> 185\n",
            "0.00046516471775248647 : Loss for Batch-> 186\n",
            "0.0006128291133791208 : Loss for Batch-> 187\n",
            "0.0012776550138369203 : Loss for Batch-> 188\n",
            "0.015476430766284466 : Loss for Batch-> 189\n",
            "0.02977728098630905 : Loss for Batch-> 190\n",
            "0.00559988571330905 : Loss for Batch-> 191\n",
            "0.005245894659310579 : Val Loss after training 191 batche(s)\n",
            "0.00236649252474308 : Loss for Batch-> 192\n",
            "0.0007878260803408921 : Loss for Batch-> 193\n",
            "0.00021350098541006446 : Loss for Batch-> 194\n",
            "0.0008578833076171577 : Loss for Batch-> 195\n",
            "0.9366567730903625 : Loss for Batch-> 196\n",
            "2.1864216327667236 : Loss for Batch-> 197\n",
            "0.039823614060878754 : Loss for Batch-> 198\n",
            "0.020295826718211174 : Loss for Batch-> 199\n",
            "0.07955751568078995 : Loss for Batch-> 200\n",
            "0.020469367504119873 : Loss for Batch-> 201\n",
            "0.005548116751015186 : Val Loss after training 201 batche(s)\n",
            "0.010335832834243774 : Loss for Batch-> 202\n",
            "0.022658638656139374 : Loss for Batch-> 203\n",
            "0.01066464651376009 : Loss for Batch-> 204\n",
            "0.006588788703083992 : Loss for Batch-> 205\n",
            "0.0057906401343643665 : Loss for Batch-> 206\n",
            "0.006605358328670263 : Loss for Batch-> 207\n",
            "0.04043532907962799 : Loss for Batch-> 208\n",
            "0.008297333493828773 : Loss for Batch-> 209\n",
            "0.015538729727268219 : Loss for Batch-> 210\n",
            "0.02899249643087387 : Loss for Batch-> 211\n",
            "0.005919624585658312 : Val Loss after training 211 batche(s)\n",
            "0.018339814618229866 : Loss for Batch-> 212\n",
            "0.0518748015165329 : Loss for Batch-> 213\n",
            "0.005148340482264757 : Loss for Batch-> 214\n",
            "0.0005989021738059819 : Loss for Batch-> 215\n",
            "0.0005236059078015387 : Loss for Batch-> 216\n",
            "0.030003545805811882 : Loss for Batch-> 217\n",
            "0.07243610918521881 : Loss for Batch-> 218\n",
            "0.013769706711173058 : Loss for Batch-> 219\n",
            "0.06496211141347885 : Loss for Batch-> 220\n",
            "0.11553306877613068 : Loss for Batch-> 221\n",
            "0.0401594303548336 : Val Loss after training 221 batche(s)\n",
            "0.0008069992181845009 : Loss for Batch-> 222\n",
            "0.00046268978621810675 : Loss for Batch-> 223\n",
            "0.00044826953671872616 : Loss for Batch-> 224\n",
            "0.012349949218332767 : Loss for Batch-> 225\n",
            "2.420452833175659 : Loss for Batch-> 226\n",
            "0.30977973341941833 : Loss for Batch-> 227\n",
            "11.129389762878418 : Loss for Batch-> 228\n",
            "0.14535358548164368 : Loss for Batch-> 229\n",
            "0.27505508065223694 : Loss for Batch-> 230\n",
            "0.031495142728090286 : Loss for Batch-> 231\n",
            "3.0839552879333496 : Val Loss after training 231 batche(s)\n",
            "0.002301307860761881 : Loss for Batch-> 232\n",
            "0.06146395578980446 : Loss for Batch-> 233\n",
            "0.006976859178394079 : Loss for Batch-> 234\n",
            "0.001431231969036162 : Loss for Batch-> 235\n",
            "0.0010408884845674038 : Loss for Batch-> 236\n",
            "0.03850837051868439 : Loss for Batch-> 237\n",
            "0.11246491223573685 : Loss for Batch-> 238\n",
            "0.11174255609512329 : Loss for Batch-> 239\n",
            "0.16486963629722595 : Loss for Batch-> 240\n",
            "0.7782953381538391 : Loss for Batch-> 241\n",
            "0.0008625098271295428 : Val Loss after training 241 batche(s)\n",
            "2.644606590270996 : Loss for Batch-> 242\n",
            "2.301981210708618 : Loss for Batch-> 243\n",
            "0.026746775954961777 : Loss for Batch-> 244\n",
            "0.2219170182943344 : Loss for Batch-> 245\n",
            "4.209620475769043 : Loss for Batch-> 246\n",
            "0.22023051977157593 : Loss for Batch-> 247\n",
            "1.014276146888733 : Loss for Batch-> 248\n",
            "0.01250603049993515 : Loss for Batch-> 249\n",
            "0.30163127183914185 : Loss for Batch-> 250\n",
            "0.4427785873413086 : Loss for Batch-> 251\n",
            "0.0007626146543771029 : Val Loss after training 251 batche(s)\n",
            "0.4919818639755249 : Loss for Batch-> 252\n",
            "2.873952627182007 : Loss for Batch-> 253\n",
            "6.542038440704346 : Loss for Batch-> 254\n",
            "0.052606578916311264 : Loss for Batch-> 255\n",
            "0.029720090329647064 : Loss for Batch-> 256\n",
            "0.6533985137939453 : Loss for Batch-> 257\n",
            "1.5039634704589844 : Loss for Batch-> 258\n",
            "3.7623443603515625 : Loss for Batch-> 259\n",
            "1.2036508321762085 : Loss for Batch-> 260\n",
            "0.009045320563018322 : Loss for Batch-> 261\n",
            "0.020870668813586235 : Val Loss after training 261 batche(s)\n",
            "0.1895885020494461 : Loss for Batch-> 262\n",
            "1.6423349380493164 : Loss for Batch-> 263\n",
            "0.34292519092559814 : Loss for Batch-> 264\n",
            "0.10100656747817993 : Loss for Batch-> 265\n",
            "0.0003259855438955128 : Loss for Batch-> 266\n",
            "0.8673531413078308 : Loss for Batch-> 267\n",
            "5.122828483581543 : Loss for Batch-> 268\n",
            "0.011896614916622639 : Loss for Batch-> 269\n",
            "0.0005703945644199848 : Loss for Batch-> 270\n",
            "0.0012414177181199193 : Loss for Batch-> 271\n",
            "0.000877961574587971 : Val Loss after training 271 batche(s)\n",
            "0.001521992846392095 : Loss for Batch-> 272\n",
            "0.0021389033645391464 : Loss for Batch-> 273\n",
            "0.0016728478949517012 : Loss for Batch-> 274\n",
            "0.002519309753552079 : Loss for Batch-> 275\n",
            "0.0021589784882962704 : Loss for Batch-> 276\n",
            "0.018667705357074738 : Loss for Batch-> 277\n",
            "0.02126060426235199 : Loss for Batch-> 278\n",
            "0.059839267283678055 : Loss for Batch-> 279\n",
            "0.06392418593168259 : Loss for Batch-> 280\n",
            "0.07031837105751038 : Loss for Batch-> 281\n",
            "0.0008474051137454808 : Val Loss after training 281 batche(s)\n",
            "0.04639064893126488 : Loss for Batch-> 282\n",
            "0.008980837650597095 : Loss for Batch-> 283\n",
            "6.760766506195068 : Loss for Batch-> 284\n",
            "7.620037078857422 : Loss for Batch-> 285\n",
            "0.011809403076767921 : Loss for Batch-> 286\n",
            "0.0021297710482031107 : Loss for Batch-> 287\n",
            "0.012224502861499786 : Loss for Batch-> 288\n",
            "0.1525195837020874 : Loss for Batch-> 289\n",
            "0.018582269549369812 : Loss for Batch-> 290\n",
            "0.0029437029734253883 : Loss for Batch-> 291\n",
            "0.0024056779220700264 : Val Loss after training 291 batche(s)\n",
            "0.0014229102525860071 : Loss for Batch-> 292\n",
            "0.0021321335807442665 : Loss for Batch-> 293\n",
            "0.06340605765581131 : Loss for Batch-> 294\n",
            "0.007513988763093948 : Loss for Batch-> 295\n",
            "0.014771715737879276 : Loss for Batch-> 296\n",
            "0.028417307883501053 : Loss for Batch-> 297\n",
            "0.5078403949737549 : Loss for Batch-> 298\n",
            "1.1007359027862549 : Loss for Batch-> 299\n",
            "1.0384012460708618 : Loss for Batch-> 300\n",
            "0.06367233395576477 : Loss for Batch-> 301\n",
            "0.000935761199798435 : Val Loss after training 301 batche(s)\n",
            "0.06615956127643585 : Loss for Batch-> 302\n",
            "0.4404476284980774 : Loss for Batch-> 303\n",
            "2.0593044757843018 : Loss for Batch-> 304\n",
            "1.4486422538757324 : Loss for Batch-> 305\n",
            "0.04001680389046669 : Loss for Batch-> 306\n",
            "0.05479917675256729 : Loss for Batch-> 307\n",
            "0.030792217701673508 : Loss for Batch-> 308\n",
            "1.3985908031463623 : Loss for Batch-> 309\n",
            "1.4230775833129883 : Loss for Batch-> 310\n",
            "0.1610492318868637 : Loss for Batch-> 311\n",
            "0.011513207107782364 : Val Loss after training 311 batche(s)\n",
            "0.017192119732499123 : Loss for Batch-> 312\n",
            "0.0008888005977496505 : Loss for Batch-> 313\n",
            "0.5374768376350403 : Loss for Batch-> 314\n",
            "0.10521150380373001 : Loss for Batch-> 315\n",
            "0.025204766541719437 : Loss for Batch-> 316\n",
            "0.0595097579061985 : Loss for Batch-> 317\n",
            "0.21091556549072266 : Loss for Batch-> 318\n",
            "0.11106801778078079 : Loss for Batch-> 319\n",
            "0.3326241970062256 : Loss for Batch-> 320\n",
            "0.15277498960494995 : Loss for Batch-> 321\n",
            "0.01129700057208538 : Val Loss after training 321 batche(s)\n",
            "0.03681546077132225 : Loss for Batch-> 322\n",
            "0.21069565415382385 : Loss for Batch-> 323\n",
            "0.25185444951057434 : Loss for Batch-> 324\n",
            "0.5063945651054382 : Loss for Batch-> 325\n",
            "0.43559372425079346 : Loss for Batch-> 326\n",
            "0.017192132771015167 : Loss for Batch-> 327\n",
            "0.09673759341239929 : Loss for Batch-> 328\n",
            "0.07846780121326447 : Loss for Batch-> 329\n",
            "0.006971359718590975 : Loss for Batch-> 330\n",
            "0.009518388658761978 : Loss for Batch-> 331\n",
            "0.005662487354129553 : Val Loss after training 331 batche(s)\n",
            "0.002937654498964548 : Loss for Batch-> 332\n",
            "0.0004900762578472495 : Loss for Batch-> 333\n",
            "0.0010417307494208217 : Loss for Batch-> 334\n",
            "0.002606688067317009 : Loss for Batch-> 335\n",
            "0.0013088715495541692 : Loss for Batch-> 336\n",
            "0.05407886207103729 : Loss for Batch-> 337\n",
            "0.047351330518722534 : Loss for Batch-> 338\n",
            "0.05771705508232117 : Loss for Batch-> 339\n",
            "0.028671767562627792 : Loss for Batch-> 340\n",
            "0.514006495475769 : Loss for Batch-> 341\n",
            "0.004961921367794275 : Val Loss after training 341 batche(s)\n",
            "0.06918983906507492 : Loss for Batch-> 342\n",
            "0.01323157548904419 : Loss for Batch-> 343\n",
            "0.05441680923104286 : Loss for Batch-> 344\n",
            "0.25938960909843445 : Loss for Batch-> 345\n",
            "0.44243553280830383 : Loss for Batch-> 346\n",
            "0.02533119171857834 : Loss for Batch-> 347\n",
            "0.04702169448137283 : Loss for Batch-> 348\n",
            "0.8127147555351257 : Loss for Batch-> 349\n",
            "0.6218259334564209 : Loss for Batch-> 350\n",
            "0.042132314294576645 : Loss for Batch-> 351\n",
            "0.0013691261410713196 : Val Loss after training 351 batche(s)\n",
            "0.10137306898832321 : Loss for Batch-> 352\n",
            "1.6516653299331665 : Loss for Batch-> 353\n",
            "0.17824296653270721 : Loss for Batch-> 354\n",
            "0.009741473942995071 : Loss for Batch-> 355\n",
            "0.003977104555815458 : Loss for Batch-> 356\n",
            "0.10132329910993576 : Loss for Batch-> 357\n",
            "0.4647057354450226 : Loss for Batch-> 358\n",
            "0.044627733528614044 : Loss for Batch-> 359\n",
            "0.23209106922149658 : Loss for Batch-> 360\n",
            "0.02239573933184147 : Loss for Batch-> 361\n",
            "0.0006559204193763435 : Val Loss after training 361 batche(s)\n",
            "0.1463039368391037 : Loss for Batch-> 362\n",
            "0.45469966530799866 : Loss for Batch-> 363\n",
            "0.03860928863286972 : Loss for Batch-> 364\n",
            "0.252619206905365 : Loss for Batch-> 365\n",
            "0.13076886534690857 : Loss for Batch-> 366\n",
            "0.0024245891254395247 : Loss for Batch-> 367\n",
            "0.03515249118208885 : Loss for Batch-> 368\n",
            "0.04492134973406792 : Loss for Batch-> 369\n",
            "0.5278719067573547 : Loss for Batch-> 370\n",
            "0.46459007263183594 : Loss for Batch-> 371\n",
            "0.0017141274875029922 : Val Loss after training 371 batche(s)\n",
            "0.23725290596485138 : Loss for Batch-> 372\n",
            "1.1460131406784058 : Loss for Batch-> 373\n",
            "0.6114675998687744 : Loss for Batch-> 374\n",
            "0.4167795181274414 : Loss for Batch-> 375\n",
            "0.3119375705718994 : Loss for Batch-> 376\n",
            "0.007784184068441391 : Loss for Batch-> 377\n",
            "0.03451552614569664 : Loss for Batch-> 378\n",
            "0.08390085399150848 : Loss for Batch-> 379\n",
            "0.17014849185943604 : Loss for Batch-> 380\n",
            "0.7818572521209717 : Loss for Batch-> 381\n",
            "0.10365454852581024 : Val Loss after training 381 batche(s)\n",
            "0.2592160105705261 : Loss for Batch-> 382\n",
            "2.2088325023651123 : Loss for Batch-> 383\n",
            "0.5786147713661194 : Loss for Batch-> 384\n",
            "0.7314550280570984 : Loss for Batch-> 385\n",
            "0.026463620364665985 : Loss for Batch-> 386\n",
            "0.008607364259660244 : Loss for Batch-> 387\n",
            "0.3386582136154175 : Loss for Batch-> 388\n",
            "1.3406049013137817 : Loss for Batch-> 389\n",
            "1.0332664251327515 : Loss for Batch-> 390\n",
            "0.02841690368950367 : Loss for Batch-> 391\n",
            "0.12828627228736877 : Val Loss after training 391 batche(s)\n",
            "0.10550633072853088 : Loss for Batch-> 392\n",
            "0.007495603058487177 : Loss for Batch-> 393\n",
            "0.15468966960906982 : Loss for Batch-> 394\n",
            "0.326852947473526 : Loss for Batch-> 395\n",
            "0.4656527042388916 : Loss for Batch-> 396\n",
            "0.9842393398284912 : Loss for Batch-> 397\n",
            "0.17799916863441467 : Loss for Batch-> 398\n",
            "0.005456088576465845 : Loss for Batch-> 399\n",
            "0.007607689127326012 : Loss for Batch-> 400\n",
            "0.018874499946832657 : Loss for Batch-> 401\n",
            "0.0013019219040870667 : Val Loss after training 401 batche(s)\n",
            "0.08339687436819077 : Loss for Batch-> 402\n",
            "0.5888912081718445 : Loss for Batch-> 403\n",
            "0.4013781249523163 : Loss for Batch-> 404\n",
            "0.15233325958251953 : Loss for Batch-> 405\n",
            "0.04308803007006645 : Loss for Batch-> 406\n",
            "0.022718310356140137 : Loss for Batch-> 407\n",
            "0.02618330903351307 : Loss for Batch-> 408\n",
            "0.2936227023601532 : Loss for Batch-> 409\n",
            "1.2573366165161133 : Loss for Batch-> 410\n",
            "0.5454133749008179 : Loss for Batch-> 411\n",
            "0.0007632763008587062 : Val Loss after training 411 batche(s)\n",
            "0.11198548972606659 : Loss for Batch-> 412\n",
            "0.033877577632665634 : Loss for Batch-> 413\n",
            "0.0007513475138694048 : Loss for Batch-> 414\n",
            "0.0017629831563681364 : Loss for Batch-> 415\n",
            "0.01213665958493948 : Loss for Batch-> 416\n",
            "0.030024593695998192 : Loss for Batch-> 417\n",
            "0.00952670257538557 : Loss for Batch-> 418\n",
            "0.11472971737384796 : Loss for Batch-> 419\n",
            "0.1279405951499939 : Loss for Batch-> 420\n",
            "0.04101727157831192 : Loss for Batch-> 421\n",
            "0.0018741728272289038 : Val Loss after training 421 batche(s)\n",
            "0.023047199472784996 : Loss for Batch-> 422\n",
            "0.022316234186291695 : Loss for Batch-> 423\n",
            "0.02227889932692051 : Loss for Batch-> 424\n",
            "0.022781791165471077 : Loss for Batch-> 425\n",
            "0.024006860330700874 : Loss for Batch-> 426\n",
            "0.024213546887040138 : Loss for Batch-> 427\n",
            "0.023500408977270126 : Loss for Batch-> 428\n",
            "0.022839177399873734 : Loss for Batch-> 429\n",
            "0.022243039682507515 : Loss for Batch-> 430\n",
            "0.1461842954158783 : Loss for Batch-> 431\n",
            "0.0022842898033559322 : Val Loss after training 431 batche(s)\n",
            "0.035429395735263824 : Loss for Batch-> 432\n",
            "0.002635522512719035 : Loss for Batch-> 433\n",
            "0.0030864840373396873 : Loss for Batch-> 434\n",
            "0.0013081757351756096 : Loss for Batch-> 435\n",
            "0.0008213820401579142 : Loss for Batch-> 436\n",
            "0.0027675924357026815 : Loss for Batch-> 437\n",
            "0.009454816579818726 : Loss for Batch-> 438\n",
            "0.06492427736520767 : Loss for Batch-> 439\n",
            "0.04159286990761757 : Loss for Batch-> 440\n",
            "0.010637067258358002 : Loss for Batch-> 441\n",
            "0.003976795822381973 : Val Loss after training 441 batche(s)\n",
            "0.03083369880914688 : Loss for Batch-> 442\n",
            "0.021175945177674294 : Loss for Batch-> 443\n",
            "0.0329456590116024 : Loss for Batch-> 444\n",
            "0.021422334015369415 : Loss for Batch-> 445\n",
            "0.417575865983963 : Loss for Batch-> 446\n",
            "0.5403539538383484 : Loss for Batch-> 447\n",
            "0.10021396726369858 : Loss for Batch-> 448\n",
            "0.0010300371795892715 : Loss for Batch-> 449\n",
            "0.0010588051518425345 : Loss for Batch-> 450\n",
            "0.0003996819141320884 : Loss for Batch-> 451\n",
            "0.006041355896741152 : Val Loss after training 451 batche(s)\n",
            "0.001133039011619985 : Loss for Batch-> 452\n",
            "0.0010122775565832853 : Loss for Batch-> 453\n",
            "0.0007967069977894425 : Loss for Batch-> 454\n",
            "0.0005252851406112313 : Loss for Batch-> 455\n",
            "0.00013787629723083228 : Loss for Batch-> 456\n",
            "0.00013850301911588758 : Loss for Batch-> 457\n",
            "0.0001366133219562471 : Loss for Batch-> 458\n",
            "0.00013724945893045515 : Loss for Batch-> 459\n",
            "0.00016301572031807154 : Loss for Batch-> 460\n",
            "0.00015082972822710872 : Loss for Batch-> 461\n",
            "0.006259148940443993 : Val Loss after training 461 batche(s)\n",
            "0.00012999135651625693 : Loss for Batch-> 462\n",
            "0.0001308390055783093 : Loss for Batch-> 463\n",
            "0.00012980429164599627 : Loss for Batch-> 464\n",
            "0.001678874483332038 : Loss for Batch-> 465\n",
            "0.0006064342451281846 : Loss for Batch-> 466\n",
            "0.0005265852087177336 : Loss for Batch-> 467\n",
            "0.001405874965712428 : Loss for Batch-> 468\n",
            "8.402911043958738e-05 : Loss for Batch-> 469\n",
            "0.0004584330599755049 : Loss for Batch-> 470\n",
            "0.0005529567133635283 : Loss for Batch-> 471\n",
            "0.006280247122049332 : Val Loss after training 471 batche(s)\n",
            "0.0009010330541059375 : Loss for Batch-> 472\n",
            "0.0004107931745238602 : Loss for Batch-> 473\n",
            "0.001123014953918755 : Loss for Batch-> 474\n",
            "0.0006676864577457309 : Loss for Batch-> 475\n",
            "0.001265343395061791 : Loss for Batch-> 476\n",
            "0.00025182380340993404 : Loss for Batch-> 477\n",
            "0.0006224213284440339 : Loss for Batch-> 478\n",
            "0.0017642739694565535 : Loss for Batch-> 479\n",
            "0.0038966878782957792 : Loss for Batch-> 480\n",
            "0.02562396228313446 : Loss for Batch-> 481\n",
            "0.0061857267282903194 : Val Loss after training 481 batche(s)\n",
            "0.004875402897596359 : Loss for Batch-> 482\n",
            "0.00337867415510118 : Loss for Batch-> 483\n",
            "0.0034970345441251993 : Loss for Batch-> 484\n",
            "0.0036665189545601606 : Loss for Batch-> 485\n",
            "0.003557074349373579 : Loss for Batch-> 486\n",
            "0.0035168444737792015 : Loss for Batch-> 487\n",
            "0.003545816522091627 : Loss for Batch-> 488\n",
            "0.003230982692912221 : Loss for Batch-> 489\n",
            "0.0021193267311900854 : Loss for Batch-> 490\n",
            "0.0020956615917384624 : Loss for Batch-> 491\n",
            "0.00594071252271533 : Val Loss after training 491 batche(s)\n",
            "0.0020148069597780704 : Loss for Batch-> 492\n",
            "0.4559396505355835 : Loss for Batch-> 493\n",
            "1.3041834831237793 : Loss for Batch-> 494\n",
            "0.008587144315242767 : Loss for Batch-> 495\n",
            "0.015951013192534447 : Loss for Batch-> 496\n",
            "0.0009637995972298086 : Loss for Batch-> 497\n",
            "0.0011406323174014688 : Loss for Batch-> 498\n",
            "0.00199618237093091 : Loss for Batch-> 499\n",
            "0.0021229072008281946 : Loss for Batch-> 500\n",
            "0.005099013913422823 : Loss for Batch-> 501\n",
            "0.0032182964496314526 : Val Loss after training 501 batche(s)\n",
            "0.05045948550105095 : Loss for Batch-> 502\n",
            "0.005874495953321457 : Loss for Batch-> 503\n",
            "0.01551623921841383 : Loss for Batch-> 504\n",
            "0.020032649859786034 : Loss for Batch-> 505\n",
            "0.0192852895706892 : Loss for Batch-> 506\n",
            "0.01888098753988743 : Loss for Batch-> 507\n",
            "0.011334461160004139 : Loss for Batch-> 508\n",
            "0.008994091302156448 : Loss for Batch-> 509\n",
            "0.008853733539581299 : Loss for Batch-> 510\n",
            "24 Epoch\n",
            "0.008836137130856514 : Loss for Batch-> 1\n",
            "0.0004941276274621487 : Val Loss after training 1 batche(s)\n",
            "0.008661860600113869 : Loss for Batch-> 2\n",
            "0.008480336517095566 : Loss for Batch-> 3\n",
            "0.007895266637206078 : Loss for Batch-> 4\n",
            "0.00782378576695919 : Loss for Batch-> 5\n",
            "0.007771953474730253 : Loss for Batch-> 6\n",
            "0.015377985313534737 : Loss for Batch-> 7\n",
            "0.017630739137530327 : Loss for Batch-> 8\n",
            "0.018670404329895973 : Loss for Batch-> 9\n",
            "0.01729402504861355 : Loss for Batch-> 10\n",
            "3.492875099182129 : Loss for Batch-> 11\n",
            "0.0017602067673578858 : Val Loss after training 11 batche(s)\n",
            "2.707446336746216 : Loss for Batch-> 12\n",
            "0.001563051249831915 : Loss for Batch-> 13\n",
            "0.0011593897361308336 : Loss for Batch-> 14\n",
            "0.001699958462268114 : Loss for Batch-> 15\n",
            "0.0015413496876135468 : Loss for Batch-> 16\n",
            "0.01228925958275795 : Loss for Batch-> 17\n",
            "0.003524115774780512 : Loss for Batch-> 18\n",
            "0.0009179979097098112 : Loss for Batch-> 19\n",
            "0.020618973299860954 : Loss for Batch-> 20\n",
            "0.02032654359936714 : Loss for Batch-> 21\n",
            "0.0011691825930029154 : Val Loss after training 21 batche(s)\n",
            "0.021248245611786842 : Loss for Batch-> 22\n",
            "0.018262648954987526 : Loss for Batch-> 23\n",
            "0.018132973462343216 : Loss for Batch-> 24\n",
            "0.005948616657406092 : Loss for Batch-> 25\n",
            "0.0006312413024716079 : Loss for Batch-> 26\n",
            "0.0003243638202548027 : Loss for Batch-> 27\n",
            "0.0028830631636083126 : Loss for Batch-> 28\n",
            "0.0031001074239611626 : Loss for Batch-> 29\n",
            "0.0020254829432815313 : Loss for Batch-> 30\n",
            "0.0007294341339729726 : Loss for Batch-> 31\n",
            "0.007775431964546442 : Val Loss after training 31 batche(s)\n",
            "6.05863424425479e-05 : Loss for Batch-> 32\n",
            "4.213828287902288e-05 : Loss for Batch-> 33\n",
            "1.9321165382280014e-05 : Loss for Batch-> 34\n",
            "0.0008849427686072886 : Loss for Batch-> 35\n",
            "0.00038639281410723925 : Loss for Batch-> 36\n",
            "0.0011654734844341874 : Loss for Batch-> 37\n",
            "0.014070622622966766 : Loss for Batch-> 38\n",
            "0.0003433810197748244 : Loss for Batch-> 39\n",
            "0.0002662228944245726 : Loss for Batch-> 40\n",
            "0.000867521099280566 : Loss for Batch-> 41\n",
            "0.017724817618727684 : Val Loss after training 41 batche(s)\n",
            "0.0003335623478051275 : Loss for Batch-> 42\n",
            "0.013348463922739029 : Loss for Batch-> 43\n",
            "0.07098639756441116 : Loss for Batch-> 44\n",
            "0.06653393059968948 : Loss for Batch-> 45\n",
            "0.05456622689962387 : Loss for Batch-> 46\n",
            "0.003709473880007863 : Loss for Batch-> 47\n",
            "0.09792602807283401 : Loss for Batch-> 48\n",
            "0.24710386991500854 : Loss for Batch-> 49\n",
            "0.246469646692276 : Loss for Batch-> 50\n",
            "0.03250420466065407 : Loss for Batch-> 51\n",
            "0.016800133511424065 : Val Loss after training 51 batche(s)\n",
            "0.001184626016765833 : Loss for Batch-> 52\n",
            "0.0018073242390528321 : Loss for Batch-> 53\n",
            "0.01855175755918026 : Loss for Batch-> 54\n",
            "0.018056731671094894 : Loss for Batch-> 55\n",
            "0.00719029363244772 : Loss for Batch-> 56\n",
            "0.0007306098123081028 : Loss for Batch-> 57\n",
            "0.007035523187369108 : Loss for Batch-> 58\n",
            "0.03712628036737442 : Loss for Batch-> 59\n",
            "0.22745078802108765 : Loss for Batch-> 60\n",
            "0.2331451028585434 : Loss for Batch-> 61\n",
            "0.0015848783077672124 : Val Loss after training 61 batche(s)\n",
            "0.0677143782377243 : Loss for Batch-> 62\n",
            "0.07997298240661621 : Loss for Batch-> 63\n",
            "0.0704159140586853 : Loss for Batch-> 64\n",
            "0.04748169332742691 : Loss for Batch-> 65\n",
            "0.10744617134332657 : Loss for Batch-> 66\n",
            "0.07214658707380295 : Loss for Batch-> 67\n",
            "0.052815865725278854 : Loss for Batch-> 68\n",
            "0.011604297906160355 : Loss for Batch-> 69\n",
            "0.011934635229408741 : Loss for Batch-> 70\n",
            "0.010810776613652706 : Loss for Batch-> 71\n",
            "0.014558657072484493 : Val Loss after training 71 batche(s)\n",
            "0.010522600263357162 : Loss for Batch-> 72\n",
            "0.010933966375887394 : Loss for Batch-> 73\n",
            "0.016631755977869034 : Loss for Batch-> 74\n",
            "0.0456099696457386 : Loss for Batch-> 75\n",
            "2.5066030025482178 : Loss for Batch-> 76\n",
            "0.03863632678985596 : Loss for Batch-> 77\n",
            "0.08656468242406845 : Loss for Batch-> 78\n",
            "0.08993586897850037 : Loss for Batch-> 79\n",
            "0.043503448367118835 : Loss for Batch-> 80\n",
            "0.0006241878145374358 : Loss for Batch-> 81\n",
            "0.0074607618153095245 : Val Loss after training 81 batche(s)\n",
            "0.013383662328124046 : Loss for Batch-> 82\n",
            "0.0845351293683052 : Loss for Batch-> 83\n",
            "0.0809931829571724 : Loss for Batch-> 84\n",
            "0.0936393290758133 : Loss for Batch-> 85\n",
            "0.03486479073762894 : Loss for Batch-> 86\n",
            "0.016797510907053947 : Loss for Batch-> 87\n",
            "0.028542958199977875 : Loss for Batch-> 88\n",
            "0.045386143028736115 : Loss for Batch-> 89\n",
            "0.0031209744047373533 : Loss for Batch-> 90\n",
            "0.02322918362915516 : Loss for Batch-> 91\n",
            "0.0014132462674751878 : Val Loss after training 91 batche(s)\n",
            "0.041322387754917145 : Loss for Batch-> 92\n",
            "0.018198560923337936 : Loss for Batch-> 93\n",
            "0.010108125396072865 : Loss for Batch-> 94\n",
            "0.010334976017475128 : Loss for Batch-> 95\n",
            "0.0015310976887121797 : Loss for Batch-> 96\n",
            "0.0008051199256442487 : Loss for Batch-> 97\n",
            "0.0036953336093574762 : Loss for Batch-> 98\n",
            "0.005353099200874567 : Loss for Batch-> 99\n",
            "0.0025872052647173405 : Loss for Batch-> 100\n",
            "0.002305330242961645 : Loss for Batch-> 101\n",
            "0.0006830713828094304 : Val Loss after training 101 batche(s)\n",
            "0.0021058714482933283 : Loss for Batch-> 102\n",
            "0.021076608449220657 : Loss for Batch-> 103\n",
            "0.016973156481981277 : Loss for Batch-> 104\n",
            "0.015506620518863201 : Loss for Batch-> 105\n",
            "0.014545135200023651 : Loss for Batch-> 106\n",
            "0.001090669073164463 : Loss for Batch-> 107\n",
            "0.004295990336686373 : Loss for Batch-> 108\n",
            "0.03074987605214119 : Loss for Batch-> 109\n",
            "0.006052351091057062 : Loss for Batch-> 110\n",
            "0.005394526291638613 : Loss for Batch-> 111\n",
            "0.0036893123760819435 : Val Loss after training 111 batche(s)\n",
            "0.0010697717079892755 : Loss for Batch-> 112\n",
            "0.024942832067608833 : Loss for Batch-> 113\n",
            "0.03851763904094696 : Loss for Batch-> 114\n",
            "0.05534712225198746 : Loss for Batch-> 115\n",
            "0.012073217891156673 : Loss for Batch-> 116\n",
            "0.03733443841338158 : Loss for Batch-> 117\n",
            "0.045268669724464417 : Loss for Batch-> 118\n",
            "0.0015385216102004051 : Loss for Batch-> 119\n",
            "0.015429453924298286 : Loss for Batch-> 120\n",
            "0.011709227226674557 : Loss for Batch-> 121\n",
            "0.0029671210795640945 : Val Loss after training 121 batche(s)\n",
            "0.10586152970790863 : Loss for Batch-> 122\n",
            "0.07066906243562698 : Loss for Batch-> 123\n",
            "0.012811983935534954 : Loss for Batch-> 124\n",
            "0.023040302097797394 : Loss for Batch-> 125\n",
            "0.017461786046624184 : Loss for Batch-> 126\n",
            "0.003002169542014599 : Loss for Batch-> 127\n",
            "0.015973152592778206 : Loss for Batch-> 128\n",
            "0.031314071267843246 : Loss for Batch-> 129\n",
            "0.1401299089193344 : Loss for Batch-> 130\n",
            "0.012714607641100883 : Loss for Batch-> 131\n",
            "0.0026599892880767584 : Val Loss after training 131 batche(s)\n",
            "0.051871296018362045 : Loss for Batch-> 132\n",
            "0.012825190089643002 : Loss for Batch-> 133\n",
            "0.051548633724451065 : Loss for Batch-> 134\n",
            "0.0657849907875061 : Loss for Batch-> 135\n",
            "0.2450077086687088 : Loss for Batch-> 136\n",
            "0.016251880675554276 : Loss for Batch-> 137\n",
            "0.024786239489912987 : Loss for Batch-> 138\n",
            "0.0005049887113273144 : Loss for Batch-> 139\n",
            "0.0007780089508742094 : Loss for Batch-> 140\n",
            "0.0006334470817819238 : Loss for Batch-> 141\n",
            "0.00021794343774672598 : Val Loss after training 141 batche(s)\n",
            "0.006775899324566126 : Loss for Batch-> 142\n",
            "0.012748253531754017 : Loss for Batch-> 143\n",
            "0.016658317297697067 : Loss for Batch-> 144\n",
            "0.011170290410518646 : Loss for Batch-> 145\n",
            "0.007300448603928089 : Loss for Batch-> 146\n",
            "0.0018230127170681953 : Loss for Batch-> 147\n",
            "0.0024747520219534636 : Loss for Batch-> 148\n",
            "0.008288508281111717 : Loss for Batch-> 149\n",
            "0.058897122740745544 : Loss for Batch-> 150\n",
            "0.047165218740701675 : Loss for Batch-> 151\n",
            "0.0020903467666357756 : Val Loss after training 151 batche(s)\n",
            "0.004306986462324858 : Loss for Batch-> 152\n",
            "0.04380878061056137 : Loss for Batch-> 153\n",
            "0.0358419232070446 : Loss for Batch-> 154\n",
            "0.012857259251177311 : Loss for Batch-> 155\n",
            "0.023752938956022263 : Loss for Batch-> 156\n",
            "0.04163853079080582 : Loss for Batch-> 157\n",
            "0.0046600280329585075 : Loss for Batch-> 158\n",
            "0.007301390636712313 : Loss for Batch-> 159\n",
            "0.03871425241231918 : Loss for Batch-> 160\n",
            "0.06921428442001343 : Loss for Batch-> 161\n",
            "0.0054808324202895164 : Val Loss after training 161 batche(s)\n",
            "0.02297305501997471 : Loss for Batch-> 162\n",
            "0.021395215764641762 : Loss for Batch-> 163\n",
            "0.04656386375427246 : Loss for Batch-> 164\n",
            "0.05188882723450661 : Loss for Batch-> 165\n",
            "0.04041187837719917 : Loss for Batch-> 166\n",
            "0.008906742557883263 : Loss for Batch-> 167\n",
            "0.03266695514321327 : Loss for Batch-> 168\n",
            "0.0011319929035380483 : Loss for Batch-> 169\n",
            "0.00035114650381729007 : Loss for Batch-> 170\n",
            "0.0013168775476515293 : Loss for Batch-> 171\n",
            "0.005518126301467419 : Val Loss after training 171 batche(s)\n",
            "0.001031150808557868 : Loss for Batch-> 172\n",
            "0.0003145966911688447 : Loss for Batch-> 173\n",
            "0.0005614192341454327 : Loss for Batch-> 174\n",
            "0.00751357339322567 : Loss for Batch-> 175\n",
            "0.02260017581284046 : Loss for Batch-> 176\n",
            "0.002656668657436967 : Loss for Batch-> 177\n",
            "0.0004401707265060395 : Loss for Batch-> 178\n",
            "0.0003545347135514021 : Loss for Batch-> 179\n",
            "3.201163053745404e-05 : Loss for Batch-> 180\n",
            "5.904216777707916e-06 : Loss for Batch-> 181\n",
            "0.004480603151023388 : Val Loss after training 181 batche(s)\n",
            "0.0004956434131599963 : Loss for Batch-> 182\n",
            "0.0002721426135394722 : Loss for Batch-> 183\n",
            "0.00033327413257211447 : Loss for Batch-> 184\n",
            "0.0007784951012581587 : Loss for Batch-> 185\n",
            "0.0002552488003857434 : Loss for Batch-> 186\n",
            "0.0005324077210389078 : Loss for Batch-> 187\n",
            "0.0005845626001246274 : Loss for Batch-> 188\n",
            "0.007176013197749853 : Loss for Batch-> 189\n",
            "0.018487947061657906 : Loss for Batch-> 190\n",
            "0.021694231778383255 : Loss for Batch-> 191\n",
            "0.0046824184246361256 : Val Loss after training 191 batche(s)\n",
            "0.006061600521206856 : Loss for Batch-> 192\n",
            "0.0006579359760507941 : Loss for Batch-> 193\n",
            "0.0007293926319107413 : Loss for Batch-> 194\n",
            "0.00019715810776688159 : Loss for Batch-> 195\n",
            "0.001743780099786818 : Loss for Batch-> 196\n",
            "3.039139986038208 : Loss for Batch-> 197\n",
            "0.09287698566913605 : Loss for Batch-> 198\n",
            "0.033159688115119934 : Loss for Batch-> 199\n",
            "0.05512306094169617 : Loss for Batch-> 200\n",
            "0.05583181232213974 : Loss for Batch-> 201\n",
            "0.001727709430269897 : Val Loss after training 201 batche(s)\n",
            "0.015956856310367584 : Loss for Batch-> 202\n",
            "0.018713658675551414 : Loss for Batch-> 203\n",
            "0.01124879252165556 : Loss for Batch-> 204\n",
            "0.009549726732075214 : Loss for Batch-> 205\n",
            "0.006225879769772291 : Loss for Batch-> 206\n",
            "0.005905812606215477 : Loss for Batch-> 207\n",
            "0.011049495078623295 : Loss for Batch-> 208\n",
            "0.04159374162554741 : Loss for Batch-> 209\n",
            "0.001702297362498939 : Loss for Batch-> 210\n",
            "0.027691267430782318 : Loss for Batch-> 211\n",
            "0.004161263816058636 : Val Loss after training 211 batche(s)\n",
            "0.02907417342066765 : Loss for Batch-> 212\n",
            "0.011502171866595745 : Loss for Batch-> 213\n",
            "0.05056892707943916 : Loss for Batch-> 214\n",
            "0.0003109541430603713 : Loss for Batch-> 215\n",
            "0.0005630227387882769 : Loss for Batch-> 216\n",
            "0.001316478126682341 : Loss for Batch-> 217\n",
            "0.06166955456137657 : Loss for Batch-> 218\n",
            "0.05169527232646942 : Loss for Batch-> 219\n",
            "0.00675793644040823 : Loss for Batch-> 220\n",
            "0.16647855937480927 : Loss for Batch-> 221\n",
            "0.06144586205482483 : Val Loss after training 221 batche(s)\n",
            "0.008875027298927307 : Loss for Batch-> 222\n",
            "0.00038959653466008604 : Loss for Batch-> 223\n",
            "0.0004328376380726695 : Loss for Batch-> 224\n",
            "0.00043143462971784174 : Loss for Batch-> 225\n",
            "0.6755359172821045 : Loss for Batch-> 226\n",
            "1.7888386249542236 : Loss for Batch-> 227\n",
            "5.37703800201416 : Loss for Batch-> 228\n",
            "6.152693271636963 : Loss for Batch-> 229\n",
            "0.13066977262496948 : Loss for Batch-> 230\n",
            "0.19620925188064575 : Loss for Batch-> 231\n",
            "0.0215446799993515 : Val Loss after training 231 batche(s)\n",
            "0.008394850417971611 : Loss for Batch-> 232\n",
            "0.02765616402029991 : Loss for Batch-> 233\n",
            "0.04100564867258072 : Loss for Batch-> 234\n",
            "0.0021726300474256277 : Loss for Batch-> 235\n",
            "0.0012119817547500134 : Loss for Batch-> 236\n",
            "0.0016922039212659001 : Loss for Batch-> 237\n",
            "0.0909169465303421 : Loss for Batch-> 238\n",
            "0.09662222862243652 : Loss for Batch-> 239\n",
            "0.13619163632392883 : Loss for Batch-> 240\n",
            "0.3575856387615204 : Loss for Batch-> 241\n",
            "0.0001614400971448049 : Val Loss after training 241 batche(s)\n",
            "0.7133276462554932 : Loss for Batch-> 242\n",
            "4.752666473388672 : Loss for Batch-> 243\n",
            "0.009639321826398373 : Loss for Batch-> 244\n",
            "0.028645474463701248 : Loss for Batch-> 245\n",
            "2.9348790645599365 : Loss for Batch-> 246\n",
            "1.4945101737976074 : Loss for Batch-> 247\n",
            "0.8996936082839966 : Loss for Batch-> 248\n",
            "0.33175182342529297 : Loss for Batch-> 249\n",
            "0.05607092007994652 : Loss for Batch-> 250\n",
            "0.385586142539978 : Loss for Batch-> 251\n",
            "0.0003026407503057271 : Val Loss after training 251 batche(s)\n",
            "0.5121915936470032 : Loss for Batch-> 252\n",
            "0.558139979839325 : Loss for Batch-> 253\n",
            "4.024625778198242 : Loss for Batch-> 254\n",
            "5.176426410675049 : Loss for Batch-> 255\n",
            "0.0031697789672762156 : Loss for Batch-> 256\n",
            "0.2609589695930481 : Loss for Batch-> 257\n",
            "0.42130547761917114 : Loss for Batch-> 258\n",
            "4.345225811004639 : Loss for Batch-> 259\n",
            "1.6767522096633911 : Loss for Batch-> 260\n",
            "0.4553777277469635 : Loss for Batch-> 261\n",
            "0.017939914017915726 : Val Loss after training 261 batche(s)\n",
            "0.00154261221177876 : Loss for Batch-> 262\n",
            "0.8250309228897095 : Loss for Batch-> 263\n",
            "1.3010690212249756 : Loss for Batch-> 264\n",
            "0.1121850460767746 : Loss for Batch-> 265\n",
            "0.03809460625052452 : Loss for Batch-> 266\n",
            "0.002409702632576227 : Loss for Batch-> 267\n",
            "3.6590161323547363 : Loss for Batch-> 268\n",
            "2.332798957824707 : Loss for Batch-> 269\n",
            "0.007519042585045099 : Loss for Batch-> 270\n",
            "0.0003893099492415786 : Loss for Batch-> 271\n",
            "0.03247002139687538 : Val Loss after training 271 batche(s)\n",
            "0.002041949424892664 : Loss for Batch-> 272\n",
            "0.0004885080270469189 : Loss for Batch-> 273\n",
            "0.002225987846031785 : Loss for Batch-> 274\n",
            "0.0035970991011708975 : Loss for Batch-> 275\n",
            "0.0006919215084053576 : Loss for Batch-> 276\n",
            "0.012810946442186832 : Loss for Batch-> 277\n",
            "0.008009198121726513 : Loss for Batch-> 278\n",
            "0.041953086853027344 : Loss for Batch-> 279\n",
            "0.0660124272108078 : Loss for Batch-> 280\n",
            "0.06506840884685516 : Loss for Batch-> 281\n",
            "0.047515563666820526 : Val Loss after training 281 batche(s)\n",
            "0.06492020934820175 : Loss for Batch-> 282\n",
            "0.025446651503443718 : Loss for Batch-> 283\n",
            "0.19209171831607819 : Loss for Batch-> 284\n",
            "11.803749084472656 : Loss for Batch-> 285\n",
            "2.3900563716888428 : Loss for Batch-> 286\n",
            "0.011608214117586613 : Loss for Batch-> 287\n",
            "0.0016995184123516083 : Loss for Batch-> 288\n",
            "0.06684456765651703 : Loss for Batch-> 289\n",
            "0.11375375837087631 : Loss for Batch-> 290\n",
            "0.004910947289317846 : Loss for Batch-> 291\n",
            "0.02011053077876568 : Val Loss after training 291 batche(s)\n",
            "0.0003310304891783744 : Loss for Batch-> 292\n",
            "0.0016596512869000435 : Loss for Batch-> 293\n",
            "0.016880113631486893 : Loss for Batch-> 294\n",
            "0.05410916730761528 : Loss for Batch-> 295\n",
            "0.001728791045024991 : Loss for Batch-> 296\n",
            "0.04018125683069229 : Loss for Batch-> 297\n",
            "0.017253490164875984 : Loss for Batch-> 298\n",
            "1.0173662900924683 : Loss for Batch-> 299\n",
            "1.0174999237060547 : Loss for Batch-> 300\n",
            "0.6294533014297485 : Loss for Batch-> 301\n",
            "0.043222516775131226 : Val Loss after training 301 batche(s)\n",
            "0.09056682884693146 : Loss for Batch-> 302\n",
            "0.010823855176568031 : Loss for Batch-> 303\n",
            "1.2167681455612183 : Loss for Batch-> 304\n",
            "1.8012934923171997 : Loss for Batch-> 305\n",
            "0.9427684545516968 : Loss for Batch-> 306\n",
            "0.0534479133784771 : Loss for Batch-> 307\n",
            "0.03748675435781479 : Loss for Batch-> 308\n",
            "0.32137930393218994 : Loss for Batch-> 309\n",
            "1.6549859046936035 : Loss for Batch-> 310\n",
            "1.0192108154296875 : Loss for Batch-> 311\n",
            "0.02671108953654766 : Val Loss after training 311 batche(s)\n",
            "0.012717150151729584 : Loss for Batch-> 312\n",
            "0.01050500851124525 : Loss for Batch-> 313\n",
            "0.08644765615463257 : Loss for Batch-> 314\n",
            "0.5482590794563293 : Loss for Batch-> 315\n",
            "0.018041539937257767 : Loss for Batch-> 316\n",
            "0.025349698960781097 : Loss for Batch-> 317\n",
            "0.14935678243637085 : Loss for Batch-> 318\n",
            "0.1834201216697693 : Loss for Batch-> 319\n",
            "0.17368493974208832 : Loss for Batch-> 320\n",
            "0.3236839175224304 : Loss for Batch-> 321\n",
            "0.008173688314855099 : Val Loss after training 321 batche(s)\n",
            "0.04855617880821228 : Loss for Batch-> 322\n",
            "0.0741834044456482 : Loss for Batch-> 323\n",
            "0.2556433379650116 : Loss for Batch-> 324\n",
            "0.3203841745853424 : Loss for Batch-> 325\n",
            "0.47814032435417175 : Loss for Batch-> 326\n",
            "0.3002573847770691 : Loss for Batch-> 327\n",
            "0.055967047810554504 : Loss for Batch-> 328\n",
            "0.0816313847899437 : Loss for Batch-> 329\n",
            "0.04777515307068825 : Loss for Batch-> 330\n",
            "0.008590097539126873 : Loss for Batch-> 331\n",
            "0.008294343017041683 : Val Loss after training 331 batche(s)\n",
            "0.009122166782617569 : Loss for Batch-> 332\n",
            "0.00021244441450107843 : Loss for Batch-> 333\n",
            "0.0005900963442400098 : Loss for Batch-> 334\n",
            "0.0012426963075995445 : Loss for Batch-> 335\n",
            "0.0027826190926134586 : Loss for Batch-> 336\n",
            "0.005708085838705301 : Loss for Batch-> 337\n",
            "0.06221730634570122 : Loss for Batch-> 338\n",
            "0.060741886496543884 : Loss for Batch-> 339\n",
            "0.05185803398489952 : Loss for Batch-> 340\n",
            "0.1386556178331375 : Loss for Batch-> 341\n",
            "0.008065727539360523 : Val Loss after training 341 batche(s)\n",
            "0.45096489787101746 : Loss for Batch-> 342\n",
            "0.0033784944098442793 : Loss for Batch-> 343\n",
            "0.016114892438054085 : Loss for Batch-> 344\n",
            "0.12159907072782516 : Loss for Batch-> 345\n",
            "0.3582887351512909 : Loss for Batch-> 346\n",
            "0.2951180934906006 : Loss for Batch-> 347\n",
            "0.003269279608502984 : Loss for Batch-> 348\n",
            "0.4003574252128601 : Loss for Batch-> 349\n",
            "0.8926204442977905 : Loss for Batch-> 350\n",
            "0.2079995721578598 : Loss for Batch-> 351\n",
            "0.008267436176538467 : Val Loss after training 351 batche(s)\n",
            "0.032180413603782654 : Loss for Batch-> 352\n",
            "0.6067079901695251 : Loss for Batch-> 353\n",
            "1.3121600151062012 : Loss for Batch-> 354\n",
            "0.0022701078560203314 : Loss for Batch-> 355\n",
            "0.010344655252993107 : Loss for Batch-> 356\n",
            "0.00321957771666348 : Loss for Batch-> 357\n",
            "0.49981066584587097 : Loss for Batch-> 358\n",
            "0.09019273519515991 : Loss for Batch-> 359\n",
            "0.13163523375988007 : Loss for Batch-> 360\n",
            "0.12133529782295227 : Loss for Batch-> 361\n",
            "0.006204419303685427 : Val Loss after training 361 batche(s)\n",
            "0.0851774588227272 : Loss for Batch-> 362\n",
            "0.20275408029556274 : Loss for Batch-> 363\n",
            "0.34660065174102783 : Loss for Batch-> 364\n",
            "0.10323917120695114 : Loss for Batch-> 365\n",
            "0.278157114982605 : Loss for Batch-> 366\n",
            "0.03048616461455822 : Loss for Batch-> 367\n",
            "0.0062566278502345085 : Loss for Batch-> 368\n",
            "0.06240788474678993 : Loss for Batch-> 369\n",
            "0.05168932303786278 : Loss for Batch-> 370\n",
            "0.7488653659820557 : Loss for Batch-> 371\n",
            "0.02105882205069065 : Val Loss after training 371 batche(s)\n",
            "0.2106066644191742 : Loss for Batch-> 372\n",
            "0.7143575549125671 : Loss for Batch-> 373\n",
            "1.1163554191589355 : Loss for Batch-> 374\n",
            "0.21051135659217834 : Loss for Batch-> 375\n",
            "0.5315181016921997 : Loss for Batch-> 376\n",
            "0.146510049700737 : Loss for Batch-> 377\n",
            "0.013299262151122093 : Loss for Batch-> 378\n",
            "0.03925260901451111 : Loss for Batch-> 379\n",
            "0.17172102630138397 : Loss for Batch-> 380\n",
            "0.25582632422447205 : Loss for Batch-> 381\n",
            "0.01407784316688776 : Val Loss after training 381 batche(s)\n",
            "0.7036746740341187 : Loss for Batch-> 382\n",
            "1.0350829362869263 : Loss for Batch-> 383\n",
            "1.5357882976531982 : Loss for Batch-> 384\n",
            "0.7683119177818298 : Loss for Batch-> 385\n",
            "0.3545553684234619 : Loss for Batch-> 386\n",
            "0.0027229413390159607 : Loss for Batch-> 387\n",
            "0.042252108454704285 : Loss for Batch-> 388\n",
            "0.8360495567321777 : Loss for Batch-> 389\n",
            "1.3789737224578857 : Loss for Batch-> 390\n",
            "0.47376564145088196 : Loss for Batch-> 391\n",
            "0.0021605263464152813 : Val Loss after training 391 batche(s)\n",
            "0.0639793649315834 : Loss for Batch-> 392\n",
            "0.061955299228429794 : Loss for Batch-> 393\n",
            "0.029315831139683723 : Loss for Batch-> 394\n",
            "0.3901699185371399 : Loss for Batch-> 395\n",
            "0.1350601762533188 : Loss for Batch-> 396\n",
            "0.9208827018737793 : Loss for Batch-> 397\n",
            "0.631740927696228 : Loss for Batch-> 398\n",
            "0.008448018692433834 : Loss for Batch-> 399\n",
            "0.0073799253441393375 : Loss for Batch-> 400\n",
            "0.016639672219753265 : Loss for Batch-> 401\n",
            "0.0005058025708422065 : Val Loss after training 401 batche(s)\n",
            "0.01397470198571682 : Loss for Batch-> 402\n",
            "0.317249059677124 : Loss for Batch-> 403\n",
            "0.5612316131591797 : Loss for Batch-> 404\n",
            "0.25331830978393555 : Loss for Batch-> 405\n",
            "0.10089869797229767 : Loss for Batch-> 406\n",
            "0.048494964838027954 : Loss for Batch-> 407\n",
            "0.007321570999920368 : Loss for Batch-> 408\n",
            "0.06855304539203644 : Loss for Batch-> 409\n",
            "0.6875989437103271 : Loss for Batch-> 410\n",
            "1.1831068992614746 : Loss for Batch-> 411\n",
            "0.03859890252351761 : Val Loss after training 411 batche(s)\n",
            "0.18046578764915466 : Loss for Batch-> 412\n",
            "0.1435650736093521 : Loss for Batch-> 413\n",
            "0.0013928095577284694 : Loss for Batch-> 414\n",
            "0.0015604494838044047 : Loss for Batch-> 415\n",
            "0.0017644289182499051 : Loss for Batch-> 416\n",
            "0.020078664645552635 : Loss for Batch-> 417\n",
            "0.029681915417313576 : Loss for Batch-> 418\n",
            "0.0017476258799433708 : Loss for Batch-> 419\n",
            "0.18314522504806519 : Loss for Batch-> 420\n",
            "0.08116105943918228 : Loss for Batch-> 421\n",
            "0.07934089004993439 : Val Loss after training 421 batche(s)\n",
            "0.027487143874168396 : Loss for Batch-> 422\n",
            "0.023034725338220596 : Loss for Batch-> 423\n",
            "0.02207350917160511 : Loss for Batch-> 424\n",
            "0.02272714115679264 : Loss for Batch-> 425\n",
            "0.023171892389655113 : Loss for Batch-> 426\n",
            "0.024239912629127502 : Loss for Batch-> 427\n",
            "0.0237117949873209 : Loss for Batch-> 428\n",
            "0.023312296718358994 : Loss for Batch-> 429\n",
            "0.022597262635827065 : Loss for Batch-> 430\n",
            "0.055662740021944046 : Loss for Batch-> 431\n",
            "0.09201683104038239 : Val Loss after training 431 batche(s)\n",
            "0.1154143363237381 : Loss for Batch-> 432\n",
            "0.024097086861729622 : Loss for Batch-> 433\n",
            "0.004424458835273981 : Loss for Batch-> 434\n",
            "0.0012707124697044492 : Loss for Batch-> 435\n",
            "0.0012549200328066945 : Loss for Batch-> 436\n",
            "0.0025328609626740217 : Loss for Batch-> 437\n",
            "0.0012043726164847612 : Loss for Batch-> 438\n",
            "0.04165441542863846 : Loss for Batch-> 439\n",
            "0.062282972037792206 : Loss for Batch-> 440\n",
            "0.016345486044883728 : Loss for Batch-> 441\n",
            "0.009293329901993275 : Val Loss after training 441 batche(s)\n",
            "0.013583939522504807 : Loss for Batch-> 442\n",
            "0.023501990363001823 : Loss for Batch-> 443\n",
            "0.04185004159808159 : Loss for Batch-> 444\n",
            "0.012795500457286835 : Loss for Batch-> 445\n",
            "0.133137509226799 : Loss for Batch-> 446\n",
            "0.33261939883232117 : Loss for Batch-> 447\n",
            "0.606985867023468 : Loss for Batch-> 448\n",
            "0.006438387092202902 : Loss for Batch-> 449\n",
            "0.0015276760095730424 : Loss for Batch-> 450\n",
            "0.0005284841172397137 : Loss for Batch-> 451\n",
            "0.0969436764717102 : Val Loss after training 451 batche(s)\n",
            "0.0008330928394570947 : Loss for Batch-> 452\n",
            "0.0006955476710572839 : Loss for Batch-> 453\n",
            "0.0007873970316722989 : Loss for Batch-> 454\n",
            "0.0011831261217594147 : Loss for Batch-> 455\n",
            "0.0001327476347796619 : Loss for Batch-> 456\n",
            "0.00013940200733486563 : Loss for Batch-> 457\n",
            "0.00013839657185599208 : Loss for Batch-> 458\n",
            "0.00013820517051499337 : Loss for Batch-> 459\n",
            "0.0001363492337986827 : Loss for Batch-> 460\n",
            "0.00017772120190784335 : Loss for Batch-> 461\n",
            "0.0333096943795681 : Val Loss after training 461 batche(s)\n",
            "0.00013241036504041404 : Loss for Batch-> 462\n",
            "0.0001290579530177638 : Loss for Batch-> 463\n",
            "0.00013078679330646992 : Loss for Batch-> 464\n",
            "0.0006189352134242654 : Loss for Batch-> 465\n",
            "0.001428675022907555 : Loss for Batch-> 466\n",
            "0.00037678226362913847 : Loss for Batch-> 467\n",
            "0.0005189108196645975 : Loss for Batch-> 468\n",
            "0.0013916539028286934 : Loss for Batch-> 469\n",
            "0.0002677824813872576 : Loss for Batch-> 470\n",
            "0.00036423883284442127 : Loss for Batch-> 471\n",
            "0.0036013710778206587 : Val Loss after training 471 batche(s)\n",
            "0.0007469998090527952 : Loss for Batch-> 472\n",
            "0.0008256110595539212 : Loss for Batch-> 473\n",
            "0.00043660125811584294 : Loss for Batch-> 474\n",
            "0.0009312662295997143 : Loss for Batch-> 475\n",
            "0.0011675445130094886 : Loss for Batch-> 476\n",
            "0.0007676284294575453 : Loss for Batch-> 477\n",
            "0.0004143109836149961 : Loss for Batch-> 478\n",
            "0.0004386448999866843 : Loss for Batch-> 479\n",
            "0.001970227574929595 : Loss for Batch-> 480\n",
            "0.014355268329381943 : Loss for Batch-> 481\n",
            "0.025703899562358856 : Val Loss after training 481 batche(s)\n",
            "0.015328769572079182 : Loss for Batch-> 482\n",
            "0.005682257004082203 : Loss for Batch-> 483\n",
            "0.003575725946575403 : Loss for Batch-> 484\n",
            "0.003536679781973362 : Loss for Batch-> 485\n",
            "0.003671207232400775 : Loss for Batch-> 486\n",
            "0.003526484128087759 : Loss for Batch-> 487\n",
            "0.0034861350432038307 : Loss for Batch-> 488\n",
            "0.0036535493563860655 : Loss for Batch-> 489\n",
            "0.0026085868012160063 : Loss for Batch-> 490\n",
            "0.0021028437186032534 : Loss for Batch-> 491\n",
            "0.012108261696994305 : Val Loss after training 491 batche(s)\n",
            "0.0020792274735867977 : Loss for Batch-> 492\n",
            "0.0019367873901501298 : Loss for Batch-> 493\n",
            "1.5486036539077759 : Loss for Batch-> 494\n",
            "0.21309050917625427 : Loss for Batch-> 495\n",
            "0.01798035390675068 : Loss for Batch-> 496\n",
            "0.004573318641632795 : Loss for Batch-> 497\n",
            "0.001356626395136118 : Loss for Batch-> 498\n",
            "0.001055269269272685 : Loss for Batch-> 499\n",
            "0.0020320063922554255 : Loss for Batch-> 500\n",
            "0.0032619694247841835 : Loss for Batch-> 501\n",
            "0.012886084616184235 : Val Loss after training 501 batche(s)\n",
            "0.007912389002740383 : Loss for Batch-> 502\n",
            "0.04982567951083183 : Loss for Batch-> 503\n",
            "0.0049967109225690365 : Loss for Batch-> 504\n",
            "0.020517468452453613 : Loss for Batch-> 505\n",
            "0.019789330661296844 : Loss for Batch-> 506\n",
            "0.019047891721129417 : Loss for Batch-> 507\n",
            "0.018268411979079247 : Loss for Batch-> 508\n",
            "0.007696155458688736 : Loss for Batch-> 509\n",
            "0.008833382278680801 : Loss for Batch-> 510\n",
            "25 Epoch\n",
            "0.008995931595563889 : Loss for Batch-> 1\n",
            "0.011042517609894276 : Val Loss after training 1 batche(s)\n",
            "0.008626289665699005 : Loss for Batch-> 2\n",
            "0.008503827266395092 : Loss for Batch-> 3\n",
            "0.00833903532475233 : Loss for Batch-> 4\n",
            "0.007808167953044176 : Loss for Batch-> 5\n",
            "0.007715088780969381 : Loss for Batch-> 6\n",
            "0.009323390200734138 : Loss for Batch-> 7\n",
            "0.017823809757828712 : Loss for Batch-> 8\n",
            "0.017603309825062752 : Loss for Batch-> 9\n",
            "0.01878751628100872 : Loss for Batch-> 10\n",
            "0.18721763789653778 : Loss for Batch-> 11\n",
            "0.009705927222967148 : Val Loss after training 11 batche(s)\n",
            "5.8664093017578125 : Loss for Batch-> 12\n",
            "0.15724843740463257 : Loss for Batch-> 13\n",
            "0.0013774216640740633 : Loss for Batch-> 14\n",
            "0.0014821136137470603 : Loss for Batch-> 15\n",
            "0.0010946261463686824 : Loss for Batch-> 16\n",
            "0.0037560290656983852 : Loss for Batch-> 17\n",
            "0.012967485003173351 : Loss for Batch-> 18\n",
            "0.0010749662760645151 : Loss for Batch-> 19\n",
            "0.0035624124575406313 : Loss for Batch-> 20\n",
            "0.02484755590558052 : Loss for Batch-> 21\n",
            "0.16146597266197205 : Val Loss after training 21 batche(s)\n",
            "0.020043345168232918 : Loss for Batch-> 22\n",
            "0.01889251172542572 : Loss for Batch-> 23\n",
            "0.020979728549718857 : Loss for Batch-> 24\n",
            "0.01620764657855034 : Loss for Batch-> 25\n",
            "0.0009585219086147845 : Loss for Batch-> 26\n",
            "0.00039022270357236266 : Loss for Batch-> 27\n",
            "0.00034485082142055035 : Loss for Batch-> 28\n",
            "0.004120464436709881 : Loss for Batch-> 29\n",
            "0.002678798045963049 : Loss for Batch-> 30\n",
            "0.001689218683168292 : Loss for Batch-> 31\n",
            "0.07837331295013428 : Val Loss after training 31 batche(s)\n",
            "0.00017448834842070937 : Loss for Batch-> 32\n",
            "7.257665856741369e-05 : Loss for Batch-> 33\n",
            "2.906664121837821e-05 : Loss for Batch-> 34\n",
            "1.773889562173281e-05 : Loss for Batch-> 35\n",
            "0.0012206200044602156 : Loss for Batch-> 36\n",
            "0.0002642218314576894 : Loss for Batch-> 37\n",
            "0.008641519583761692 : Loss for Batch-> 38\n",
            "0.006503961514681578 : Loss for Batch-> 39\n",
            "0.00018347968580201268 : Loss for Batch-> 40\n",
            "0.000537764048203826 : Loss for Batch-> 41\n",
            "0.08579397946596146 : Val Loss after training 41 batche(s)\n",
            "0.0006415719399228692 : Loss for Batch-> 42\n",
            "0.0004917297628708184 : Loss for Batch-> 43\n",
            "0.04481792822480202 : Loss for Batch-> 44\n",
            "0.06504513323307037 : Loss for Batch-> 45\n",
            "0.07018698006868362 : Loss for Batch-> 46\n",
            "0.02530108392238617 : Loss for Batch-> 47\n",
            "0.006423677783459425 : Loss for Batch-> 48\n",
            "0.18840862810611725 : Loss for Batch-> 49\n",
            "0.25458255410194397 : Loss for Batch-> 50\n",
            "0.176976278424263 : Loss for Batch-> 51\n",
            "0.09694276005029678 : Val Loss after training 51 batche(s)\n",
            "0.0020587460603564978 : Loss for Batch-> 52\n",
            "0.0009034062968567014 : Loss for Batch-> 53\n",
            "0.00815841369330883 : Loss for Batch-> 54\n",
            "0.018066029995679855 : Loss for Batch-> 55\n",
            "0.016735613346099854 : Loss for Batch-> 56\n",
            "0.003353186883032322 : Loss for Batch-> 57\n",
            "0.0006365642184391618 : Loss for Batch-> 58\n",
            "0.012548076920211315 : Loss for Batch-> 59\n",
            "0.11193565279245377 : Loss for Batch-> 60\n",
            "0.23334912955760956 : Loss for Batch-> 61\n",
            "0.03819258138537407 : Val Loss after training 61 batche(s)\n",
            "0.20658431947231293 : Loss for Batch-> 62\n",
            "0.03767853602766991 : Loss for Batch-> 63\n",
            "0.09022478014230728 : Loss for Batch-> 64\n",
            "0.03334224596619606 : Loss for Batch-> 65\n",
            "0.13792288303375244 : Loss for Batch-> 66\n",
            "0.04274832829833031 : Loss for Batch-> 67\n",
            "0.0898146703839302 : Loss for Batch-> 68\n",
            "0.010723861865699291 : Loss for Batch-> 69\n",
            "0.011655506677925587 : Loss for Batch-> 70\n",
            "0.011628909036517143 : Loss for Batch-> 71\n",
            "0.03999646008014679 : Val Loss after training 71 batche(s)\n",
            "0.01032483670860529 : Loss for Batch-> 72\n",
            "0.01078997366130352 : Loss for Batch-> 73\n",
            "0.010659120976924896 : Loss for Batch-> 74\n",
            "0.024530349299311638 : Loss for Batch-> 75\n",
            "0.6923397183418274 : Loss for Batch-> 76\n",
            "1.8772333860397339 : Loss for Batch-> 77\n",
            "0.042413000017404556 : Loss for Batch-> 78\n",
            "0.07861179113388062 : Loss for Batch-> 79\n",
            "0.09339585900306702 : Loss for Batch-> 80\n",
            "0.014215998351573944 : Loss for Batch-> 81\n",
            "0.04351511970162392 : Val Loss after training 81 batche(s)\n",
            "0.0016809221124276519 : Loss for Batch-> 82\n",
            "0.044616952538490295 : Loss for Batch-> 83\n",
            "0.08653692901134491 : Loss for Batch-> 84\n",
            "0.08673852682113647 : Loss for Batch-> 85\n",
            "0.07490098476409912 : Loss for Batch-> 86\n",
            "0.0226419847458601 : Loss for Batch-> 87\n",
            "0.011222000233829021 : Loss for Batch-> 88\n",
            "0.043101124465465546 : Loss for Batch-> 89\n",
            "0.02872541733086109 : Loss for Batch-> 90\n",
            "0.0016099694184958935 : Loss for Batch-> 91\n",
            "0.042443979531526566 : Val Loss after training 91 batche(s)\n",
            "0.044633280485868454 : Loss for Batch-> 92\n",
            "0.036086395382881165 : Loss for Batch-> 93\n",
            "0.0015272973105311394 : Loss for Batch-> 94\n",
            "0.019629254937171936 : Loss for Batch-> 95\n",
            "0.0007452991558238864 : Loss for Batch-> 96\n",
            "0.0017346000531688333 : Loss for Batch-> 97\n",
            "0.001808562083169818 : Loss for Batch-> 98\n",
            "0.006803257390856743 : Loss for Batch-> 99\n",
            "0.0015603362116962671 : Loss for Batch-> 100\n",
            "0.0031489317771047354 : Loss for Batch-> 101\n",
            "0.04016558825969696 : Val Loss after training 101 batche(s)\n",
            "0.0020494493655860424 : Loss for Batch-> 102\n",
            "0.009818747639656067 : Loss for Batch-> 103\n",
            "0.019270580261945724 : Loss for Batch-> 104\n",
            "0.019046634435653687 : Loss for Batch-> 105\n",
            "0.01744876801967621 : Loss for Batch-> 106\n",
            "0.004549005534499884 : Loss for Batch-> 107\n",
            "0.0017850087024271488 : Loss for Batch-> 108\n",
            "0.01551821269094944 : Loss for Batch-> 109\n",
            "0.02394552156329155 : Loss for Batch-> 110\n",
            "0.0028089191764593124 : Loss for Batch-> 111\n",
            "0.038323357701301575 : Val Loss after training 111 batche(s)\n",
            "0.003984936513006687 : Loss for Batch-> 112\n",
            "0.0010906142415478826 : Loss for Batch-> 113\n",
            "0.046542778611183167 : Loss for Batch-> 114\n",
            "0.03600915148854256 : Loss for Batch-> 115\n",
            "0.046497929841279984 : Loss for Batch-> 116\n",
            "0.0021689897403120995 : Loss for Batch-> 117\n",
            "0.06803715974092484 : Loss for Batch-> 118\n",
            "0.014218529686331749 : Loss for Batch-> 119\n",
            "0.011017467826604843 : Loss for Batch-> 120\n",
            "0.012033743783831596 : Loss for Batch-> 121\n",
            "0.03677384927868843 : Val Loss after training 121 batche(s)\n",
            "0.02912920154631138 : Loss for Batch-> 122\n",
            "0.1095501109957695 : Loss for Batch-> 123\n",
            "0.04908391833305359 : Loss for Batch-> 124\n",
            "0.009836581535637379 : Loss for Batch-> 125\n",
            "0.03603139519691467 : Loss for Batch-> 126\n",
            "0.0026704140473157167 : Loss for Batch-> 127\n",
            "0.0024470610078424215 : Loss for Batch-> 128\n",
            "0.028492247685790062 : Loss for Batch-> 129\n",
            "0.040085338056087494 : Loss for Batch-> 130\n",
            "0.12834195792675018 : Loss for Batch-> 131\n",
            "0.028944140300154686 : Val Loss after training 131 batche(s)\n",
            "0.014246946200728416 : Loss for Batch-> 132\n",
            "0.04052602872252464 : Loss for Batch-> 133\n",
            "0.04314679652452469 : Loss for Batch-> 134\n",
            "0.025573670864105225 : Loss for Batch-> 135\n",
            "0.20732513070106506 : Loss for Batch-> 136\n",
            "0.10928205400705338 : Loss for Batch-> 137\n",
            "0.030792344361543655 : Loss for Batch-> 138\n",
            "0.00043074999121017754 : Loss for Batch-> 139\n",
            "0.0007682500290684402 : Loss for Batch-> 140\n",
            "0.0005273452843539417 : Loss for Batch-> 141\n",
            "0.036031514406204224 : Val Loss after training 141 batche(s)\n",
            "0.0011881017126142979 : Loss for Batch-> 142\n",
            "0.010498312301933765 : Loss for Batch-> 143\n",
            "0.017399534583091736 : Loss for Batch-> 144\n",
            "0.011910686269402504 : Loss for Batch-> 145\n",
            "0.011447069235146046 : Loss for Batch-> 146\n",
            "0.0037218048237264156 : Loss for Batch-> 147\n",
            "0.0020234521944075823 : Loss for Batch-> 148\n",
            "0.0015232744626700878 : Loss for Batch-> 149\n",
            "0.026807459071278572 : Loss for Batch-> 150\n",
            "0.06836386024951935 : Loss for Batch-> 151\n",
            "0.04546690359711647 : Val Loss after training 151 batche(s)\n",
            "0.01903478242456913 : Loss for Batch-> 152\n",
            "0.01965397223830223 : Loss for Batch-> 153\n",
            "0.04452020302414894 : Loss for Batch-> 154\n",
            "0.03126782551407814 : Loss for Batch-> 155\n",
            "0.0011790511198341846 : Loss for Batch-> 156\n",
            "0.051096972078084946 : Loss for Batch-> 157\n",
            "0.014277517795562744 : Loss for Batch-> 158\n",
            "0.011375844478607178 : Loss for Batch-> 159\n",
            "0.0011141299037262797 : Loss for Batch-> 160\n",
            "0.06403839588165283 : Loss for Batch-> 161\n",
            "0.009912555105984211 : Val Loss after training 161 batche(s)\n",
            "0.059114933013916016 : Loss for Batch-> 162\n",
            "0.008527420461177826 : Loss for Batch-> 163\n",
            "0.03915178403258324 : Loss for Batch-> 164\n",
            "0.0465148389339447 : Loss for Batch-> 165\n",
            "0.047927238047122955 : Loss for Batch-> 166\n",
            "0.0285334512591362 : Loss for Batch-> 167\n",
            "0.022126352414488792 : Loss for Batch-> 168\n",
            "0.016958843916654587 : Loss for Batch-> 169\n",
            "0.00031982906511984766 : Loss for Batch-> 170\n",
            "0.0010088811395689845 : Loss for Batch-> 171\n",
            "0.002366679720580578 : Val Loss after training 171 batche(s)\n",
            "0.001353982137516141 : Loss for Batch-> 172\n",
            "0.00036746187834069133 : Loss for Batch-> 173\n",
            "0.0003326220903545618 : Loss for Batch-> 174\n",
            "0.0007236149976961315 : Loss for Batch-> 175\n",
            "0.015947697684168816 : Loss for Batch-> 176\n",
            "0.016565855592489243 : Loss for Batch-> 177\n",
            "0.0004490824358072132 : Loss for Batch-> 178\n",
            "0.00047113493201322854 : Loss for Batch-> 179\n",
            "0.00015235654427669942 : Loss for Batch-> 180\n",
            "8.975560376711655e-06 : Loss for Batch-> 181\n",
            "0.09398617595434189 : Val Loss after training 181 batche(s)\n",
            "3.469374496489763e-05 : Loss for Batch-> 182\n",
            "0.0006381887360475957 : Loss for Batch-> 183\n",
            "0.0001657914399402216 : Loss for Batch-> 184\n",
            "0.0007631970220245421 : Loss for Batch-> 185\n",
            "0.0003204974054824561 : Loss for Batch-> 186\n",
            "0.00042727030813694 : Loss for Batch-> 187\n",
            "0.0003547661763150245 : Loss for Batch-> 188\n",
            "0.0015318277291953564 : Loss for Batch-> 189\n",
            "0.013780257664620876 : Loss for Batch-> 190\n",
            "0.027820760384202003 : Loss for Batch-> 191\n",
            "0.07336971908807755 : Val Loss after training 191 batche(s)\n",
            "0.007613748777657747 : Loss for Batch-> 192\n",
            "0.003943670075386763 : Loss for Batch-> 193\n",
            "0.0008109516929835081 : Loss for Batch-> 194\n",
            "0.00020481666433624923 : Loss for Batch-> 195\n",
            "0.00020477594807744026 : Loss for Batch-> 196\n",
            "0.2505953311920166 : Loss for Batch-> 197\n",
            "2.873274564743042 : Loss for Batch-> 198\n",
            "0.035223837941884995 : Loss for Batch-> 199\n",
            "0.010928221046924591 : Loss for Batch-> 200\n",
            "0.07616881281137466 : Loss for Batch-> 201\n",
            "0.006039048545062542 : Val Loss after training 201 batche(s)\n",
            "0.03273503854870796 : Loss for Batch-> 202\n",
            "0.013973022811114788 : Loss for Batch-> 203\n",
            "0.024382418021559715 : Loss for Batch-> 204\n",
            "0.009845766238868237 : Loss for Batch-> 205\n",
            "0.007031974382698536 : Loss for Batch-> 206\n",
            "0.005667843855917454 : Loss for Batch-> 207\n",
            "0.006417233496904373 : Loss for Batch-> 208\n",
            "0.030945342034101486 : Loss for Batch-> 209\n",
            "0.019568899646401405 : Loss for Batch-> 210\n",
            "0.006822593044489622 : Loss for Batch-> 211\n",
            "0.07333460450172424 : Val Loss after training 211 batche(s)\n",
            "0.03326546028256416 : Loss for Batch-> 212\n",
            "0.02261953614652157 : Loss for Batch-> 213\n",
            "0.0360955148935318 : Loss for Batch-> 214\n",
            "0.021290816366672516 : Loss for Batch-> 215\n",
            "0.0006260431837290525 : Loss for Batch-> 216\n",
            "0.000514166837092489 : Loss for Batch-> 217\n",
            "0.015042143873870373 : Loss for Batch-> 218\n",
            "0.07488195598125458 : Loss for Batch-> 219\n",
            "0.02553246170282364 : Loss for Batch-> 220\n",
            "0.039205167442560196 : Loss for Batch-> 221\n",
            "0.17082154750823975 : Val Loss after training 221 batche(s)\n",
            "0.14137575030326843 : Loss for Batch-> 222\n",
            "0.0008255006978288293 : Loss for Batch-> 223\n",
            "0.00043080569594167173 : Loss for Batch-> 224\n",
            "0.0004359380982350558 : Loss for Batch-> 225\n",
            "0.001781982951797545 : Loss for Batch-> 226\n",
            "2.185060977935791 : Loss for Batch-> 227\n",
            "0.46704766154289246 : Loss for Batch-> 228\n",
            "10.884949684143066 : Loss for Batch-> 229\n",
            "0.4637020230293274 : Loss for Batch-> 230\n",
            "0.26249179244041443 : Loss for Batch-> 231\n",
            "5.304766654968262 : Val Loss after training 231 batche(s)\n",
            "0.0633024275302887 : Loss for Batch-> 232\n",
            "0.0006398718105629086 : Loss for Batch-> 233\n",
            "0.056773800402879715 : Loss for Batch-> 234\n",
            "0.012909194454550743 : Loss for Batch-> 235\n",
            "0.0016320043941959739 : Loss for Batch-> 236\n",
            "0.0009908834472298622 : Loss for Batch-> 237\n",
            "0.025879688560962677 : Loss for Batch-> 238\n",
            "0.10412956029176712 : Loss for Batch-> 239\n",
            "0.1033029556274414 : Loss for Batch-> 240\n",
            "0.13378219306468964 : Loss for Batch-> 241\n",
            "0.04185464233160019 : Val Loss after training 241 batche(s)\n",
            "0.6588889956474304 : Loss for Batch-> 242\n",
            "1.0088410377502441 : Loss for Batch-> 243\n",
            "4.116391658782959 : Loss for Batch-> 244\n",
            "0.015575282275676727 : Loss for Batch-> 245\n",
            "0.03658919036388397 : Loss for Batch-> 246\n",
            "4.405184745788574 : Loss for Batch-> 247\n",
            "0.0740647092461586 : Loss for Batch-> 248\n",
            "1.156440258026123 : Loss for Batch-> 249\n",
            "0.016015270724892616 : Loss for Batch-> 250\n",
            "0.2217446267604828 : Loss for Batch-> 251\n",
            "0.0486174114048481 : Val Loss after training 251 batche(s)\n",
            "0.42131099104881287 : Loss for Batch-> 252\n",
            "0.49647390842437744 : Loss for Batch-> 253\n",
            "2.2688722610473633 : Loss for Batch-> 254\n",
            "6.983949661254883 : Loss for Batch-> 255\n",
            "0.31512585282325745 : Loss for Batch-> 256\n",
            "0.004273734521120787 : Loss for Batch-> 257\n",
            "0.629018247127533 : Loss for Batch-> 258\n",
            "0.22974874079227448 : Loss for Batch-> 259\n",
            "4.65844202041626 : Loss for Batch-> 260\n",
            "1.6074644327163696 : Loss for Batch-> 261\n",
            "0.0007328447536565363 : Val Loss after training 261 batche(s)\n",
            "0.03358437865972519 : Loss for Batch-> 262\n",
            "0.021448904648423195 : Loss for Batch-> 263\n",
            "1.460231065750122 : Loss for Batch-> 264\n",
            "0.6590892672538757 : Loss for Batch-> 265\n",
            "0.1341606080532074 : Loss for Batch-> 266\n",
            "0.0008318837499246001 : Loss for Batch-> 267\n",
            "0.34341365098953247 : Loss for Batch-> 268\n",
            "5.5618977546691895 : Loss for Batch-> 269\n",
            "0.09634143859148026 : Loss for Batch-> 270\n",
            "0.0013377598952502012 : Loss for Batch-> 271\n",
            "0.0014038427034392953 : Val Loss after training 271 batche(s)\n",
            "0.0007427896489389241 : Loss for Batch-> 272\n",
            "0.0018235709285363555 : Loss for Batch-> 273\n",
            "0.0022994144819676876 : Loss for Batch-> 274\n",
            "0.001090054283849895 : Loss for Batch-> 275\n",
            "0.002918919315561652 : Loss for Batch-> 276\n",
            "0.000836792285554111 : Loss for Batch-> 277\n",
            "0.01937713287770748 : Loss for Batch-> 278\n",
            "0.010892672464251518 : Loss for Batch-> 279\n",
            "0.05449257045984268 : Loss for Batch-> 280\n",
            "0.06902702152729034 : Loss for Batch-> 281\n",
            "0.001976961735635996 : Val Loss after training 281 batche(s)\n",
            "0.06955075263977051 : Loss for Batch-> 282\n",
            "0.05827174335718155 : Loss for Batch-> 283\n",
            "0.006546147633343935 : Loss for Batch-> 284\n",
            "3.876513957977295 : Loss for Batch-> 285\n",
            "10.503830909729004 : Loss for Batch-> 286\n",
            "0.01049776654690504 : Loss for Batch-> 287\n",
            "0.007673955988138914 : Loss for Batch-> 288\n",
            "0.002975223585963249 : Loss for Batch-> 289\n",
            "0.13235361874103546 : Loss for Batch-> 290\n",
            "0.0468018501996994 : Loss for Batch-> 291\n",
            "0.0008672529947943985 : Val Loss after training 291 batche(s)\n",
            "0.004031071439385414 : Loss for Batch-> 292\n",
            "0.0013388502411544323 : Loss for Batch-> 293\n",
            "0.0012666435213759542 : Loss for Batch-> 294\n",
            "0.05330495536327362 : Loss for Batch-> 295\n",
            "0.01845145784318447 : Loss for Batch-> 296\n",
            "0.005241346545517445 : Loss for Batch-> 297\n",
            "0.037920717149972916 : Loss for Batch-> 298\n",
            "0.2665889859199524 : Loss for Batch-> 299\n",
            "1.183880090713501 : Loss for Batch-> 300\n",
            "1.0635337829589844 : Loss for Batch-> 301\n",
            "0.000727156933862716 : Val Loss after training 301 batche(s)\n",
            "0.17095035314559937 : Loss for Batch-> 302\n",
            "0.09044896811246872 : Loss for Batch-> 303\n",
            "0.17933793365955353 : Loss for Batch-> 304\n",
            "1.9132652282714844 : Loss for Batch-> 305\n",
            "1.7008941173553467 : Loss for Batch-> 306\n",
            "0.18516092002391815 : Loss for Batch-> 307\n",
            "0.0591626837849617 : Loss for Batch-> 308\n",
            "0.02332097850739956 : Loss for Batch-> 309\n",
            "1.0672649145126343 : Loss for Batch-> 310\n",
            "1.4499619007110596 : Loss for Batch-> 311\n",
            "0.009637560695409775 : Val Loss after training 311 batche(s)\n",
            "0.47416722774505615 : Loss for Batch-> 312\n",
            "0.020781660452485085 : Loss for Batch-> 313\n",
            "0.002193917753174901 : Loss for Batch-> 314\n",
            "0.38838332891464233 : Loss for Batch-> 315\n",
            "0.25023353099823 : Loss for Batch-> 316\n",
            "0.019471146166324615 : Loss for Batch-> 317\n",
            "0.028712697327136993 : Loss for Batch-> 318\n",
            "0.21497923135757446 : Loss for Batch-> 319\n",
            "0.12135820090770721 : Loss for Batch-> 320\n",
            "0.2993055284023285 : Loss for Batch-> 321\n",
            "0.015922779217362404 : Val Loss after training 321 batche(s)\n",
            "0.20218992233276367 : Loss for Batch-> 322\n",
            "0.041005656123161316 : Loss for Batch-> 323\n",
            "0.16166886687278748 : Loss for Batch-> 324\n",
            "0.24760514497756958 : Loss for Batch-> 325\n",
            "0.48267069458961487 : Loss for Batch-> 326\n",
            "0.4621104300022125 : Loss for Batch-> 327\n",
            "0.06678590178489685 : Loss for Batch-> 328\n",
            "0.09118455648422241 : Loss for Batch-> 329\n",
            "0.08242014050483704 : Loss for Batch-> 330\n",
            "0.014714961871504784 : Loss for Batch-> 331\n",
            "0.00014049540914129466 : Val Loss after training 331 batche(s)\n",
            "0.0064645446836948395 : Loss for Batch-> 332\n",
            "0.007124226540327072 : Loss for Batch-> 333\n",
            "0.0004028943076264113 : Loss for Batch-> 334\n",
            "0.0006038868450559676 : Loss for Batch-> 335\n",
            "0.0018875424284487963 : Loss for Batch-> 336\n",
            "0.002045240020379424 : Loss for Batch-> 337\n",
            "0.03401918709278107 : Loss for Batch-> 338\n",
            "0.054227299988269806 : Loss for Batch-> 339\n",
            "0.06175148859620094 : Loss for Batch-> 340\n",
            "0.034739281982183456 : Loss for Batch-> 341\n",
            "0.004080192651599646 : Val Loss after training 341 batche(s)\n",
            "0.373953640460968 : Loss for Batch-> 342\n",
            "0.21244527399539948 : Loss for Batch-> 343\n",
            "0.013357741758227348 : Loss for Batch-> 344\n",
            "0.029702717438340187 : Loss for Batch-> 345\n",
            "0.20194503664970398 : Loss for Batch-> 346\n",
            "0.45209991931915283 : Loss for Batch-> 347\n",
            "0.09763989597558975 : Loss for Batch-> 348\n",
            "0.002919067395851016 : Loss for Batch-> 349\n",
            "0.7035270929336548 : Loss for Batch-> 350\n",
            "0.759928286075592 : Loss for Batch-> 351\n",
            "0.0027639379259198904 : Val Loss after training 351 batche(s)\n",
            "0.04925396293401718 : Loss for Batch-> 352\n",
            "0.03115532360970974 : Loss for Batch-> 353\n",
            "1.3657857179641724 : Loss for Batch-> 354\n",
            "0.5426560044288635 : Loss for Batch-> 355\n",
            "0.008219177834689617 : Loss for Batch-> 356\n",
            "0.005550742615014315 : Loss for Batch-> 357\n",
            "0.01830948330461979 : Loss for Batch-> 358\n",
            "0.5472445487976074 : Loss for Batch-> 359\n",
            "0.0380270816385746 : Loss for Batch-> 360\n",
            "0.2300766408443451 : Loss for Batch-> 361\n",
            "0.0586627721786499 : Val Loss after training 361 batche(s)\n",
            "0.02018965408205986 : Loss for Batch-> 362\n",
            "0.13847097754478455 : Loss for Batch-> 363\n",
            "0.38384154438972473 : Loss for Batch-> 364\n",
            "0.10866042971611023 : Loss for Batch-> 365\n",
            "0.20541533827781677 : Loss for Batch-> 366\n",
            "0.1917993426322937 : Loss for Batch-> 367\n",
            "0.008206280879676342 : Loss for Batch-> 368\n",
            "0.024532057344913483 : Loss for Batch-> 369\n",
            "0.0548398494720459 : Loss for Batch-> 370\n",
            "0.33291980624198914 : Loss for Batch-> 371\n",
            "0.013931120745837688 : Val Loss after training 371 batche(s)\n",
            "0.6519712805747986 : Loss for Batch-> 372\n",
            "0.11617159843444824 : Loss for Batch-> 373\n",
            "1.0257883071899414 : Loss for Batch-> 374\n",
            "0.8598523736000061 : Loss for Batch-> 375\n",
            "0.3010331094264984 : Loss for Batch-> 376\n",
            "0.40590140223503113 : Loss for Batch-> 377\n",
            "0.030402112752199173 : Loss for Batch-> 378\n",
            "0.028961438685655594 : Loss for Batch-> 379\n",
            "0.06954783201217651 : Loss for Batch-> 380\n",
            "0.18434017896652222 : Loss for Batch-> 381\n",
            "0.001038621412590146 : Val Loss after training 381 batche(s)\n",
            "0.6459442973136902 : Loss for Batch-> 382\n",
            "0.27043265104293823 : Loss for Batch-> 383\n",
            "2.122012138366699 : Loss for Batch-> 384\n",
            "0.5676711201667786 : Loss for Batch-> 385\n",
            "0.9073826670646667 : Loss for Batch-> 386\n",
            "0.07913130521774292 : Loss for Batch-> 387\n",
            "0.005735705606639385 : Loss for Batch-> 388\n",
            "0.1939125657081604 : Loss for Batch-> 389\n",
            "1.2152881622314453 : Loss for Batch-> 390\n",
            "1.2471822500228882 : Loss for Batch-> 391\n",
            "0.0020909709855914116 : Val Loss after training 391 batche(s)\n",
            "0.07533761113882065 : Loss for Batch-> 392\n",
            "0.1083536148071289 : Loss for Batch-> 393\n",
            "0.016890157014131546 : Loss for Batch-> 394\n",
            "0.09302572906017303 : Loss for Batch-> 395\n",
            "0.38193386793136597 : Loss for Batch-> 396\n",
            "0.2737933099269867 : Loss for Batch-> 397\n",
            "1.0945916175842285 : Loss for Batch-> 398\n",
            "0.2631102204322815 : Loss for Batch-> 399\n",
            "0.007840562611818314 : Loss for Batch-> 400\n",
            "0.007915753871202469 : Loss for Batch-> 401\n",
            "0.17351150512695312 : Val Loss after training 401 batche(s)\n",
            "0.016974162310361862 : Loss for Batch-> 402\n",
            "0.021796176210045815 : Loss for Batch-> 403\n",
            "0.527233898639679 : Loss for Batch-> 404\n",
            "0.49331387877464294 : Loss for Batch-> 405\n",
            "0.17367449402809143 : Loss for Batch-> 406\n",
            "0.04810386151075363 : Loss for Batch-> 407\n",
            "0.03101949207484722 : Loss for Batch-> 408\n",
            "0.015104380436241627 : Loss for Batch-> 409\n",
            "0.15734133124351501 : Loss for Batch-> 410\n",
            "1.1411871910095215 : Loss for Batch-> 411\n",
            "0.2812701463699341 : Val Loss after training 411 batche(s)\n",
            "0.8060673475265503 : Loss for Batch-> 412\n",
            "0.07584211230278015 : Loss for Batch-> 413\n",
            "0.07170572131872177 : Loss for Batch-> 414\n",
            "0.0008618788560852408 : Loss for Batch-> 415\n",
            "0.0016925453674048185 : Loss for Batch-> 416\n",
            "0.0065560610964894295 : Loss for Batch-> 417\n",
            "0.02652270719408989 : Loss for Batch-> 418\n",
            "0.0186674352735281 : Loss for Batch-> 419\n",
            "0.05328988656401634 : Loss for Batch-> 420\n",
            "0.17430783808231354 : Loss for Batch-> 421\n",
            "0.009101590141654015 : Val Loss after training 421 batche(s)\n",
            "0.05214221775531769 : Loss for Batch-> 422\n",
            "0.02284761145710945 : Loss for Batch-> 423\n",
            "0.022387677803635597 : Loss for Batch-> 424\n",
            "0.022350842133164406 : Loss for Batch-> 425\n",
            "0.022776227444410324 : Loss for Batch-> 426\n",
            "0.024082131683826447 : Loss for Batch-> 427\n",
            "0.024290069937705994 : Loss for Batch-> 428\n",
            "0.02354712411761284 : Loss for Batch-> 429\n",
            "0.023147951811552048 : Loss for Batch-> 430\n",
            "0.022173918783664703 : Loss for Batch-> 431\n",
            "0.0036201286129653454 : Val Loss after training 431 batche(s)\n",
            "0.13829006254673004 : Loss for Batch-> 432\n",
            "0.041941605508327484 : Loss for Batch-> 433\n",
            "0.008700535632669926 : Loss for Batch-> 434\n",
            "0.003108087694272399 : Loss for Batch-> 435\n",
            "0.0014101463602855802 : Loss for Batch-> 436\n",
            "0.0005498126265592873 : Loss for Batch-> 437\n",
            "0.002574617275968194 : Loss for Batch-> 438\n",
            "0.0045738834887743 : Loss for Batch-> 439\n",
            "0.06082106754183769 : Loss for Batch-> 440\n",
            "0.05043726786971092 : Loss for Batch-> 441\n",
            "0.005083373282104731 : Val Loss after training 441 batche(s)\n",
            "0.008050736039876938 : Loss for Batch-> 442\n",
            "0.03348494693636894 : Loss for Batch-> 443\n",
            "0.007334097754210234 : Loss for Batch-> 444\n",
            "0.04507117345929146 : Loss for Batch-> 445\n",
            "0.006486226338893175 : Loss for Batch-> 446\n",
            "0.3213414251804352 : Loss for Batch-> 447\n",
            "0.4280421733856201 : Loss for Batch-> 448\n",
            "0.3259510099887848 : Loss for Batch-> 449\n",
            "0.0006580158369615674 : Loss for Batch-> 450\n",
            "0.001380337169393897 : Loss for Batch-> 451\n",
            "0.004811822436749935 : Val Loss after training 451 batche(s)\n",
            "0.00047822852502577007 : Loss for Batch-> 452\n",
            "0.0011493400670588017 : Loss for Batch-> 453\n",
            "0.0009624859085306525 : Loss for Batch-> 454\n",
            "0.0003336470981594175 : Loss for Batch-> 455\n",
            "0.0010259468108415604 : Loss for Batch-> 456\n",
            "0.00014376478793565184 : Loss for Batch-> 457\n",
            "0.00014357658801600337 : Loss for Batch-> 458\n",
            "0.00014266867947299033 : Loss for Batch-> 459\n",
            "0.00014102415298111737 : Loss for Batch-> 460\n",
            "0.00016024117940105498 : Loss for Batch-> 461\n",
            "0.004942004568874836 : Val Loss after training 461 batche(s)\n",
            "0.0001528475695522502 : Loss for Batch-> 462\n",
            "0.00012543241609819233 : Loss for Batch-> 463\n",
            "0.00012450083158910275 : Loss for Batch-> 464\n",
            "0.00012529660307336599 : Loss for Batch-> 465\n",
            "0.0012166486121714115 : Loss for Batch-> 466\n",
            "0.0010732858208939433 : Loss for Batch-> 467\n",
            "0.0005236874567344785 : Loss for Batch-> 468\n",
            "0.001251759473234415 : Loss for Batch-> 469\n",
            "0.0002189035585615784 : Loss for Batch-> 470\n",
            "0.0004636189842130989 : Loss for Batch-> 471\n",
            "0.011762299574911594 : Val Loss after training 471 batche(s)\n",
            "0.00039749735151417553 : Loss for Batch-> 472\n",
            "0.0008319537155330181 : Loss for Batch-> 473\n",
            "0.0005738798645325005 : Loss for Batch-> 474\n",
            "0.0011518249521031976 : Loss for Batch-> 475\n",
            "0.0005293813883326948 : Loss for Batch-> 476\n",
            "0.0012128633679822087 : Loss for Batch-> 477\n",
            "0.00048402632819488645 : Loss for Batch-> 478\n",
            "0.0005858155200257897 : Loss for Batch-> 479\n",
            "0.0008843077230267227 : Loss for Batch-> 480\n",
            "0.0014388341223821044 : Loss for Batch-> 481\n",
            "1.1678757667541504 : Val Loss after training 481 batche(s)\n",
            "0.028714148327708244 : Loss for Batch-> 482\n",
            "0.004742620512843132 : Loss for Batch-> 483\n",
            "0.003203894477337599 : Loss for Batch-> 484\n",
            "0.0034896493889391422 : Loss for Batch-> 485\n",
            "0.0036964397877454758 : Loss for Batch-> 486\n",
            "0.0036210399121046066 : Loss for Batch-> 487\n",
            "0.0035103531554341316 : Loss for Batch-> 488\n",
            "0.0036095688119530678 : Loss for Batch-> 489\n",
            "0.003557003801688552 : Loss for Batch-> 490\n",
            "0.002113206312060356 : Loss for Batch-> 491\n",
            "1.9769343137741089 : Val Loss after training 491 batche(s)\n",
            "0.0021370630711317062 : Loss for Batch-> 492\n",
            "0.002074315445497632 : Loss for Batch-> 493\n",
            "0.18510831892490387 : Loss for Batch-> 494\n",
            "1.5744634866714478 : Loss for Batch-> 495\n",
            "0.0038550137542188168 : Loss for Batch-> 496\n",
            "0.020688969641923904 : Loss for Batch-> 497\n",
            "0.0011933845235034823 : Loss for Batch-> 498\n",
            "0.0010306003969162703 : Loss for Batch-> 499\n",
            "0.0013527829432860017 : Loss for Batch-> 500\n",
            "0.002406401326879859 : Loss for Batch-> 501\n",
            "0.00021979599841870368 : Val Loss after training 501 batche(s)\n",
            "0.004100950434803963 : Loss for Batch-> 502\n",
            "0.036120664328336716 : Loss for Batch-> 503\n",
            "0.020506717264652252 : Loss for Batch-> 504\n",
            "0.012994401156902313 : Loss for Batch-> 505\n",
            "0.019962824881076813 : Loss for Batch-> 506\n",
            "0.019314400851726532 : Loss for Batch-> 507\n",
            "0.019189277663826942 : Loss for Batch-> 508\n",
            "0.01290422584861517 : Loss for Batch-> 509\n",
            "0.009014845825731754 : Loss for Batch-> 510\n",
            "26 Epoch\n",
            "0.008722692728042603 : Loss for Batch-> 1\n",
            "0.006976738106459379 : Val Loss after training 1 batche(s)\n",
            "0.00879280362278223 : Loss for Batch-> 2\n",
            "0.008684460073709488 : Loss for Batch-> 3\n",
            "0.008437755517661572 : Loss for Batch-> 4\n",
            "0.008059128187596798 : Loss for Batch-> 5\n",
            "0.007705383934080601 : Loss for Batch-> 6\n",
            "0.007689964026212692 : Loss for Batch-> 7\n",
            "0.013464703224599361 : Loss for Batch-> 8\n",
            "0.017783455550670624 : Loss for Batch-> 9\n",
            "0.017921650782227516 : Loss for Batch-> 10\n",
            "0.01765577495098114 : Loss for Batch-> 11\n",
            "0.016827275976538658 : Val Loss after training 11 batche(s)\n",
            "2.093519926071167 : Loss for Batch-> 12\n",
            "4.10894250869751 : Loss for Batch-> 13\n",
            "0.0011101370910182595 : Loss for Batch-> 14\n",
            "0.001726842951029539 : Loss for Batch-> 15\n",
            "0.0014142163563519716 : Loss for Batch-> 16\n",
            "0.0013045440427958965 : Loss for Batch-> 17\n",
            "0.008816481567919254 : Loss for Batch-> 18\n",
            "0.007368078920990229 : Loss for Batch-> 19\n",
            "0.0009881887817755342 : Loss for Batch-> 20\n",
            "0.01692028157413006 : Loss for Batch-> 21\n",
            "0.0010358929866924882 : Val Loss after training 21 batche(s)\n",
            "0.020311105996370316 : Loss for Batch-> 22\n",
            "0.021664254367351532 : Loss for Batch-> 23\n",
            "0.01769072562456131 : Loss for Batch-> 24\n",
            "0.01802222803235054 : Loss for Batch-> 25\n",
            "0.010361743159592152 : Loss for Batch-> 26\n",
            "0.0007485647220164537 : Loss for Batch-> 27\n",
            "0.0004046008107252419 : Loss for Batch-> 28\n",
            "0.002276712330058217 : Loss for Batch-> 29\n",
            "0.0033242329955101013 : Loss for Batch-> 30\n",
            "0.002295590005815029 : Loss for Batch-> 31\n",
            "0.002561725676059723 : Val Loss after training 31 batche(s)\n",
            "0.0010777143761515617 : Loss for Batch-> 32\n",
            "3.827900945907459e-05 : Loss for Batch-> 33\n",
            "5.2001782023580745e-05 : Loss for Batch-> 34\n",
            "2.6432026061229408e-05 : Loss for Batch-> 35\n",
            "0.0008545804885216057 : Loss for Batch-> 36\n",
            "0.0004076434997841716 : Loss for Batch-> 37\n",
            "0.00027565882191993296 : Loss for Batch-> 38\n",
            "0.014413602650165558 : Loss for Batch-> 39\n",
            "0.0007847807719372213 : Loss for Batch-> 40\n",
            "0.0002043784043053165 : Loss for Batch-> 41\n",
            "0.0004758053692057729 : Val Loss after training 41 batche(s)\n",
            "0.0008036165963858366 : Loss for Batch-> 42\n",
            "0.0003484175249468535 : Loss for Batch-> 43\n",
            "0.00250630103982985 : Loss for Batch-> 44\n",
            "0.07076197117567062 : Loss for Batch-> 45\n",
            "0.06201768293976784 : Loss for Batch-> 46\n",
            "0.06408247351646423 : Loss for Batch-> 47\n",
            "0.009104946628212929 : Loss for Batch-> 48\n",
            "0.0520789809525013 : Loss for Batch-> 49\n",
            "0.2441275715827942 : Loss for Batch-> 50\n",
            "0.25275692343711853 : Loss for Batch-> 51\n",
            "0.0015292614698410034 : Val Loss after training 51 batche(s)\n",
            "0.07629599422216415 : Loss for Batch-> 52\n",
            "0.001011135522276163 : Loss for Batch-> 53\n",
            "0.0007745178299956024 : Loss for Batch-> 54\n",
            "0.01612677052617073 : Loss for Batch-> 55\n",
            "0.0179031603038311 : Loss for Batch-> 56\n",
            "0.011206554248929024 : Loss for Batch-> 57\n",
            "0.0009162534843198955 : Loss for Batch-> 58\n",
            "0.004326141905039549 : Loss for Batch-> 59\n",
            "0.0177552942186594 : Loss for Batch-> 60\n",
            "0.20353864133358002 : Loss for Batch-> 61\n",
            "0.01790480688214302 : Val Loss after training 61 batche(s)\n",
            "0.22964005172252655 : Loss for Batch-> 62\n",
            "0.11135952919721603 : Loss for Batch-> 63\n",
            "0.06843257695436478 : Loss for Batch-> 64\n",
            "0.08755732327699661 : Loss for Batch-> 65\n",
            "0.017853986471891403 : Loss for Batch-> 66\n",
            "0.13027328252792358 : Loss for Batch-> 67\n",
            "0.05259433761239052 : Loss for Batch-> 68\n",
            "0.07713988423347473 : Loss for Batch-> 69\n",
            "0.011503027752041817 : Loss for Batch-> 70\n",
            "0.011727716773748398 : Loss for Batch-> 71\n",
            "0.004094988107681274 : Val Loss after training 71 batche(s)\n",
            "0.01099369302392006 : Loss for Batch-> 72\n",
            "0.010408151894807816 : Loss for Batch-> 73\n",
            "0.010848786681890488 : Loss for Batch-> 74\n",
            "0.01156812347471714 : Loss for Batch-> 75\n",
            "0.022025318816304207 : Loss for Batch-> 76\n",
            "1.921471357345581 : Loss for Batch-> 77\n",
            "0.6457633376121521 : Loss for Batch-> 78\n",
            "0.07874370366334915 : Loss for Batch-> 79\n",
            "0.0862068459391594 : Loss for Batch-> 80\n",
            "0.06217065826058388 : Loss for Batch-> 81\n",
            "0.00927514024078846 : Val Loss after training 81 batche(s)\n",
            "0.0009244158281944692 : Loss for Batch-> 82\n",
            "0.0036420701071619987 : Loss for Batch-> 83\n",
            "0.07732073962688446 : Loss for Batch-> 84\n",
            "0.08331479132175446 : Loss for Batch-> 85\n",
            "0.0907791405916214 : Loss for Batch-> 86\n",
            "0.04636628180742264 : Loss for Batch-> 87\n",
            "0.021709134802222252 : Loss for Batch-> 88\n",
            "0.019162315875291824 : Loss for Batch-> 89\n",
            "0.04718939960002899 : Loss for Batch-> 90\n",
            "0.011441689915955067 : Loss for Batch-> 91\n",
            "0.0014980282867327332 : Val Loss after training 91 batche(s)\n",
            "0.016222402453422546 : Loss for Batch-> 92\n",
            "0.043363332748413086 : Loss for Batch-> 93\n",
            "0.022800572216510773 : Loss for Batch-> 94\n",
            "0.009401562623679638 : Loss for Batch-> 95\n",
            "0.010963968001306057 : Loss for Batch-> 96\n",
            "0.0014445828273892403 : Loss for Batch-> 97\n",
            "0.0006954569835215807 : Loss for Batch-> 98\n",
            "0.002583257621154189 : Loss for Batch-> 99\n",
            "0.00659320130944252 : Loss for Batch-> 100\n",
            "0.0019835359416902065 : Loss for Batch-> 101\n",
            "0.001181097817607224 : Val Loss after training 101 batche(s)\n",
            "0.002909438917413354 : Loss for Batch-> 102\n",
            "0.0017278727609664202 : Loss for Batch-> 103\n",
            "0.016846049576997757 : Loss for Batch-> 104\n",
            "0.018697477877140045 : Loss for Batch-> 105\n",
            "0.016670694574713707 : Loss for Batch-> 106\n",
            "0.017008021473884583 : Loss for Batch-> 107\n",
            "0.0009755276842042804 : Loss for Batch-> 108\n",
            "0.0024781455285847187 : Loss for Batch-> 109\n",
            "0.026724252849817276 : Loss for Batch-> 110\n",
            "0.0123831108212471 : Loss for Batch-> 111\n",
            "0.0010116423945873976 : Val Loss after training 111 batche(s)\n",
            "0.005355075001716614 : Loss for Batch-> 112\n",
            "0.0011505505535751581 : Loss for Batch-> 113\n",
            "0.009739604778587818 : Loss for Batch-> 114\n",
            "0.0426131933927536 : Loss for Batch-> 115\n",
            "0.0550931915640831 : Loss for Batch-> 116\n",
            "0.02281651273369789 : Loss for Batch-> 117\n",
            "0.0179231446236372 : Loss for Batch-> 118\n",
            "0.06552651524543762 : Loss for Batch-> 119\n",
            "0.000842575856950134 : Loss for Batch-> 120\n",
            "0.013411999680101871 : Loss for Batch-> 121\n",
            "0.015388253144919872 : Val Loss after training 121 batche(s)\n",
            "0.012610389851033688 : Loss for Batch-> 122\n",
            "0.09506511688232422 : Loss for Batch-> 123\n",
            "0.07522217929363251 : Loss for Batch-> 124\n",
            "0.019085245206952095 : Loss for Batch-> 125\n",
            "0.022599957883358 : Loss for Batch-> 126\n",
            "0.018996573984622955 : Loss for Batch-> 127\n",
            "0.003177737584337592 : Loss for Batch-> 128\n",
            "0.007102872245013714 : Loss for Batch-> 129\n",
            "0.03363858535885811 : Loss for Batch-> 130\n",
            "0.09562350064516068 : Loss for Batch-> 131\n",
            "0.14506012201309204 : Val Loss after training 131 batche(s)\n",
            "0.06306491047143936 : Loss for Batch-> 132\n",
            "0.05224716663360596 : Loss for Batch-> 133\n",
            "0.003000243566930294 : Loss for Batch-> 134\n",
            "0.060345713049173355 : Loss for Batch-> 135\n",
            "0.04239775985479355 : Loss for Batch-> 136\n",
            "0.26560720801353455 : Loss for Batch-> 137\n",
            "0.017870834097266197 : Loss for Batch-> 138\n",
            "0.028549034148454666 : Loss for Batch-> 139\n",
            "0.0004339589213486761 : Loss for Batch-> 140\n",
            "0.0008993947994895279 : Loss for Batch-> 141\n",
            "0.07624046504497528 : Val Loss after training 141 batche(s)\n",
            "0.00038626373861916363 : Loss for Batch-> 142\n",
            "0.0038493480533361435 : Loss for Batch-> 143\n",
            "0.012209921143949032 : Loss for Batch-> 144\n",
            "0.017901316285133362 : Loss for Batch-> 145\n",
            "0.011153561063110828 : Loss for Batch-> 146\n",
            "0.009150456637144089 : Loss for Batch-> 147\n",
            "0.001929296413436532 : Loss for Batch-> 148\n",
            "0.001920275273732841 : Loss for Batch-> 149\n",
            "0.002196740824729204 : Loss for Batch-> 150\n",
            "0.04954398050904274 : Loss for Batch-> 151\n",
            "0.000844379945192486 : Val Loss after training 151 batche(s)\n",
            "0.06227071210741997 : Loss for Batch-> 152\n",
            "0.0017745845252647996 : Loss for Batch-> 153\n",
            "0.0370183065533638 : Loss for Batch-> 154\n",
            "0.039591189473867416 : Loss for Batch-> 155\n",
            "0.01937691494822502 : Loss for Batch-> 156\n",
            "0.009746282361447811 : Loss for Batch-> 157\n",
            "0.055961184203624725 : Loss for Batch-> 158\n",
            "0.0011192418169230223 : Loss for Batch-> 159\n",
            "0.010611853562295437 : Loss for Batch-> 160\n",
            "0.0226626917719841 : Loss for Batch-> 161\n",
            "0.0015514906262978911 : Val Loss after training 161 batche(s)\n",
            "0.07479088008403778 : Loss for Batch-> 162\n",
            "0.0342547744512558 : Loss for Batch-> 163\n",
            "0.013255727477371693 : Loss for Batch-> 164\n",
            "0.04589139297604561 : Loss for Batch-> 165\n",
            "0.050380639731884 : Loss for Batch-> 166\n",
            "0.04755498841404915 : Loss for Batch-> 167\n",
            "0.0070630996488034725 : Loss for Batch-> 168\n",
            "0.03363919258117676 : Loss for Batch-> 169\n",
            "0.004464615136384964 : Loss for Batch-> 170\n",
            "0.00018351190374232829 : Loss for Batch-> 171\n",
            "0.0013112170854583383 : Val Loss after training 171 batche(s)\n",
            "0.001021318486891687 : Loss for Batch-> 172\n",
            "0.0015459618298336864 : Loss for Batch-> 173\n",
            "0.00019901234190911055 : Loss for Batch-> 174\n",
            "0.0003636143228504807 : Loss for Batch-> 175\n",
            "0.00323282927274704 : Loss for Batch-> 176\n",
            "0.024941660463809967 : Loss for Batch-> 177\n",
            "0.005133066792041063 : Loss for Batch-> 178\n",
            "0.0005067978054285049 : Loss for Batch-> 179\n",
            "0.00044324196642264724 : Loss for Batch-> 180\n",
            "3.4829216019716114e-05 : Loss for Batch-> 181\n",
            "0.002746500074863434 : Val Loss after training 181 batche(s)\n",
            "8.839106158120558e-06 : Loss for Batch-> 182\n",
            "0.000264105387032032 : Loss for Batch-> 183\n",
            "0.0004770787199959159 : Loss for Batch-> 184\n",
            "0.00013052507711108774 : Loss for Batch-> 185\n",
            "0.0010084478417411447 : Loss for Batch-> 186\n",
            "0.00016852101543918252 : Loss for Batch-> 187\n",
            "0.0006051001837477088 : Loss for Batch-> 188\n",
            "0.0005226199864409864 : Loss for Batch-> 189\n",
            "0.003224062966182828 : Loss for Batch-> 190\n",
            "0.018198875710368156 : Loss for Batch-> 191\n",
            "0.00540071539580822 : Val Loss after training 191 batche(s)\n",
            "0.026283254846930504 : Loss for Batch-> 192\n",
            "0.005902240052819252 : Loss for Batch-> 193\n",
            "0.0009461997542530298 : Loss for Batch-> 194\n",
            "0.0007168467273004353 : Loss for Batch-> 195\n",
            "0.00021435228700283915 : Loss for Batch-> 196\n",
            "0.0015872539952397346 : Loss for Batch-> 197\n",
            "2.111260414123535 : Loss for Batch-> 198\n",
            "1.0126605033874512 : Loss for Batch-> 199\n",
            "0.03911638632416725 : Loss for Batch-> 200\n",
            "0.03713715076446533 : Loss for Batch-> 201\n",
            "0.0069902376271784306 : Val Loss after training 201 batche(s)\n",
            "0.07166921347379684 : Loss for Batch-> 202\n",
            "0.015532896853983402 : Loss for Batch-> 203\n",
            "0.013365354388952255 : Loss for Batch-> 204\n",
            "0.01578042469918728 : Loss for Batch-> 205\n",
            "0.011033875867724419 : Loss for Batch-> 206\n",
            "0.006271944846957922 : Loss for Batch-> 207\n",
            "0.005761997774243355 : Loss for Batch-> 208\n",
            "0.007251260802149773 : Loss for Batch-> 209\n",
            "0.044850464910268784 : Loss for Batch-> 210\n",
            "0.0027319116052240133 : Loss for Batch-> 211\n",
            "0.007277392782270908 : Val Loss after training 211 batche(s)\n",
            "0.02229399047791958 : Loss for Batch-> 212\n",
            "0.029804885387420654 : Loss for Batch-> 213\n",
            "0.011184846051037312 : Loss for Batch-> 214\n",
            "0.05562663450837135 : Loss for Batch-> 215\n",
            "0.0006367320311255753 : Loss for Batch-> 216\n",
            "0.0004289458738639951 : Loss for Batch-> 217\n",
            "0.0005234598647803068 : Loss for Batch-> 218\n",
            "0.04648693650960922 : Loss for Batch-> 219\n",
            "0.0652734637260437 : Loss for Batch-> 220\n",
            "0.006728494074195623 : Loss for Batch-> 221\n",
            "0.007222778629511595 : Val Loss after training 221 batche(s)\n",
            "0.12351924926042557 : Loss for Batch-> 222\n",
            "0.05500686541199684 : Loss for Batch-> 223\n",
            "0.0003863691526930779 : Loss for Batch-> 224\n",
            "0.00045745811075903475 : Loss for Batch-> 225\n",
            "0.0004513220046646893 : Loss for Batch-> 226\n",
            "0.18631362915039062 : Loss for Batch-> 227\n",
            "2.2556512355804443 : Loss for Batch-> 228\n",
            "0.46776917576789856 : Loss for Batch-> 229\n",
            "11.077454566955566 : Loss for Batch-> 230\n",
            "0.06860987842082977 : Loss for Batch-> 231\n",
            "0.007697132416069508 : Val Loss after training 231 batche(s)\n",
            "0.25245821475982666 : Loss for Batch-> 232\n",
            "0.015377524308860302 : Loss for Batch-> 233\n",
            "0.016040503978729248 : Loss for Batch-> 234\n",
            "0.050918322056531906 : Loss for Batch-> 235\n",
            "0.004357984755188227 : Loss for Batch-> 236\n",
            "0.0008902515401132405 : Loss for Batch-> 237\n",
            "0.0012328686425462365 : Loss for Batch-> 238\n",
            "0.07243330776691437 : Loss for Batch-> 239\n",
            "0.0918009951710701 : Loss for Batch-> 240\n",
            "0.13661587238311768 : Loss for Batch-> 241\n",
            "0.006509900093078613 : Val Loss after training 241 batche(s)\n",
            "0.24557609856128693 : Loss for Batch-> 242\n",
            "0.7924229502677917 : Loss for Batch-> 243\n",
            "4.4155192375183105 : Loss for Batch-> 244\n",
            "0.4014061689376831 : Loss for Batch-> 245\n",
            "0.03007403202354908 : Loss for Batch-> 246\n",
            "1.0104248523712158 : Loss for Batch-> 247\n",
            "3.415708303451538 : Loss for Batch-> 248\n",
            "0.5480536818504333 : Loss for Batch-> 249\n",
            "0.6858691573143005 : Loss for Batch-> 250\n",
            "0.020194511860609055 : Loss for Batch-> 251\n",
            "0.0021982600446790457 : Val Loss after training 251 batche(s)\n",
            "0.35999149084091187 : Loss for Batch-> 252\n",
            "0.47048884630203247 : Loss for Batch-> 253\n",
            "0.4212770462036133 : Loss for Batch-> 254\n",
            "3.0865590572357178 : Loss for Batch-> 255\n",
            "6.349442005157471 : Loss for Batch-> 256\n",
            "0.009963806718587875 : Loss for Batch-> 257\n",
            "0.18146003782749176 : Loss for Batch-> 258\n",
            "0.5020143389701843 : Loss for Batch-> 259\n",
            "3.591909408569336 : Loss for Batch-> 260\n",
            "2.0719029903411865 : Loss for Batch-> 261\n",
            "0.0004417219024617225 : Val Loss after training 261 batche(s)\n",
            "0.8093251585960388 : Loss for Batch-> 262\n",
            "0.005400263704359531 : Loss for Batch-> 263\n",
            "0.5051217079162598 : Loss for Batch-> 264\n",
            "1.5798035860061646 : Loss for Batch-> 265\n",
            "0.1215234100818634 : Loss for Batch-> 266\n",
            "0.06817350536584854 : Loss for Batch-> 267\n",
            "0.0004175507347099483 : Loss for Batch-> 268\n",
            "1.9338102340698242 : Loss for Batch-> 269\n",
            "4.060945510864258 : Loss for Batch-> 270\n",
            "0.009537814185023308 : Loss for Batch-> 271\n",
            "0.0016514771850779653 : Val Loss after training 271 batche(s)\n",
            "0.0003649268765002489 : Loss for Batch-> 272\n",
            "0.0022200553212314844 : Loss for Batch-> 273\n",
            "0.00047413099673576653 : Loss for Batch-> 274\n",
            "0.0021618420723825693 : Loss for Batch-> 275\n",
            "0.0034447989892214537 : Loss for Batch-> 276\n",
            "0.0008875378989614546 : Loss for Batch-> 277\n",
            "0.005029851105064154 : Loss for Batch-> 278\n",
            "0.015487808734178543 : Loss for Batch-> 279\n",
            "0.03494872897863388 : Loss for Batch-> 280\n",
            "0.06035314500331879 : Loss for Batch-> 281\n",
            "0.0015479758149012923 : Val Loss after training 281 batche(s)\n",
            "0.06489670276641846 : Loss for Batch-> 282\n",
            "0.06693790853023529 : Loss for Batch-> 283\n",
            "0.035907454788684845 : Loss for Batch-> 284\n",
            "0.010643941350281239 : Loss for Batch-> 285\n",
            "9.455026626586914 : Loss for Batch-> 286\n",
            "4.930370807647705 : Loss for Batch-> 287\n",
            "0.010834735818207264 : Loss for Batch-> 288\n",
            "0.0017604276072233915 : Loss for Batch-> 289\n",
            "0.035703372210264206 : Loss for Batch-> 290\n",
            "0.14044564962387085 : Loss for Batch-> 291\n",
            "0.011562403291463852 : Val Loss after training 291 batche(s)\n",
            "0.008012308739125729 : Loss for Batch-> 292\n",
            "0.0013618306256830692 : Loss for Batch-> 293\n",
            "0.0014226622879505157 : Loss for Batch-> 294\n",
            "0.004709973465651274 : Loss for Batch-> 295\n",
            "0.06358025223016739 : Loss for Batch-> 296\n",
            "0.004371185787022114 : Loss for Batch-> 297\n",
            "0.028289925307035446 : Loss for Batch-> 298\n",
            "0.015215814113616943 : Loss for Batch-> 299\n",
            "0.7387831211090088 : Loss for Batch-> 300\n",
            "1.0756915807724 : Loss for Batch-> 301\n",
            "0.019872792065143585 : Val Loss after training 301 batche(s)\n",
            "0.8574150800704956 : Loss for Batch-> 302\n",
            "0.06959613412618637 : Loss for Batch-> 303\n",
            "0.03244847059249878 : Loss for Batch-> 304\n",
            "0.8181688189506531 : Loss for Batch-> 305\n",
            "1.9397327899932861 : Loss for Batch-> 306\n",
            "1.2031384706497192 : Loss for Batch-> 307\n",
            "0.04492463171482086 : Loss for Batch-> 308\n",
            "0.04511279985308647 : Loss for Batch-> 309\n",
            "0.08826562017202377 : Loss for Batch-> 310\n",
            "1.6236984729766846 : Loss for Batch-> 311\n",
            "0.011510955169796944 : Val Loss after training 311 batche(s)\n",
            "1.271684169769287 : Loss for Batch-> 312\n",
            "0.026654643937945366 : Loss for Batch-> 313\n",
            "0.01270997803658247 : Loss for Batch-> 314\n",
            "0.008001728914678097 : Loss for Batch-> 315\n",
            "0.6084496974945068 : Loss for Batch-> 316\n",
            "0.03253521770238876 : Loss for Batch-> 317\n",
            "0.028619175776839256 : Loss for Batch-> 318\n",
            "0.09953390806913376 : Loss for Batch-> 319\n",
            "0.20215369760990143 : Loss for Batch-> 320\n",
            "0.13582046329975128 : Loss for Batch-> 321\n",
            "0.001535881427116692 : Val Loss after training 321 batche(s)\n",
            "0.3383926451206207 : Loss for Batch-> 322\n",
            "0.09274580329656601 : Loss for Batch-> 323\n",
            "0.04596247151494026 : Loss for Batch-> 324\n",
            "0.24805068969726562 : Loss for Batch-> 325\n",
            "0.27132508158683777 : Loss for Batch-> 326\n",
            "0.49390992522239685 : Loss for Batch-> 327\n",
            "0.38110870122909546 : Loss for Batch-> 328\n",
            "0.03044137917459011 : Loss for Batch-> 329\n",
            "0.08877754211425781 : Loss for Batch-> 330\n",
            "0.06617902219295502 : Loss for Batch-> 331\n",
            "0.017869742587208748 : Val Loss after training 331 batche(s)\n",
            "0.006579568609595299 : Loss for Batch-> 332\n",
            "0.011899576522409916 : Loss for Batch-> 333\n",
            "0.00017459376249462366 : Loss for Batch-> 334\n",
            "0.0006099193124100566 : Loss for Batch-> 335\n",
            "0.00127502647228539 : Loss for Batch-> 336\n",
            "0.002735931659117341 : Loss for Batch-> 337\n",
            "0.00310253887437284 : Loss for Batch-> 338\n",
            "0.06286012381315231 : Loss for Batch-> 339\n",
            "0.04716723412275314 : Loss for Batch-> 340\n",
            "0.058314476162195206 : Loss for Batch-> 341\n",
            "0.0040842387825250626 : Val Loss after training 341 batche(s)\n",
            "0.07092178612947464 : Loss for Batch-> 342\n",
            "0.520190954208374 : Loss for Batch-> 343\n",
            "0.009494653902947903 : Loss for Batch-> 344\n",
            "0.013355042785406113 : Loss for Batch-> 345\n",
            "0.09033393114805222 : Loss for Batch-> 346\n",
            "0.31096282601356506 : Loss for Batch-> 347\n",
            "0.3745584785938263 : Loss for Batch-> 348\n",
            "0.005552409216761589 : Loss for Batch-> 349\n",
            "0.20078788697719574 : Loss for Batch-> 350\n",
            "0.8618951439857483 : Loss for Batch-> 351\n",
            "0.0007725355098955333 : Val Loss after training 351 batche(s)\n",
            "0.42924949526786804 : Loss for Batch-> 352\n",
            "0.042039647698402405 : Loss for Batch-> 353\n",
            "0.3081510365009308 : Loss for Batch-> 354\n",
            "1.593957781791687 : Loss for Batch-> 355\n",
            "0.01810438185930252 : Loss for Batch-> 356\n",
            "0.010624390095472336 : Loss for Batch-> 357\n",
            "0.0033830844331532717 : Loss for Batch-> 358\n",
            "0.30775585770606995 : Loss for Batch-> 359\n",
            "0.26537182927131653 : Loss for Batch-> 360\n",
            "0.07710883766412735 : Loss for Batch-> 361\n",
            "0.0016687033930793405 : Val Loss after training 361 batche(s)\n",
            "0.19237302243709564 : Loss for Batch-> 362\n",
            "0.04995298758149147 : Loss for Batch-> 363\n",
            "0.1460667997598648 : Loss for Batch-> 364\n",
            "0.43721622228622437 : Loss for Batch-> 365\n",
            "0.06487316638231277 : Loss for Batch-> 366\n",
            "0.2732754051685333 : Loss for Batch-> 367\n",
            "0.0758066475391388 : Loss for Batch-> 368\n",
            "0.003058244474232197 : Loss for Batch-> 369\n",
            "0.05142156779766083 : Loss for Batch-> 370\n",
            "0.02717701718211174 : Loss for Batch-> 371\n",
            "0.0035620946437120438 : Val Loss after training 371 batche(s)\n",
            "0.6528080701828003 : Loss for Batch-> 372\n",
            "0.3399665057659149 : Loss for Batch-> 373\n",
            "0.45152419805526733 : Loss for Batch-> 374\n",
            "1.1691011190414429 : Loss for Batch-> 375\n",
            "0.3829081654548645 : Loss for Batch-> 376\n",
            "0.4927113354206085 : Loss for Batch-> 377\n",
            "0.2279672622680664 : Loss for Batch-> 378\n",
            "0.006297423969954252 : Loss for Batch-> 379\n",
            "0.03794058412313461 : Loss for Batch-> 380\n",
            "0.11670160293579102 : Loss for Batch-> 381\n",
            "0.0029021170921623707 : Val Loss after training 381 batche(s)\n",
            "0.1967410147190094 : Loss for Batch-> 382\n",
            "0.7906230092048645 : Loss for Batch-> 383\n",
            "0.575235903263092 : Loss for Batch-> 384\n",
            "1.9995620250701904 : Loss for Batch-> 385\n",
            "0.6214632391929626 : Loss for Batch-> 386\n",
            "0.5328872203826904 : Loss for Batch-> 387\n",
            "0.0032127893064171076 : Loss for Batch-> 388\n",
            "0.013897616416215897 : Loss for Batch-> 389\n",
            "0.5876129269599915 : Loss for Batch-> 390\n",
            "1.3854314088821411 : Loss for Batch-> 391\n",
            "0.0015472082886844873 : Val Loss after training 391 batche(s)\n",
            "0.7442039251327515 : Loss for Batch-> 392\n",
            "0.037861477583646774 : Loss for Batch-> 393\n",
            "0.08857043087482452 : Loss for Batch-> 394\n",
            "0.010224009864032269 : Loss for Batch-> 395\n",
            "0.30885636806488037 : Loss for Batch-> 396\n",
            "0.19003638625144958 : Loss for Batch-> 397\n",
            "0.7035374641418457 : Loss for Batch-> 398\n",
            "0.826040506362915 : Loss for Batch-> 399\n",
            "0.07682190835475922 : Loss for Batch-> 400\n",
            "0.004802431911230087 : Loss for Batch-> 401\n",
            "0.0002775178581941873 : Val Loss after training 401 batche(s)\n",
            "0.011957729235291481 : Loss for Batch-> 402\n",
            "0.01662941835820675 : Loss for Batch-> 403\n",
            "0.19493843615055084 : Loss for Batch-> 404\n",
            "0.5997588038444519 : Loss for Batch-> 405\n",
            "0.31308627128601074 : Loss for Batch-> 406\n",
            "0.12434835731983185 : Loss for Batch-> 407\n",
            "0.0446062907576561 : Loss for Batch-> 408\n",
            "0.014294188469648361 : Loss for Batch-> 409\n",
            "0.04448552057147026 : Loss for Batch-> 410\n",
            "0.48509499430656433 : Loss for Batch-> 411\n",
            "0.003582422621548176 : Val Loss after training 411 batche(s)\n",
            "1.2456095218658447 : Loss for Batch-> 412\n",
            "0.3456370234489441 : Loss for Batch-> 413\n",
            "0.13434843719005585 : Loss for Batch-> 414\n",
            "0.011679944582283497 : Loss for Batch-> 415\n",
            "0.0007271179347299039 : Loss for Batch-> 416\n",
            "0.002296719467267394 : Loss for Batch-> 417\n",
            "0.01520424336194992 : Loss for Batch-> 418\n",
            "0.034561578184366226 : Loss for Batch-> 419\n",
            "0.001493636635132134 : Loss for Batch-> 420\n",
            "0.15257662534713745 : Loss for Batch-> 421\n",
            "0.005574660375714302 : Val Loss after training 421 batche(s)\n",
            "0.10429633408784866 : Loss for Batch-> 422\n",
            "0.031236892566084862 : Loss for Batch-> 423\n",
            "0.023342428728938103 : Loss for Batch-> 424\n",
            "0.02214617282152176 : Loss for Batch-> 425\n",
            "0.022572066634893417 : Loss for Batch-> 426\n",
            "0.022922484204173088 : Loss for Batch-> 427\n",
            "0.024319492280483246 : Loss for Batch-> 428\n",
            "0.024035567417740822 : Loss for Batch-> 429\n",
            "0.023361915722489357 : Loss for Batch-> 430\n",
            "0.022672999650239944 : Loss for Batch-> 431\n",
            "0.005225048866122961 : Val Loss after training 431 batche(s)\n",
            "0.024198293685913086 : Loss for Batch-> 432\n",
            "0.14583514630794525 : Loss for Batch-> 433\n",
            "0.03000742755830288 : Loss for Batch-> 434\n",
            "0.003984082490205765 : Loss for Batch-> 435\n",
            "0.0017486106371507049 : Loss for Batch-> 436\n",
            "0.0013585276901721954 : Loss for Batch-> 437\n",
            "0.0020444474648684263 : Loss for Batch-> 438\n",
            "0.001691641053184867 : Loss for Batch-> 439\n",
            "0.02235162816941738 : Loss for Batch-> 440\n",
            "0.06525182723999023 : Loss for Batch-> 441\n",
            "0.003788161091506481 : Val Loss after training 441 batche(s)\n",
            "0.030962765216827393 : Loss for Batch-> 442\n",
            "0.011057104915380478 : Loss for Batch-> 443\n",
            "0.027775166556239128 : Loss for Batch-> 444\n",
            "0.03192514553666115 : Loss for Batch-> 445\n",
            "0.022449593991041183 : Loss for Batch-> 446\n",
            "0.06673534214496613 : Loss for Batch-> 447\n",
            "0.3956257998943329 : Loss for Batch-> 448\n",
            "0.5882073640823364 : Loss for Batch-> 449\n",
            "0.02855699695646763 : Loss for Batch-> 450\n",
            "0.0017322319326922297 : Loss for Batch-> 451\n",
            "0.005490106996148825 : Val Loss after training 451 batche(s)\n",
            "0.0006086997454985976 : Loss for Batch-> 452\n",
            "0.00048245969810523093 : Loss for Batch-> 453\n",
            "0.0008127646869979799 : Loss for Batch-> 454\n",
            "0.00104582030326128 : Loss for Batch-> 455\n",
            "0.0011803394882008433 : Loss for Batch-> 456\n",
            "0.00013266502355691046 : Loss for Batch-> 457\n",
            "0.00014494822244159877 : Loss for Batch-> 458\n",
            "0.00014474995259661227 : Loss for Batch-> 459\n",
            "0.00014386515249498188 : Loss for Batch-> 460\n",
            "0.00014227810606826097 : Loss for Batch-> 461\n",
            "0.0004809359961654991 : Val Loss after training 461 batche(s)\n",
            "0.00016895588487386703 : Loss for Batch-> 462\n",
            "0.000135229027364403 : Loss for Batch-> 463\n",
            "0.0001232103240909055 : Loss for Batch-> 464\n",
            "0.00012493405665736645 : Loss for Batch-> 465\n",
            "0.00031998931081034243 : Loss for Batch-> 466\n",
            "0.001635260763578117 : Loss for Batch-> 467\n",
            "0.00044568945304490626 : Loss for Batch-> 468\n",
            "0.0005133452359586954 : Loss for Batch-> 469\n",
            "0.001423401408828795 : Loss for Batch-> 470\n",
            "0.0001219693585881032 : Loss for Batch-> 471\n",
            "0.022906027734279633 : Val Loss after training 471 batche(s)\n",
            "0.0005177957355044782 : Loss for Batch-> 472\n",
            "0.0005476584192365408 : Loss for Batch-> 473\n",
            "0.0008217464201152325 : Loss for Batch-> 474\n",
            "0.0004695479292422533 : Loss for Batch-> 475\n",
            "0.001097405795007944 : Loss for Batch-> 476\n",
            "0.0006807667086832225 : Loss for Batch-> 477\n",
            "0.0012507573701441288 : Loss for Batch-> 478\n",
            "0.00037398561835289 : Loss for Batch-> 479\n",
            "0.0004971413873136044 : Loss for Batch-> 480\n",
            "0.001911762054078281 : Loss for Batch-> 481\n",
            "0.05714557692408562 : Val Loss after training 481 batche(s)\n",
            "0.012759226374328136 : Loss for Batch-> 482\n",
            "0.016767606139183044 : Loss for Batch-> 483\n",
            "0.005268014036118984 : Loss for Batch-> 484\n",
            "0.003599167102947831 : Loss for Batch-> 485\n",
            "0.0035668108612298965 : Loss for Batch-> 486\n",
            "0.003668057033792138 : Loss for Batch-> 487\n",
            "0.00359240360558033 : Loss for Batch-> 488\n",
            "0.0035517928190529346 : Loss for Batch-> 489\n",
            "0.0036162177566438913 : Loss for Batch-> 490\n",
            "0.0029409858398139477 : Loss for Batch-> 491\n",
            "0.0021237649489194155 : Val Loss after training 491 batche(s)\n",
            "0.0021274725440889597 : Loss for Batch-> 492\n",
            "0.0021232159342616796 : Loss for Batch-> 493\n",
            "0.0020609048660844564 : Loss for Batch-> 494\n",
            "0.9765839576721191 : Loss for Batch-> 495\n",
            "0.7838529944419861 : Loss for Batch-> 496\n",
            "0.012951742857694626 : Loss for Batch-> 497\n",
            "0.010309820994734764 : Loss for Batch-> 498\n",
            "0.001221312559209764 : Loss for Batch-> 499\n",
            "0.001213926007039845 : Loss for Batch-> 500\n",
            "0.0017649669898673892 : Loss for Batch-> 501\n",
            "0.00025843377807177603 : Val Loss after training 501 batche(s)\n",
            "0.0023899239022284746 : Loss for Batch-> 502\n",
            "0.00469432957470417 : Loss for Batch-> 503\n",
            "0.05400452762842178 : Loss for Batch-> 504\n",
            "0.003469480900093913 : Loss for Batch-> 505\n",
            "0.018254786729812622 : Loss for Batch-> 506\n",
            "0.01974695362150669 : Loss for Batch-> 507\n",
            "0.01930411532521248 : Loss for Batch-> 508\n",
            "0.01841604709625244 : Loss for Batch-> 509\n",
            "0.009565250016748905 : Loss for Batch-> 510\n",
            "27 Epoch\n",
            "0.008781221695244312 : Loss for Batch-> 1\n",
            "0.00047538173384964466 : Val Loss after training 1 batche(s)\n",
            "0.008883866481482983 : Loss for Batch-> 2\n",
            "0.008691317401826382 : Loss for Batch-> 3\n",
            "0.008543703705072403 : Loss for Batch-> 4\n",
            "0.008509354665875435 : Loss for Batch-> 5\n",
            "0.007638108916580677 : Loss for Batch-> 6\n",
            "0.007694233674556017 : Loss for Batch-> 7\n",
            "0.007996848784387112 : Loss for Batch-> 8\n",
            "0.017050834372639656 : Loss for Batch-> 9\n",
            "0.017733542248606682 : Loss for Batch-> 10\n",
            "0.018502386286854744 : Loss for Batch-> 11\n",
            "0.03319333866238594 : Val Loss after training 11 batche(s)\n",
            "0.02008426934480667 : Loss for Batch-> 12\n",
            "4.947195053100586 : Loss for Batch-> 13\n",
            "1.2456223964691162 : Loss for Batch-> 14\n",
            "0.0017815292812883854 : Loss for Batch-> 15\n",
            "0.0014293506974354386 : Loss for Batch-> 16\n",
            "0.0011769522679969668 : Loss for Batch-> 17\n",
            "0.00234430655837059 : Loss for Batch-> 18\n",
            "0.013650009408593178 : Loss for Batch-> 19\n",
            "0.0015837644459679723 : Loss for Batch-> 20\n",
            "0.0009187727700918913 : Loss for Batch-> 21\n",
            "0.03522028028964996 : Val Loss after training 21 batche(s)\n",
            "0.02432866021990776 : Loss for Batch-> 22\n",
            "0.019752856343984604 : Loss for Batch-> 23\n",
            "0.0201654601842165 : Loss for Batch-> 24\n",
            "0.021045111119747162 : Loss for Batch-> 25\n",
            "0.018118098378181458 : Loss for Batch-> 26\n",
            "0.001719487365335226 : Loss for Batch-> 27\n",
            "0.0004706810286734253 : Loss for Batch-> 28\n",
            "0.0003117069718427956 : Loss for Batch-> 29\n",
            "0.0035717447753995657 : Loss for Batch-> 30\n",
            "0.0028696791268885136 : Loss for Batch-> 31\n",
            "0.03421095386147499 : Val Loss after training 31 batche(s)\n",
            "0.001902954769320786 : Loss for Batch-> 32\n",
            "0.0004542050010059029 : Loss for Batch-> 33\n",
            "6.901004235260189e-05 : Loss for Batch-> 34\n",
            "3.19755490636453e-05 : Loss for Batch-> 35\n",
            "2.115656934620347e-05 : Loss for Batch-> 36\n",
            "0.0009813187643885612 : Loss for Batch-> 37\n",
            "0.0003169599804095924 : Loss for Batch-> 38\n",
            "0.005212297663092613 : Loss for Batch-> 39\n",
            "0.009979617781937122 : Loss for Batch-> 40\n",
            "0.00026685791090130806 : Loss for Batch-> 41\n",
            "0.032075509428977966 : Val Loss after training 41 batche(s)\n",
            "0.00041328719817101955 : Loss for Batch-> 42\n",
            "0.0007996063213795424 : Loss for Batch-> 43\n",
            "0.00047872596769593656 : Loss for Batch-> 44\n",
            "0.02873016521334648 : Loss for Batch-> 45\n",
            "0.06755068898200989 : Loss for Batch-> 46\n",
            "0.06880539655685425 : Loss for Batch-> 47\n",
            "0.04000767692923546 : Loss for Batch-> 48\n",
            "0.0036689224652945995 : Loss for Batch-> 49\n",
            "0.14495842158794403 : Loss for Batch-> 50\n",
            "0.2509300410747528 : Loss for Batch-> 51\n",
            "0.040901776403188705 : Val Loss after training 51 batche(s)\n",
            "0.22113655507564545 : Loss for Batch-> 52\n",
            "0.007821356877684593 : Loss for Batch-> 53\n",
            "0.0011956488015130162 : Loss for Batch-> 54\n",
            "0.004539452958852053 : Loss for Batch-> 55\n",
            "0.018921080976724625 : Loss for Batch-> 56\n",
            "0.01805492676794529 : Loss for Batch-> 57\n",
            "0.004719048272818327 : Loss for Batch-> 58\n",
            "0.0003959914611186832 : Loss for Batch-> 59\n",
            "0.009983115829527378 : Loss for Batch-> 60\n",
            "0.07484868168830872 : Loss for Batch-> 61\n",
            "0.017957493662834167 : Val Loss after training 61 batche(s)\n",
            "0.22924721240997314 : Loss for Batch-> 62\n",
            "0.23900370299816132 : Loss for Batch-> 63\n",
            "0.03338916227221489 : Loss for Batch-> 64\n",
            "0.08514638245105743 : Loss for Batch-> 65\n",
            "0.05338197574019432 : Loss for Batch-> 66\n",
            "0.10084360837936401 : Loss for Batch-> 67\n",
            "0.06130187585949898 : Loss for Batch-> 68\n",
            "0.0886591225862503 : Loss for Batch-> 69\n",
            "0.028966326266527176 : Loss for Batch-> 70\n",
            "0.01174470316618681 : Loss for Batch-> 71\n",
            "0.008448462933301926 : Val Loss after training 71 batche(s)\n",
            "0.011700903996825218 : Loss for Batch-> 72\n",
            "0.010289337486028671 : Loss for Batch-> 73\n",
            "0.010757004842162132 : Loss for Batch-> 74\n",
            "0.010841203853487968 : Loss for Batch-> 75\n",
            "0.021967850625514984 : Loss for Batch-> 76\n",
            "0.28903746604919434 : Loss for Batch-> 77\n",
            "2.283764600753784 : Loss for Batch-> 78\n",
            "0.023181436583399773 : Loss for Batch-> 79\n",
            "0.08668337762355804 : Loss for Batch-> 80\n",
            "0.09091021865606308 : Loss for Batch-> 81\n",
            "0.007866856642067432 : Val Loss after training 81 batche(s)\n",
            "0.02877162955701351 : Loss for Batch-> 82\n",
            "0.0011620675213634968 : Loss for Batch-> 83\n",
            "0.02705908752977848 : Loss for Batch-> 84\n",
            "0.08704712241888046 : Loss for Batch-> 85\n",
            "0.08090832084417343 : Loss for Batch-> 86\n",
            "0.0915406122803688 : Loss for Batch-> 87\n",
            "0.02431606315076351 : Loss for Batch-> 88\n",
            "0.013386065140366554 : Loss for Batch-> 89\n",
            "0.0377216562628746 : Loss for Batch-> 90\n",
            "0.03594633936882019 : Loss for Batch-> 91\n",
            "0.0077436696738004684 : Val Loss after training 91 batche(s)\n",
            "0.002124097431078553 : Loss for Batch-> 92\n",
            "0.03542480617761612 : Loss for Batch-> 93\n",
            "0.037574149668216705 : Loss for Batch-> 94\n",
            "0.009439241141080856 : Loss for Batch-> 95\n",
            "0.014405211433768272 : Loss for Batch-> 96\n",
            "0.00605428870767355 : Loss for Batch-> 97\n",
            "0.0016994288889691234 : Loss for Batch-> 98\n",
            "0.001127014053054154 : Loss for Batch-> 99\n",
            "0.006966643501073122 : Loss for Batch-> 100\n",
            "0.0018116342835128307 : Loss for Batch-> 101\n",
            "0.007166389841586351 : Val Loss after training 101 batche(s)\n",
            "0.00318117905408144 : Loss for Batch-> 102\n",
            "0.00208688760176301 : Loss for Batch-> 103\n",
            "0.004724831320345402 : Loss for Batch-> 104\n",
            "0.021750902757048607 : Loss for Batch-> 105\n",
            "0.017335008829832077 : Loss for Batch-> 106\n",
            "0.01627924107015133 : Loss for Batch-> 107\n",
            "0.010356826707720757 : Loss for Batch-> 108\n",
            "0.0014964997535571456 : Loss for Batch-> 109\n",
            "0.009077786467969418 : Loss for Batch-> 110\n",
            "0.028831109404563904 : Loss for Batch-> 111\n",
            "0.005598151590675116 : Val Loss after training 111 batche(s)\n",
            "0.0029285699129104614 : Loss for Batch-> 112\n",
            "0.005485280882567167 : Loss for Batch-> 113\n",
            "0.000895726669114083 : Loss for Batch-> 114\n",
            "0.03836728632450104 : Loss for Batch-> 115\n",
            "0.036352161318063736 : Loss for Batch-> 116\n",
            "0.05175182968378067 : Loss for Batch-> 117\n",
            "0.004860724322497845 : Loss for Batch-> 118\n",
            "0.05771251767873764 : Loss for Batch-> 119\n",
            "0.024765262380242348 : Loss for Batch-> 120\n",
            "0.007886301726102829 : Loss for Batch-> 121\n",
            "0.02494567073881626 : Val Loss after training 121 batche(s)\n",
            "0.01475647371262312 : Loss for Batch-> 122\n",
            "0.01031737681478262 : Loss for Batch-> 123\n",
            "0.1137581542134285 : Loss for Batch-> 124\n",
            "0.061940692365169525 : Loss for Batch-> 125\n",
            "0.009179326705634594 : Loss for Batch-> 126\n",
            "0.03435324877500534 : Loss for Batch-> 127\n",
            "0.0071619367226958275 : Loss for Batch-> 128\n",
            "0.0020755345467478037 : Loss for Batch-> 129\n",
            "0.02210417203605175 : Loss for Batch-> 130\n",
            "0.03695869818329811 : Loss for Batch-> 131\n",
            "0.00923353061079979 : Val Loss after training 131 batche(s)\n",
            "0.13814447820186615 : Loss for Batch-> 132\n",
            "0.006125611253082752 : Loss for Batch-> 133\n",
            "0.04868138208985329 : Loss for Batch-> 134\n",
            "0.02717774547636509 : Loss for Batch-> 135\n",
            "0.04082288220524788 : Loss for Batch-> 136\n",
            "0.08561988174915314 : Loss for Batch-> 137\n",
            "0.22776642441749573 : Loss for Batch-> 138\n",
            "0.029880128800868988 : Loss for Batch-> 139\n",
            "0.0055334800854325294 : Loss for Batch-> 140\n",
            "0.00044527288991957903 : Loss for Batch-> 141\n",
            "0.0009152435231953859 : Val Loss after training 141 batche(s)\n",
            "0.0006714381743222475 : Loss for Batch-> 142\n",
            "0.0008349999552592635 : Loss for Batch-> 143\n",
            "0.009111378341913223 : Loss for Batch-> 144\n",
            "0.016051195561885834 : Loss for Batch-> 145\n",
            "0.0123799629509449 : Loss for Batch-> 146\n",
            "0.012387624941766262 : Loss for Batch-> 147\n",
            "0.004328253213316202 : Loss for Batch-> 148\n",
            "0.0017686205683276057 : Loss for Batch-> 149\n",
            "0.0028660970274358988 : Loss for Batch-> 150\n",
            "0.017699064686894417 : Loss for Batch-> 151\n",
            "0.00045573897659778595 : Val Loss after training 151 batche(s)\n",
            "0.06265057623386383 : Loss for Batch-> 152\n",
            "0.03405367210507393 : Loss for Batch-> 153\n",
            "0.01167798601090908 : Loss for Batch-> 154\n",
            "0.043661102652549744 : Loss for Batch-> 155\n",
            "0.03664366528391838 : Loss for Batch-> 156\n",
            "0.004302637651562691 : Loss for Batch-> 157\n",
            "0.03902003541588783 : Loss for Batch-> 158\n",
            "0.02658872678875923 : Loss for Batch-> 159\n",
            "0.010004574432969093 : Loss for Batch-> 160\n",
            "0.0020679840818047523 : Loss for Batch-> 161\n",
            "0.06923699378967285 : Val Loss after training 161 batche(s)\n",
            "0.051436953246593475 : Loss for Batch-> 162\n",
            "0.06393710523843765 : Loss for Batch-> 163\n",
            "0.015854498371481895 : Loss for Batch-> 164\n",
            "0.031681887805461884 : Loss for Batch-> 165\n",
            "0.04557524248957634 : Loss for Batch-> 166\n",
            "0.05089351534843445 : Loss for Batch-> 167\n",
            "0.032843705266714096 : Loss for Batch-> 168\n",
            "0.015421697869896889 : Loss for Batch-> 169\n",
            "0.025550654157996178 : Loss for Batch-> 170\n",
            "0.000436911650467664 : Loss for Batch-> 171\n",
            "0.08525222539901733 : Val Loss after training 171 batche(s)\n",
            "0.0008512602071277797 : Loss for Batch-> 172\n",
            "0.0015080093871802092 : Loss for Batch-> 173\n",
            "0.00037740851985290647 : Loss for Batch-> 174\n",
            "0.00034993336885236204 : Loss for Batch-> 175\n",
            "0.0006613142904825509 : Loss for Batch-> 176\n",
            "0.01214160118252039 : Loss for Batch-> 177\n",
            "0.019700786098837852 : Loss for Batch-> 178\n",
            "0.0011594011448323727 : Loss for Batch-> 179\n",
            "0.000428941217251122 : Loss for Batch-> 180\n",
            "0.00026267272187396884 : Loss for Batch-> 181\n",
            "0.06426430493593216 : Val Loss after training 181 batche(s)\n",
            "1.867702667368576e-05 : Loss for Batch-> 182\n",
            "7.455700142600108e-06 : Loss for Batch-> 183\n",
            "0.0006193635053932667 : Loss for Batch-> 184\n",
            "0.0001995482889469713 : Loss for Batch-> 185\n",
            "0.0002943722647614777 : Loss for Batch-> 186\n",
            "0.0007883615326136351 : Loss for Batch-> 187\n",
            "0.00035418246989138424 : Loss for Batch-> 188\n",
            "0.0004480648785829544 : Loss for Batch-> 189\n",
            "0.0010941788787022233 : Loss for Batch-> 190\n",
            "0.010777142830193043 : Loss for Batch-> 191\n",
            "0.03287748247385025 : Val Loss after training 191 batche(s)\n",
            "0.024666599929332733 : Loss for Batch-> 192\n",
            "0.012203010730445385 : Loss for Batch-> 193\n",
            "0.0059540425427258015 : Loss for Batch-> 194\n",
            "0.0008448800654150546 : Loss for Batch-> 195\n",
            "0.00019040150800719857 : Loss for Batch-> 196\n",
            "0.00020932457118760794 : Loss for Batch-> 197\n",
            "0.01699542999267578 : Loss for Batch-> 198\n",
            "3.1065680980682373 : Loss for Batch-> 199\n",
            "0.019494326785206795 : Loss for Batch-> 200\n",
            "0.02268512174487114 : Loss for Batch-> 201\n",
            "0.09406822174787521 : Val Loss after training 201 batche(s)\n",
            "0.06988880038261414 : Loss for Batch-> 202\n",
            "0.04056687280535698 : Loss for Batch-> 203\n",
            "0.01626449078321457 : Loss for Batch-> 204\n",
            "0.02304280549287796 : Loss for Batch-> 205\n",
            "0.009015657007694244 : Loss for Batch-> 206\n",
            "0.008130461908876896 : Loss for Batch-> 207\n",
            "0.005813743453472853 : Loss for Batch-> 208\n",
            "0.006094851065427065 : Loss for Batch-> 209\n",
            "0.018807267770171165 : Loss for Batch-> 210\n",
            "0.03281072527170181 : Loss for Batch-> 211\n",
            "0.007173753809183836 : Val Loss after training 211 batche(s)\n",
            "0.002096189185976982 : Loss for Batch-> 212\n",
            "0.03300710767507553 : Loss for Batch-> 213\n",
            "0.02696188911795616 : Loss for Batch-> 214\n",
            "0.019863689318299294 : Loss for Batch-> 215\n",
            "0.038183584809303284 : Loss for Batch-> 216\n",
            "0.0005480691906996071 : Loss for Batch-> 217\n",
            "0.0004069017886649817 : Loss for Batch-> 218\n",
            "0.005747461225837469 : Loss for Batch-> 219\n",
            "0.07164768874645233 : Loss for Batch-> 220\n",
            "0.03794979676604271 : Loss for Batch-> 221\n",
            "0.0026763558853417635 : Val Loss after training 221 batche(s)\n",
            "0.008418373763561249 : Loss for Batch-> 222\n",
            "0.17139428853988647 : Loss for Batch-> 223\n",
            "0.0021226259414106607 : Loss for Batch-> 224\n",
            "0.00042931511416099966 : Loss for Batch-> 225\n",
            "0.00045241371844895184 : Loss for Batch-> 226\n",
            "0.0007871793932281435 : Loss for Batch-> 227\n",
            "1.3553563356399536 : Loss for Batch-> 228\n",
            "1.164911150932312 : Loss for Batch-> 229\n",
            "9.861164093017578 : Loss for Batch-> 230\n",
            "1.6110478639602661 : Loss for Batch-> 231\n",
            "0.022033503279089928 : Val Loss after training 231 batche(s)\n",
            "0.20796974003314972 : Loss for Batch-> 232\n",
            "0.12123246490955353 : Loss for Batch-> 233\n",
            "0.0030273902229964733 : Loss for Batch-> 234\n",
            "0.04137849807739258 : Loss for Batch-> 235\n",
            "0.02823006361722946 : Loss for Batch-> 236\n",
            "0.0017602227162569761 : Loss for Batch-> 237\n",
            "0.0012015802785754204 : Loss for Batch-> 238\n",
            "0.005326048005372286 : Loss for Batch-> 239\n",
            "0.10396086424589157 : Loss for Batch-> 240\n",
            "0.10423197597265244 : Loss for Batch-> 241\n",
            "0.02322746068239212 : Val Loss after training 241 batche(s)\n",
            "0.13593080639839172 : Loss for Batch-> 242\n",
            "0.5021812319755554 : Loss for Batch-> 243\n",
            "0.5668622851371765 : Loss for Batch-> 244\n",
            "4.731524467468262 : Loss for Batch-> 245\n",
            "0.010534266009926796 : Loss for Batch-> 246\n",
            "0.03007529303431511 : Loss for Batch-> 247\n",
            "4.266631126403809 : Loss for Batch-> 248\n",
            "0.16915465891361237 : Loss for Batch-> 249\n",
            "1.1281540393829346 : Loss for Batch-> 250\n",
            "0.09422694891691208 : Loss for Batch-> 251\n",
            "0.010851776227355003 : Val Loss after training 251 batche(s)\n",
            "0.1214214339852333 : Loss for Batch-> 252\n",
            "0.41323965787887573 : Loss for Batch-> 253\n",
            "0.5059397220611572 : Loss for Batch-> 254\n",
            "1.366886854171753 : Loss for Batch-> 255\n",
            "6.5609636306762695 : Loss for Batch-> 256\n",
            "1.7463688850402832 : Loss for Batch-> 257\n",
            "0.002519131638109684 : Loss for Batch-> 258\n",
            "0.3432784378528595 : Loss for Batch-> 259\n",
            "0.3456534445285797 : Loss for Batch-> 260\n",
            "4.408157825469971 : Loss for Batch-> 261\n",
            "0.003709759097546339 : Val Loss after training 261 batche(s)\n",
            "1.8892009258270264 : Loss for Batch-> 262\n",
            "0.17326684296131134 : Loss for Batch-> 263\n",
            "0.0018262399826198816 : Loss for Batch-> 264\n",
            "1.1491715908050537 : Loss for Batch-> 265\n",
            "0.9792461395263672 : Loss for Batch-> 266\n",
            "0.13838733732700348 : Loss for Batch-> 267\n",
            "0.008772657252848148 : Loss for Batch-> 268\n",
            "0.06603149324655533 : Loss for Batch-> 269\n",
            "4.9508442878723145 : Loss for Batch-> 270\n",
            "0.9827775359153748 : Loss for Batch-> 271\n",
            "0.07008552551269531 : Val Loss after training 271 batche(s)\n",
            "0.0023310550022870302 : Loss for Batch-> 272\n",
            "0.0005210338858887553 : Loss for Batch-> 273\n",
            "0.001974626211449504 : Loss for Batch-> 274\n",
            "0.0008169636130332947 : Loss for Batch-> 275\n",
            "0.002003556815907359 : Loss for Batch-> 276\n",
            "0.0034442823380231857 : Loss for Batch-> 277\n",
            "0.0007038943585939705 : Loss for Batch-> 278\n",
            "0.01806095987558365 : Loss for Batch-> 279\n",
            "0.004768689163029194 : Loss for Batch-> 280\n",
            "0.04894787818193436 : Loss for Batch-> 281\n",
            "0.16036872565746307 : Val Loss after training 281 batche(s)\n",
            "0.07021260261535645 : Loss for Batch-> 282\n",
            "0.06702687591314316 : Loss for Batch-> 283\n",
            "0.06308451294898987 : Loss for Batch-> 284\n",
            "0.015004830434918404 : Loss for Batch-> 285\n",
            "1.4975672960281372 : Loss for Batch-> 286\n",
            "12.592151641845703 : Loss for Batch-> 287\n",
            "0.29420003294944763 : Loss for Batch-> 288\n",
            "0.011431055143475533 : Loss for Batch-> 289\n",
            "0.002196162473410368 : Loss for Batch-> 290\n",
            "0.10022888332605362 : Loss for Batch-> 291\n",
            "0.040735770016908646 : Val Loss after training 291 batche(s)\n",
            "0.08013097941875458 : Loss for Batch-> 292\n",
            "0.004301281645894051 : Loss for Batch-> 293\n",
            "0.0007004840881563723 : Loss for Batch-> 294\n",
            "0.0015928632346913218 : Loss for Batch-> 295\n",
            "0.03643186390399933 : Loss for Batch-> 296\n",
            "0.035905469208955765 : Loss for Batch-> 297\n",
            "0.000560340762604028 : Loss for Batch-> 298\n",
            "0.04231642186641693 : Loss for Batch-> 299\n",
            "0.08017832785844803 : Loss for Batch-> 300\n",
            "1.201431393623352 : Loss for Batch-> 301\n",
            "0.10546911507844925 : Val Loss after training 301 batche(s)\n",
            "1.030573844909668 : Loss for Batch-> 302\n",
            "0.36854398250579834 : Loss for Batch-> 303\n",
            "0.09651550650596619 : Loss for Batch-> 304\n",
            "0.033091798424720764 : Loss for Batch-> 305\n",
            "1.617830753326416 : Loss for Batch-> 306\n",
            "1.7486202716827393 : Loss for Batch-> 307\n",
            "0.5701308846473694 : Loss for Batch-> 308\n",
            "0.057384420186281204 : Loss for Batch-> 309\n",
            "0.03186523914337158 : Loss for Batch-> 310\n",
            "0.6731364727020264 : Loss for Batch-> 311\n",
            "0.061717718839645386 : Val Loss after training 311 batche(s)\n",
            "1.5713986158370972 : Loss for Batch-> 312\n",
            "0.7487871050834656 : Loss for Batch-> 313\n",
            "0.01805790141224861 : Loss for Batch-> 314\n",
            "0.005002595949918032 : Loss for Batch-> 315\n",
            "0.22899827361106873 : Loss for Batch-> 316\n",
            "0.40748652815818787 : Loss for Batch-> 317\n",
            "0.016213852912187576 : Loss for Batch-> 318\n",
            "0.025209948420524597 : Loss for Batch-> 319\n",
            "0.18587051331996918 : Loss for Batch-> 320\n",
            "0.15489357709884644 : Loss for Batch-> 321\n",
            "0.04646315425634384 : Val Loss after training 321 batche(s)\n",
            "0.2341269552707672 : Loss for Batch-> 322\n",
            "0.2653907835483551 : Loss for Batch-> 323\n",
            "0.04648788273334503 : Loss for Batch-> 324\n",
            "0.11086432635784149 : Loss for Batch-> 325\n",
            "0.25265276432037354 : Loss for Batch-> 326\n",
            "0.39992332458496094 : Loss for Batch-> 327\n",
            "0.4639889597892761 : Loss for Batch-> 328\n",
            "0.19082656502723694 : Loss for Batch-> 329\n",
            "0.07465849816799164 : Loss for Batch-> 330\n",
            "0.08346002548933029 : Loss for Batch-> 331\n",
            "0.0447399728000164 : Val Loss after training 331 batche(s)\n",
            "0.03035920299589634 : Loss for Batch-> 332\n",
            "0.007758915890008211 : Loss for Batch-> 333\n",
            "0.007119926158338785 : Loss for Batch-> 334\n",
            "0.0002724479418247938 : Loss for Batch-> 335\n",
            "0.0006057603168301284 : Loss for Batch-> 336\n",
            "0.0013056970201432705 : Loss for Batch-> 337\n",
            "0.0026358095929026604 : Loss for Batch-> 338\n",
            "0.014126257039606571 : Loss for Batch-> 339\n",
            "0.0582076758146286 : Loss for Batch-> 340\n",
            "0.06884791702032089 : Loss for Batch-> 341\n",
            "0.04217563569545746 : Val Loss after training 341 batche(s)\n",
            "0.04389599710702896 : Loss for Batch-> 342\n",
            "0.23704038560390472 : Loss for Batch-> 343\n",
            "0.34844210743904114 : Loss for Batch-> 344\n",
            "0.008676559664309025 : Loss for Batch-> 345\n",
            "0.018918650224804878 : Loss for Batch-> 346\n",
            "0.15615583956241608 : Loss for Batch-> 347\n",
            "0.41598251461982727 : Loss for Batch-> 348\n",
            "0.19620929658412933 : Loss for Batch-> 349\n",
            "0.002152354922145605 : Loss for Batch-> 350\n",
            "0.5466983318328857 : Loss for Batch-> 351\n",
            "0.0413614846765995 : Val Loss after training 351 batche(s)\n",
            "0.8719232082366943 : Loss for Batch-> 352\n",
            "0.08905359357595444 : Loss for Batch-> 353\n",
            "0.025187361985445023 : Loss for Batch-> 354\n",
            "0.9319611191749573 : Loss for Batch-> 355\n",
            "0.9879984259605408 : Loss for Batch-> 356\n",
            "0.004791430663317442 : Loss for Batch-> 357\n",
            "0.00868774950504303 : Loss for Batch-> 358\n",
            "0.0027919558342546225 : Loss for Batch-> 359\n",
            "0.5520039200782776 : Loss for Batch-> 360\n",
            "0.04516472667455673 : Loss for Batch-> 361\n",
            "0.03829040750861168 : Val Loss after training 361 batche(s)\n",
            "0.19018667936325073 : Loss for Batch-> 362\n",
            "0.056348271667957306 : Loss for Batch-> 363\n",
            "0.11950616538524628 : Loss for Batch-> 364\n",
            "0.28259798884391785 : Loss for Batch-> 365\n",
            "0.22988532483577728 : Loss for Batch-> 366\n",
            "0.1506185680627823 : Loss for Batch-> 367\n",
            "0.2469584345817566 : Loss for Batch-> 368\n",
            "0.01439692173153162 : Loss for Batch-> 369\n",
            "0.014147955924272537 : Loss for Batch-> 370\n",
            "0.06105099618434906 : Loss for Batch-> 371\n",
            "0.03759237378835678 : Val Loss after training 371 batche(s)\n",
            "0.15392230451107025 : Loss for Batch-> 372\n",
            "0.7753517627716064 : Loss for Batch-> 373\n",
            "0.10354061424732208 : Loss for Batch-> 374\n",
            "0.8910682201385498 : Loss for Batch-> 375\n",
            "1.0463652610778809 : Loss for Batch-> 376\n",
            "0.16430188715457916 : Loss for Batch-> 377\n",
            "0.5003833770751953 : Loss for Batch-> 378\n",
            "0.09093127399682999 : Loss for Batch-> 379\n",
            "0.021921321749687195 : Loss for Batch-> 380\n",
            "0.04891477897763252 : Loss for Batch-> 381\n",
            "0.02985924296081066 : Val Loss after training 381 batche(s)\n",
            "0.1926123946905136 : Loss for Batch-> 382\n",
            "0.44369280338287354 : Loss for Batch-> 383\n",
            "0.4775971472263336 : Loss for Batch-> 384\n",
            "1.6273062229156494 : Loss for Batch-> 385\n",
            "0.9624286890029907 : Loss for Batch-> 386\n",
            "0.9119157195091248 : Loss for Batch-> 387\n",
            "0.1896868199110031 : Loss for Batch-> 388\n",
            "0.004713643342256546 : Loss for Batch-> 389\n",
            "0.09707887470722198 : Loss for Batch-> 390\n",
            "1.0396417379379272 : Loss for Batch-> 391\n",
            "0.03428816795349121 : Val Loss after training 391 batche(s)\n",
            "1.3514138460159302 : Loss for Batch-> 392\n",
            "0.2373068481683731 : Loss for Batch-> 393\n",
            "0.09100192785263062 : Loss for Batch-> 394\n",
            "0.03895237296819687 : Loss for Batch-> 395\n",
            "0.056003812700510025 : Loss for Batch-> 396\n",
            "0.4082697629928589 : Loss for Batch-> 397\n",
            "0.19608934223651886 : Loss for Batch-> 398\n",
            "1.0611519813537598 : Loss for Batch-> 399\n",
            "0.38562193512916565 : Loss for Batch-> 400\n",
            "0.006914957892149687 : Loss for Batch-> 401\n",
            "0.03542441502213478 : Val Loss after training 401 batche(s)\n",
            "0.008775725960731506 : Loss for Batch-> 402\n",
            "0.018270598724484444 : Loss for Batch-> 403\n",
            "0.011280777864158154 : Loss for Batch-> 404\n",
            "0.4217090308666229 : Loss for Batch-> 405\n",
            "0.5462377667427063 : Loss for Batch-> 406\n",
            "0.2007928043603897 : Loss for Batch-> 407\n",
            "0.0717097744345665 : Loss for Batch-> 408\n",
            "0.040591511875391006 : Loss for Batch-> 409\n",
            "0.009526529349386692 : Loss for Batch-> 410\n",
            "0.08625224232673645 : Loss for Batch-> 411\n",
            "0.03742082789540291 : Val Loss after training 411 batche(s)\n",
            "0.8917186260223389 : Loss for Batch-> 412\n",
            "1.0890552997589111 : Loss for Batch-> 413\n",
            "0.07507599145174026 : Loss for Batch-> 414\n",
            "0.11780942976474762 : Loss for Batch-> 415\n",
            "0.0013764983741566539 : Loss for Batch-> 416\n",
            "0.001761246589012444 : Loss for Batch-> 417\n",
            "0.002688291482627392 : Loss for Batch-> 418\n",
            "0.025377847254276276 : Loss for Batch-> 419\n",
            "0.023934029042720795 : Loss for Batch-> 420\n",
            "0.007379856891930103 : Loss for Batch-> 421\n",
            "0.0031595593318343163 : Val Loss after training 421 batche(s)\n",
            "0.19914890825748444 : Loss for Batch-> 422\n",
            "0.06564734876155853 : Loss for Batch-> 423\n",
            "0.024666545912623405 : Loss for Batch-> 424\n",
            "0.0226505808532238 : Loss for Batch-> 425\n",
            "0.021925698965787888 : Loss for Batch-> 426\n",
            "0.022404814139008522 : Loss for Batch-> 427\n",
            "0.02374104969203472 : Loss for Batch-> 428\n",
            "0.023838303983211517 : Loss for Batch-> 429\n",
            "0.023513037711381912 : Loss for Batch-> 430\n",
            "0.023225802928209305 : Loss for Batch-> 431\n",
            "0.03179390728473663 : Val Loss after training 431 batche(s)\n",
            "0.022443650290369987 : Loss for Batch-> 432\n",
            "0.10437390953302383 : Loss for Batch-> 433\n",
            "0.07208383828401566 : Loss for Batch-> 434\n",
            "0.014299888163805008 : Loss for Batch-> 435\n",
            "0.004243082366883755 : Loss for Batch-> 436\n",
            "0.0014056467916816473 : Loss for Batch-> 437\n",
            "0.0005182799068279564 : Loss for Batch-> 438\n",
            "0.002458342118188739 : Loss for Batch-> 439\n",
            "0.002162846503779292 : Loss for Batch-> 440\n",
            "0.056108757853507996 : Loss for Batch-> 441\n",
            "0.09214690327644348 : Val Loss after training 441 batche(s)\n",
            "0.05524348095059395 : Loss for Batch-> 442\n",
            "0.008648213930428028 : Loss for Batch-> 443\n",
            "0.02514694258570671 : Loss for Batch-> 444\n",
            "0.012275108136236668 : Loss for Batch-> 445\n",
            "0.04697899892926216 : Loss for Batch-> 446\n",
            "0.006815897300839424 : Loss for Batch-> 447\n",
            "0.21109609305858612 : Loss for Batch-> 448\n",
            "0.32849588990211487 : Loss for Batch-> 449\n",
            "0.5388576984405518 : Loss for Batch-> 450\n",
            "0.0007943017990328372 : Loss for Batch-> 451\n",
            "0.043942641466856 : Val Loss after training 451 batche(s)\n",
            "0.0013959127245470881 : Loss for Batch-> 452\n",
            "0.00047874797019176185 : Loss for Batch-> 453\n",
            "0.0008814454777166247 : Loss for Batch-> 454\n",
            "0.0012065042974427342 : Loss for Batch-> 455\n",
            "0.00016887838137336075 : Loss for Batch-> 456\n",
            "0.001189103233627975 : Loss for Batch-> 457\n",
            "0.0001282055745832622 : Loss for Batch-> 458\n",
            "0.00013019390462432057 : Loss for Batch-> 459\n",
            "0.00012674769095610827 : Loss for Batch-> 460\n",
            "0.00012765763676725328 : Loss for Batch-> 461\n",
            "0.00786853302270174 : Val Loss after training 461 batche(s)\n",
            "0.0001274734386242926 : Loss for Batch-> 462\n",
            "0.00019101612269878387 : Loss for Batch-> 463\n",
            "0.0001399748434778303 : Loss for Batch-> 464\n",
            "0.00014277829905040562 : Loss for Batch-> 465\n",
            "0.00014077975356485695 : Loss for Batch-> 466\n",
            "0.0011215097038075328 : Loss for Batch-> 467\n",
            "0.0010502137010917068 : Loss for Batch-> 468\n",
            "0.0005404629046097398 : Loss for Batch-> 469\n",
            "0.0010513742454349995 : Loss for Batch-> 470\n",
            "0.0005610359949059784 : Loss for Batch-> 471\n",
            "0.1041293516755104 : Val Loss after training 471 batche(s)\n",
            "0.00033872551284730434 : Loss for Batch-> 472\n",
            "0.00040285271825268865 : Loss for Batch-> 473\n",
            "0.0008407587301917374 : Loss for Batch-> 474\n",
            "0.0006246232078410685 : Loss for Batch-> 475\n",
            "0.0010481639765203 : Loss for Batch-> 476\n",
            "0.0005657898145727813 : Loss for Batch-> 477\n",
            "0.0010837488807737827 : Loss for Batch-> 478\n",
            "0.0006228031124919653 : Loss for Batch-> 479\n",
            "0.0004729169304482639 : Loss for Batch-> 480\n",
            "0.00043674802873283625 : Loss for Batch-> 481\n",
            "2.8270514011383057 : Val Loss after training 481 batche(s)\n",
            "0.0018681675428524613 : Loss for Batch-> 482\n",
            "0.023468229919672012 : Loss for Batch-> 483\n",
            "0.008740877732634544 : Loss for Batch-> 484\n",
            "0.003756054677069187 : Loss for Batch-> 485\n",
            "0.003450082615017891 : Loss for Batch-> 486\n",
            "0.0035487692803144455 : Loss for Batch-> 487\n",
            "0.003578213043510914 : Loss for Batch-> 488\n",
            "0.0034707458689808846 : Loss for Batch-> 489\n",
            "0.0034653227776288986 : Loss for Batch-> 490\n",
            "0.0035951442550867796 : Loss for Batch-> 491\n",
            "2.5871193408966064 : Val Loss after training 491 batche(s)\n",
            "0.0022405327763408422 : Loss for Batch-> 492\n",
            "0.0020782891660928726 : Loss for Batch-> 493\n",
            "0.0020180633291602135 : Loss for Batch-> 494\n",
            "0.034232426434755325 : Loss for Batch-> 495\n",
            "1.7261683940887451 : Loss for Batch-> 496\n",
            "0.00492211664095521 : Loss for Batch-> 497\n",
            "0.018509436398744583 : Loss for Batch-> 498\n",
            "0.003539125435054302 : Loss for Batch-> 499\n",
            "0.0009658131166361272 : Loss for Batch-> 500\n",
            "0.0011022398248314857 : Loss for Batch-> 501\n",
            "0.08184465765953064 : Val Loss after training 501 batche(s)\n",
            "0.0022915848530828953 : Loss for Batch-> 502\n",
            "0.0034249722957611084 : Loss for Batch-> 503\n",
            "0.03303251415491104 : Loss for Batch-> 504\n",
            "0.0244655292481184 : Loss for Batch-> 505\n",
            "0.009075717069208622 : Loss for Batch-> 506\n",
            "0.020319053903222084 : Loss for Batch-> 507\n",
            "0.019900614395737648 : Loss for Batch-> 508\n",
            "0.018998505547642708 : Loss for Batch-> 509\n",
            "0.015496356412768364 : Loss for Batch-> 510\n",
            "28 Epoch\n",
            "0.008599978871643543 : Loss for Batch-> 1\n",
            "0.012547623366117477 : Val Loss after training 1 batche(s)\n",
            "0.008842132054269314 : Loss for Batch-> 2\n",
            "0.008985009044408798 : Loss for Batch-> 3\n",
            "0.00879023689776659 : Loss for Batch-> 4\n",
            "0.008513166569173336 : Loss for Batch-> 5\n",
            "0.00828593410551548 : Loss for Batch-> 6\n",
            "0.007880115881562233 : Loss for Batch-> 7\n",
            "0.007786175236105919 : Loss for Batch-> 8\n",
            "0.011455629020929337 : Loss for Batch-> 9\n",
            "0.01793072000145912 : Loss for Batch-> 10\n",
            "0.01780296489596367 : Loss for Batch-> 11\n",
            "0.0005264164647087455 : Val Loss after training 11 batche(s)\n",
            "0.018526000902056694 : Loss for Batch-> 12\n",
            "0.9640439748764038 : Loss for Batch-> 13\n",
            "5.243750095367432 : Loss for Batch-> 14\n",
            "0.0018861793214455247 : Loss for Batch-> 15\n",
            "0.0016454111319035292 : Loss for Batch-> 16\n",
            "0.0013247705064713955 : Loss for Batch-> 17\n",
            "0.001473413547500968 : Loss for Batch-> 18\n",
            "0.005330873187631369 : Loss for Batch-> 19\n",
            "0.011070230044424534 : Loss for Batch-> 20\n",
            "0.0010205794824287295 : Loss for Batch-> 21\n",
            "0.0021903570741415024 : Val Loss after training 21 batche(s)\n",
            "0.010963119566440582 : Loss for Batch-> 22\n",
            "0.021491490304470062 : Loss for Batch-> 23\n",
            "0.0203109011054039 : Loss for Batch-> 24\n",
            "0.018024079501628876 : Loss for Batch-> 25\n",
            "0.019440732896327972 : Loss for Batch-> 26\n",
            "0.013672281056642532 : Loss for Batch-> 27\n",
            "0.0007854671566747129 : Loss for Batch-> 28\n",
            "0.0004034210869576782 : Loss for Batch-> 29\n",
            "0.0010112017625942826 : Loss for Batch-> 30\n",
            "0.003968645352870226 : Loss for Batch-> 31\n",
            "0.0013590453891083598 : Val Loss after training 31 batche(s)\n",
            "0.0024621207267045975 : Loss for Batch-> 32\n",
            "0.0012899036519229412 : Loss for Batch-> 33\n",
            "9.551940456731245e-05 : Loss for Batch-> 34\n",
            "6.811306957388297e-05 : Loss for Batch-> 35\n",
            "2.2325713871396147e-05 : Loss for Batch-> 36\n",
            "0.0007504586828872561 : Loss for Batch-> 37\n",
            "0.0005116438260301948 : Loss for Batch-> 38\n",
            "0.0002726578386500478 : Loss for Batch-> 39\n",
            "0.012506323866546154 : Loss for Batch-> 40\n",
            "0.0027809124439954758 : Loss for Batch-> 41\n",
            "0.0013733523664996028 : Val Loss after training 41 batche(s)\n",
            "0.00017677978030405939 : Loss for Batch-> 42\n",
            "0.0006496839923784137 : Loss for Batch-> 43\n",
            "0.0004568629083223641 : Loss for Batch-> 44\n",
            "0.0005611577071249485 : Loss for Batch-> 45\n",
            "0.0601411908864975 : Loss for Batch-> 46\n",
            "0.0633724182844162 : Loss for Batch-> 47\n",
            "0.06888017058372498 : Loss for Batch-> 48\n",
            "0.014962361194193363 : Loss for Batch-> 49\n",
            "0.018114782869815826 : Loss for Batch-> 50\n",
            "0.22652743756771088 : Loss for Batch-> 51\n",
            "0.0008002453250810504 : Val Loss after training 51 batche(s)\n",
            "0.2534658908843994 : Loss for Batch-> 52\n",
            "0.1268332600593567 : Loss for Batch-> 53\n",
            "0.0008424788829870522 : Loss for Batch-> 54\n",
            "0.000759703223593533 : Loss for Batch-> 55\n",
            "0.01236697193235159 : Loss for Batch-> 56\n",
            "0.016452187672257423 : Loss for Batch-> 57\n",
            "0.015303862281143665 : Loss for Batch-> 58\n",
            "0.0017589933704584837 : Loss for Batch-> 59\n",
            "0.0017108544707298279 : Loss for Batch-> 60\n",
            "0.01433945819735527 : Loss for Batch-> 61\n",
            "0.02344449982047081 : Val Loss after training 61 batche(s)\n",
            "0.16033491492271423 : Loss for Batch-> 62\n",
            "0.228561669588089 : Loss for Batch-> 63\n",
            "0.15896809101104736 : Loss for Batch-> 64\n",
            "0.05477498471736908 : Loss for Batch-> 65\n",
            "0.09361837059259415 : Loss for Batch-> 66\n",
            "0.013120736926794052 : Loss for Batch-> 67\n",
            "0.14028410613536835 : Loss for Batch-> 68\n",
            "0.04894525557756424 : Loss for Batch-> 69\n",
            "0.08370622992515564 : Loss for Batch-> 70\n",
            "0.011461961083114147 : Loss for Batch-> 71\n",
            "0.001147688482888043 : Val Loss after training 71 batche(s)\n",
            "0.011760091409087181 : Loss for Batch-> 72\n",
            "0.011441626586019993 : Loss for Batch-> 73\n",
            "0.010492238216102123 : Loss for Batch-> 74\n",
            "0.011000705882906914 : Loss for Batch-> 75\n",
            "0.010700923390686512 : Loss for Batch-> 76\n",
            "0.0246280524879694 : Loss for Batch-> 77\n",
            "1.1473307609558105 : Loss for Batch-> 78\n",
            "1.4219290018081665 : Loss for Batch-> 79\n",
            "0.06564277410507202 : Loss for Batch-> 80\n",
            "0.07343454658985138 : Loss for Batch-> 81\n",
            "0.002464261604472995 : Val Loss after training 81 batche(s)\n",
            "0.08517129719257355 : Loss for Batch-> 82\n",
            "0.0034961665514856577 : Loss for Batch-> 83\n",
            "0.001962859882041812 : Loss for Batch-> 84\n",
            "0.06398341804742813 : Loss for Batch-> 85\n",
            "0.08438526839017868 : Loss for Batch-> 86\n",
            "0.08813808113336563 : Loss for Batch-> 87\n",
            "0.060604874044656754 : Loss for Batch-> 88\n",
            "0.022557836025953293 : Loss for Batch-> 89\n",
            "0.013285423628985882 : Loss for Batch-> 90\n",
            "0.04653671383857727 : Loss for Batch-> 91\n",
            "0.0018672976875677705 : Val Loss after training 91 batche(s)\n",
            "0.02006302773952484 : Loss for Batch-> 92\n",
            "0.005882661323994398 : Loss for Batch-> 93\n",
            "0.04831548035144806 : Loss for Batch-> 94\n",
            "0.028887443244457245 : Loss for Batch-> 95\n",
            "0.0017509961035102606 : Loss for Batch-> 96\n",
            "0.018811535090208054 : Loss for Batch-> 97\n",
            "0.0013520296197384596 : Loss for Batch-> 98\n",
            "0.0006778807146474719 : Loss for Batch-> 99\n",
            "0.0018646317766979337 : Loss for Batch-> 100\n",
            "0.007107574958354235 : Loss for Batch-> 101\n",
            "0.030272912234067917 : Val Loss after training 101 batche(s)\n",
            "0.0013750276993960142 : Loss for Batch-> 102\n",
            "0.0029348647221922874 : Loss for Batch-> 103\n",
            "0.0020443631801754236 : Loss for Batch-> 104\n",
            "0.013502920977771282 : Loss for Batch-> 105\n",
            "0.01821821741759777 : Loss for Batch-> 106\n",
            "0.017880218103528023 : Loss for Batch-> 107\n",
            "0.01924474723637104 : Loss for Batch-> 108\n",
            "0.0007876692689023912 : Loss for Batch-> 109\n",
            "0.0019800111185759306 : Loss for Batch-> 110\n",
            "0.020911721512675285 : Loss for Batch-> 111\n",
            "0.038669317960739136 : Val Loss after training 111 batche(s)\n",
            "0.018332261592149734 : Loss for Batch-> 112\n",
            "0.004607631824910641 : Loss for Batch-> 113\n",
            "0.001774971024133265 : Loss for Batch-> 114\n",
            "0.0025465607177466154 : Loss for Batch-> 115\n",
            "0.047215767204761505 : Loss for Batch-> 116\n",
            "0.04630623757839203 : Loss for Batch-> 117\n",
            "0.03375422582030296 : Loss for Batch-> 118\n",
            "0.004345548339188099 : Loss for Batch-> 119\n",
            "0.07435666769742966 : Loss for Batch-> 120\n",
            "0.005404449068009853 : Loss for Batch-> 121\n",
            "0.007101858034729958 : Val Loss after training 121 batche(s)\n",
            "0.011704182252287865 : Loss for Batch-> 122\n",
            "0.012761389836668968 : Loss for Batch-> 123\n",
            "0.07160400599241257 : Loss for Batch-> 124\n",
            "0.0852566808462143 : Loss for Batch-> 125\n",
            "0.03291655331850052 : Loss for Batch-> 126\n",
            "0.015599655918776989 : Loss for Batch-> 127\n",
            "0.027968134731054306 : Loss for Batch-> 128\n",
            "0.0023011653684079647 : Loss for Batch-> 129\n",
            "0.0029841577634215355 : Loss for Batch-> 130\n",
            "0.033849045634269714 : Loss for Batch-> 131\n",
            "0.0010345048503950238 : Val Loss after training 131 batche(s)\n",
            "0.05102857947349548 : Loss for Batch-> 132\n",
            "0.11088129132986069 : Loss for Batch-> 133\n",
            "0.03711620345711708 : Loss for Batch-> 134\n",
            "0.017964795231819153 : Loss for Batch-> 135\n",
            "0.056820619851350784 : Loss for Batch-> 136\n",
            "0.01260815653949976 : Loss for Batch-> 137\n",
            "0.2787973880767822 : Loss for Batch-> 138\n",
            "0.03824678435921669 : Loss for Batch-> 139\n",
            "0.028424793854355812 : Loss for Batch-> 140\n",
            "0.0003995048755314201 : Loss for Batch-> 141\n",
            "0.037436243146657944 : Val Loss after training 141 batche(s)\n",
            "0.0009357492672279477 : Loss for Batch-> 142\n",
            "0.00043736203224398196 : Loss for Batch-> 143\n",
            "0.002753055887296796 : Loss for Batch-> 144\n",
            "0.011192072182893753 : Loss for Batch-> 145\n",
            "0.01861024834215641 : Loss for Batch-> 146\n",
            "0.011481331661343575 : Loss for Batch-> 147\n",
            "0.010815496556460857 : Loss for Batch-> 148\n",
            "0.0021661485079675913 : Loss for Batch-> 149\n",
            "0.0018893032101914287 : Loss for Batch-> 150\n",
            "0.001260103890672326 : Loss for Batch-> 151\n",
            "0.19498483836650848 : Val Loss after training 151 batche(s)\n",
            "0.03618363291025162 : Loss for Batch-> 152\n",
            "0.06890767812728882 : Loss for Batch-> 153\n",
            "0.008443091064691544 : Loss for Batch-> 154\n",
            "0.02717176266014576 : Loss for Batch-> 155\n",
            "0.0440741628408432 : Loss for Batch-> 156\n",
            "0.025057684630155563 : Loss for Batch-> 157\n",
            "0.003432076657190919 : Loss for Batch-> 158\n",
            "0.0596005916595459 : Loss for Batch-> 159\n",
            "0.002908270573243499 : Loss for Batch-> 160\n",
            "0.01167534850537777 : Loss for Batch-> 161\n",
            "0.23606470227241516 : Val Loss after training 161 batche(s)\n",
            "0.007738787680864334 : Loss for Batch-> 162\n",
            "0.07150177657604218 : Loss for Batch-> 163\n",
            "0.05003632232546806 : Loss for Batch-> 164\n",
            "0.007893992587924004 : Loss for Batch-> 165\n",
            "0.04571474716067314 : Loss for Batch-> 166\n",
            "0.04597020521759987 : Loss for Batch-> 167\n",
            "0.050674643367528915 : Loss for Batch-> 168\n",
            "0.015759257599711418 : Loss for Batch-> 169\n",
            "0.02812587283551693 : Loss for Batch-> 170\n",
            "0.010534503497183323 : Loss for Batch-> 171\n",
            "0.0005731573910452425 : Val Loss after training 171 batche(s)\n",
            "0.0002476359950378537 : Loss for Batch-> 172\n",
            "0.0009843718726187944 : Loss for Batch-> 173\n",
            "0.0013995416229590774 : Loss for Batch-> 174\n",
            "0.00031189023866318166 : Loss for Batch-> 175\n",
            "0.0003282767429482192 : Loss for Batch-> 176\n",
            "0.0012595229782164097 : Loss for Batch-> 177\n",
            "0.020311908796429634 : Loss for Batch-> 178\n",
            "0.011311355046927929 : Loss for Batch-> 179\n",
            "0.00048776614130474627 : Loss for Batch-> 180\n",
            "0.00045392391621135175 : Loss for Batch-> 181\n",
            "0.005636049434542656 : Val Loss after training 181 batche(s)\n",
            "4.606477887136862e-05 : Loss for Batch-> 182\n",
            "7.429756806232035e-06 : Loss for Batch-> 183\n",
            "0.00016144989058375359 : Loss for Batch-> 184\n",
            "0.0005465387366712093 : Loss for Batch-> 185\n",
            "0.00015317570068873465 : Loss for Batch-> 186\n",
            "0.000997617724351585 : Loss for Batch-> 187\n",
            "0.00011961315613007173 : Loss for Batch-> 188\n",
            "0.0004321412998251617 : Loss for Batch-> 189\n",
            "0.0005873592454008758 : Loss for Batch-> 190\n",
            "0.0012297044740989804 : Loss for Batch-> 191\n",
            "0.005157873500138521 : Val Loss after training 191 batche(s)\n",
            "0.015302875079214573 : Loss for Batch-> 192\n",
            "0.029613185673952103 : Loss for Batch-> 193\n",
            "0.005495326593518257 : Loss for Batch-> 194\n",
            "0.002331443829461932 : Loss for Batch-> 195\n",
            "0.0007936772890388966 : Loss for Batch-> 196\n",
            "0.0001899356720969081 : Loss for Batch-> 197\n",
            "0.0008238393347710371 : Loss for Batch-> 198\n",
            "0.9376364350318909 : Loss for Batch-> 199\n",
            "2.1881635189056396 : Loss for Batch-> 200\n",
            "0.0401616245508194 : Loss for Batch-> 201\n",
            "0.00535321282222867 : Val Loss after training 201 batche(s)\n",
            "0.02052377723157406 : Loss for Batch-> 202\n",
            "0.08021026849746704 : Loss for Batch-> 203\n",
            "0.020480170845985413 : Loss for Batch-> 204\n",
            "0.010253092274069786 : Loss for Batch-> 205\n",
            "0.02301163226366043 : Loss for Batch-> 206\n",
            "0.0109261991456151 : Loss for Batch-> 207\n",
            "0.006796421483159065 : Loss for Batch-> 208\n",
            "0.005986577831208706 : Loss for Batch-> 209\n",
            "0.006814055610448122 : Loss for Batch-> 210\n",
            "0.0409194678068161 : Loss for Batch-> 211\n",
            "0.005614602472633123 : Val Loss after training 211 batche(s)\n",
            "0.008428532630205154 : Loss for Batch-> 212\n",
            "0.015695761889219284 : Loss for Batch-> 213\n",
            "0.029422888532280922 : Loss for Batch-> 214\n",
            "0.018555540591478348 : Loss for Batch-> 215\n",
            "0.05134591460227966 : Loss for Batch-> 216\n",
            "0.005066275596618652 : Loss for Batch-> 217\n",
            "0.000599633960518986 : Loss for Batch-> 218\n",
            "0.0005120330024510622 : Loss for Batch-> 219\n",
            "0.029656702652573586 : Loss for Batch-> 220\n",
            "0.07178299129009247 : Loss for Batch-> 221\n",
            "0.01642068661749363 : Val Loss after training 221 batche(s)\n",
            "0.013624296523630619 : Loss for Batch-> 222\n",
            "0.06519424170255661 : Loss for Batch-> 223\n",
            "0.11491034179925919 : Loss for Batch-> 224\n",
            "0.000747301965020597 : Loss for Batch-> 225\n",
            "0.0004101595259271562 : Loss for Batch-> 226\n",
            "0.0003979176690336317 : Loss for Batch-> 227\n",
            "0.012488579377532005 : Loss for Batch-> 228\n",
            "2.4241178035736084 : Loss for Batch-> 229\n",
            "0.3090583086013794 : Loss for Batch-> 230\n",
            "11.136251449584961 : Loss for Batch-> 231\n",
            "3.037036418914795 : Val Loss after training 231 batche(s)\n",
            "0.14603210985660553 : Loss for Batch-> 232\n",
            "0.27621424198150635 : Loss for Batch-> 233\n",
            "0.03179727494716644 : Loss for Batch-> 234\n",
            "0.002269790507853031 : Loss for Batch-> 235\n",
            "0.06097311154007912 : Loss for Batch-> 236\n",
            "0.00705722626298666 : Loss for Batch-> 237\n",
            "0.0013887779787182808 : Loss for Batch-> 238\n",
            "0.0010130165610462427 : Loss for Batch-> 239\n",
            "0.03822841867804527 : Loss for Batch-> 240\n",
            "0.11183933168649673 : Loss for Batch-> 241\n",
            "0.08282151073217392 : Val Loss after training 241 batche(s)\n",
            "0.11113075166940689 : Loss for Batch-> 242\n",
            "0.16414394974708557 : Loss for Batch-> 243\n",
            "0.776671290397644 : Loss for Batch-> 244\n",
            "2.6459884643554688 : Loss for Batch-> 245\n",
            "2.3035194873809814 : Loss for Batch-> 246\n",
            "0.026851749047636986 : Loss for Batch-> 247\n",
            "0.22252677381038666 : Loss for Batch-> 248\n",
            "4.212948799133301 : Loss for Batch-> 249\n",
            "0.2196301817893982 : Loss for Batch-> 250\n",
            "1.0126835107803345 : Loss for Batch-> 251\n",
            "0.0005105870077386498 : Val Loss after training 251 batche(s)\n",
            "0.01235473994165659 : Loss for Batch-> 252\n",
            "0.3008027970790863 : Loss for Batch-> 253\n",
            "0.4417465329170227 : Loss for Batch-> 254\n",
            "0.49094319343566895 : Loss for Batch-> 255\n",
            "2.8759765625 : Loss for Batch-> 256\n",
            "6.539427280426025 : Loss for Batch-> 257\n",
            "0.05241058021783829 : Loss for Batch-> 258\n",
            "0.029777025803923607 : Loss for Batch-> 259\n",
            "0.653282642364502 : Loss for Batch-> 260\n",
            "1.5045361518859863 : Loss for Batch-> 261\n",
            "0.014925447292625904 : Val Loss after training 261 batche(s)\n",
            "3.762624740600586 : Loss for Batch-> 262\n",
            "1.203027367591858 : Loss for Batch-> 263\n",
            "0.009023181162774563 : Loss for Batch-> 264\n",
            "0.18971335887908936 : Loss for Batch-> 265\n",
            "1.6430141925811768 : Loss for Batch-> 266\n",
            "0.34303298592567444 : Loss for Batch-> 267\n",
            "0.10084675997495651 : Loss for Batch-> 268\n",
            "0.0003240313089918345 : Loss for Batch-> 269\n",
            "0.8668885231018066 : Loss for Batch-> 270\n",
            "5.12129020690918 : Loss for Batch-> 271\n",
            "0.00682296697050333 : Val Loss after training 271 batche(s)\n",
            "0.011946358717978 : Loss for Batch-> 272\n",
            "0.0005646677454933524 : Loss for Batch-> 273\n",
            "0.0012271456653252244 : Loss for Batch-> 274\n",
            "0.0015092549147084355 : Loss for Batch-> 275\n",
            "0.0021178245078772306 : Loss for Batch-> 276\n",
            "0.0016577139031141996 : Loss for Batch-> 277\n",
            "0.0025101162027567625 : Loss for Batch-> 278\n",
            "0.002179271774366498 : Loss for Batch-> 279\n",
            "0.018776167184114456 : Loss for Batch-> 280\n",
            "0.02116335555911064 : Loss for Batch-> 281\n",
            "0.0009317868971265852 : Val Loss after training 281 batche(s)\n",
            "0.05963186174631119 : Loss for Batch-> 282\n",
            "0.06371071934700012 : Loss for Batch-> 283\n",
            "0.07010501623153687 : Loss for Batch-> 284\n",
            "0.04622408002614975 : Loss for Batch-> 285\n",
            "0.008962297812104225 : Loss for Batch-> 286\n",
            "6.759040355682373 : Loss for Batch-> 287\n",
            "7.6184468269348145 : Loss for Batch-> 288\n",
            "0.011837810277938843 : Loss for Batch-> 289\n",
            "0.002135887509211898 : Loss for Batch-> 290\n",
            "0.012256239540874958 : Loss for Batch-> 291\n",
            "0.0025690486654639244 : Val Loss after training 291 batche(s)\n",
            "0.15265579521656036 : Loss for Batch-> 292\n",
            "0.018618084490299225 : Loss for Batch-> 293\n",
            "0.0029512960463762283 : Loss for Batch-> 294\n",
            "0.0014316578162834048 : Loss for Batch-> 295\n",
            "0.0021277975756675005 : Loss for Batch-> 296\n",
            "0.06347904354333878 : Loss for Batch-> 297\n",
            "0.007533583790063858 : Loss for Batch-> 298\n",
            "0.014746802859008312 : Loss for Batch-> 299\n",
            "0.028385572135448456 : Loss for Batch-> 300\n",
            "0.5080086588859558 : Loss for Batch-> 301\n",
            "0.0006100803148001432 : Val Loss after training 301 batche(s)\n",
            "1.1010569334030151 : Loss for Batch-> 302\n",
            "1.038748025894165 : Loss for Batch-> 303\n",
            "0.06375662982463837 : Loss for Batch-> 304\n",
            "0.06623124331235886 : Loss for Batch-> 305\n",
            "0.440265953540802 : Loss for Batch-> 306\n",
            "2.0587239265441895 : Loss for Batch-> 307\n",
            "1.4480984210968018 : Loss for Batch-> 308\n",
            "0.03992243483662605 : Loss for Batch-> 309\n",
            "0.05468101426959038 : Loss for Batch-> 310\n",
            "0.03076900728046894 : Loss for Batch-> 311\n",
            "0.0036337978672236204 : Val Loss after training 311 batche(s)\n",
            "1.3991873264312744 : Loss for Batch-> 312\n",
            "1.4237173795700073 : Loss for Batch-> 313\n",
            "0.16119523346424103 : Loss for Batch-> 314\n",
            "0.017264781519770622 : Loss for Batch-> 315\n",
            "0.0008799205534160137 : Loss for Batch-> 316\n",
            "0.537086546421051 : Loss for Batch-> 317\n",
            "0.10506600141525269 : Loss for Batch-> 318\n",
            "0.025117266923189163 : Loss for Batch-> 319\n",
            "0.05943084880709648 : Loss for Batch-> 320\n",
            "0.21066653728485107 : Loss for Batch-> 321\n",
            "0.018881114199757576 : Val Loss after training 321 batche(s)\n",
            "0.11099781095981598 : Loss for Batch-> 322\n",
            "0.332940012216568 : Loss for Batch-> 323\n",
            "0.1529470682144165 : Loss for Batch-> 324\n",
            "0.03686150535941124 : Loss for Batch-> 325\n",
            "0.21047727763652802 : Loss for Batch-> 326\n",
            "0.25162503123283386 : Loss for Batch-> 327\n",
            "0.5060757994651794 : Loss for Batch-> 328\n",
            "0.4353012144565582 : Loss for Batch-> 329\n",
            "0.017192417755723 : Loss for Batch-> 330\n",
            "0.09688153117895126 : Loss for Batch-> 331\n",
            "0.0009591896086931229 : Val Loss after training 331 batche(s)\n",
            "0.07859283685684204 : Loss for Batch-> 332\n",
            "0.0069612967781722546 : Loss for Batch-> 333\n",
            "0.009526649489998817 : Loss for Batch-> 334\n",
            "0.0029243621975183487 : Loss for Batch-> 335\n",
            "0.0004885616945102811 : Loss for Batch-> 336\n",
            "0.0010415964061394334 : Loss for Batch-> 337\n",
            "0.0025973492302000523 : Loss for Batch-> 338\n",
            "0.0013103740056976676 : Loss for Batch-> 339\n",
            "0.05417211353778839 : Loss for Batch-> 340\n",
            "0.04744049161672592 : Loss for Batch-> 341\n",
            "0.009915915317833424 : Val Loss after training 341 batche(s)\n",
            "0.05781857669353485 : Loss for Batch-> 342\n",
            "0.0287164356559515 : Loss for Batch-> 343\n",
            "0.5137087106704712 : Loss for Batch-> 344\n",
            "0.06911590695381165 : Loss for Batch-> 345\n",
            "0.013195306062698364 : Loss for Batch-> 346\n",
            "0.05451088398694992 : Loss for Batch-> 347\n",
            "0.259635329246521 : Loss for Batch-> 348\n",
            "0.44276320934295654 : Loss for Batch-> 349\n",
            "0.025338830426335335 : Loss for Batch-> 350\n",
            "0.04697064310312271 : Loss for Batch-> 351\n",
            "0.0014405122492462397 : Val Loss after training 351 batche(s)\n",
            "0.8122754693031311 : Loss for Batch-> 352\n",
            "0.6214704513549805 : Loss for Batch-> 353\n",
            "0.0420299656689167 : Loss for Batch-> 354\n",
            "0.10142337530851364 : Loss for Batch-> 355\n",
            "1.65240478515625 : Loss for Batch-> 356\n",
            "0.17839854955673218 : Loss for Batch-> 357\n",
            "0.009798496030271053 : Loss for Batch-> 358\n",
            "0.003967126365751028 : Loss for Batch-> 359\n",
            "0.10117331147193909 : Loss for Batch-> 360\n",
            "0.4643075466156006 : Loss for Batch-> 361\n",
            "0.000403685902711004 : Val Loss after training 361 batche(s)\n",
            "0.044788945466279984 : Loss for Batch-> 362\n",
            "0.23248210549354553 : Loss for Batch-> 363\n",
            "0.022309251129627228 : Loss for Batch-> 364\n",
            "0.1459522545337677 : Loss for Batch-> 365\n",
            "0.4540788531303406 : Loss for Batch-> 366\n",
            "0.038639556616544724 : Loss for Batch-> 367\n",
            "0.25308695435523987 : Loss for Batch-> 368\n",
            "0.131066232919693 : Loss for Batch-> 369\n",
            "0.002438037656247616 : Loss for Batch-> 370\n",
            "0.03499719500541687 : Loss for Batch-> 371\n",
            "0.0010448666289448738 : Val Loss after training 371 batche(s)\n",
            "0.0447603240609169 : Loss for Batch-> 372\n",
            "0.5283920764923096 : Loss for Batch-> 373\n",
            "0.4650977849960327 : Loss for Batch-> 374\n",
            "0.23692627251148224 : Loss for Batch-> 375\n",
            "1.1451622247695923 : Loss for Batch-> 376\n",
            "0.6109601855278015 : Loss for Batch-> 377\n",
            "0.4172600507736206 : Loss for Batch-> 378\n",
            "0.31237685680389404 : Loss for Batch-> 379\n",
            "0.007768630515784025 : Loss for Batch-> 380\n",
            "0.03437746688723564 : Loss for Batch-> 381\n",
            "0.056067511439323425 : Val Loss after training 381 batche(s)\n",
            "0.08369051665067673 : Loss for Batch-> 382\n",
            "0.1698950231075287 : Loss for Batch-> 383\n",
            "0.7824962735176086 : Loss for Batch-> 384\n",
            "0.2592146396636963 : Loss for Batch-> 385\n",
            "2.2078537940979004 : Loss for Batch-> 386\n",
            "0.5787283182144165 : Loss for Batch-> 387\n",
            "0.7320213317871094 : Loss for Batch-> 388\n",
            "0.02649126574397087 : Loss for Batch-> 389\n",
            "0.008561001159250736 : Loss for Batch-> 390\n",
            "0.33830776810646057 : Loss for Batch-> 391\n",
            "0.15334001183509827 : Val Loss after training 391 batche(s)\n",
            "1.3398436307907104 : Loss for Batch-> 392\n",
            "1.0326259136199951 : Loss for Batch-> 393\n",
            "0.028433287516236305 : Loss for Batch-> 394\n",
            "0.10571064800024033 : Loss for Batch-> 395\n",
            "0.007476080674678087 : Loss for Batch-> 396\n",
            "0.15445885062217712 : Loss for Batch-> 397\n",
            "0.3265826404094696 : Loss for Batch-> 398\n",
            "0.466033399105072 : Loss for Batch-> 399\n",
            "0.9847859144210815 : Loss for Batch-> 400\n",
            "0.17811746895313263 : Loss for Batch-> 401\n",
            "0.024761758744716644 : Val Loss after training 401 batche(s)\n",
            "0.005480585619807243 : Loss for Batch-> 402\n",
            "0.007628444582223892 : Loss for Batch-> 403\n",
            "0.018934976309537888 : Loss for Batch-> 404\n",
            "0.08335500955581665 : Loss for Batch-> 405\n",
            "0.5885264873504639 : Loss for Batch-> 406\n",
            "0.4010985195636749 : Loss for Batch-> 407\n",
            "0.15216684341430664 : Loss for Batch-> 408\n",
            "0.04300469532608986 : Loss for Batch-> 409\n",
            "0.0226735956966877 : Loss for Batch-> 410\n",
            "0.02624223195016384 : Loss for Batch-> 411\n",
            "0.0007442085188813508 : Val Loss after training 411 batche(s)\n",
            "0.29381850361824036 : Loss for Batch-> 412\n",
            "1.2577615976333618 : Loss for Batch-> 413\n",
            "0.5456550121307373 : Loss for Batch-> 414\n",
            "0.11189710348844528 : Loss for Batch-> 415\n",
            "0.03382645174860954 : Loss for Batch-> 416\n",
            "0.0007501169457100332 : Loss for Batch-> 417\n",
            "0.0017492080805823207 : Loss for Batch-> 418\n",
            "0.012165270745754242 : Loss for Batch-> 419\n",
            "0.03011670894920826 : Loss for Batch-> 420\n",
            "0.009542474523186684 : Loss for Batch-> 421\n",
            "0.0018735527992248535 : Val Loss after training 421 batche(s)\n",
            "0.11460184305906296 : Loss for Batch-> 422\n",
            "0.12774105370044708 : Loss for Batch-> 423\n",
            "0.040900349617004395 : Loss for Batch-> 424\n",
            "0.02295459806919098 : Loss for Batch-> 425\n",
            "0.022225679829716682 : Loss for Batch-> 426\n",
            "0.02218797616660595 : Loss for Batch-> 427\n",
            "0.02268858440220356 : Loss for Batch-> 428\n",
            "0.023911047726869583 : Loss for Batch-> 429\n",
            "0.024116449058055878 : Loss for Batch-> 430\n",
            "0.023405421525239944 : Loss for Batch-> 431\n",
            "0.0022266856394708157 : Val Loss after training 431 batche(s)\n",
            "0.02274562604725361 : Loss for Batch-> 432\n",
            "0.02215203084051609 : Loss for Batch-> 433\n",
            "0.14596213400363922 : Loss for Batch-> 434\n",
            "0.035314109176397324 : Loss for Batch-> 435\n",
            "0.002647467888891697 : Loss for Batch-> 436\n",
            "0.0030763791874051094 : Loss for Batch-> 437\n",
            "0.0012907558120787144 : Loss for Batch-> 438\n",
            "0.0008214535773731768 : Loss for Batch-> 439\n",
            "0.0027547264471650124 : Loss for Batch-> 440\n",
            "0.009415569715201855 : Loss for Batch-> 441\n",
            "0.0033415898215025663 : Val Loss after training 441 batche(s)\n",
            "0.06476791203022003 : Loss for Batch-> 442\n",
            "0.04149806872010231 : Loss for Batch-> 443\n",
            "0.010697717778384686 : Loss for Batch-> 444\n",
            "0.030930504202842712 : Loss for Batch-> 445\n",
            "0.0212362352758646 : Loss for Batch-> 446\n",
            "0.033054985105991364 : Loss for Batch-> 447\n",
            "0.021482961252331734 : Loss for Batch-> 448\n",
            "0.41799652576446533 : Loss for Batch-> 449\n",
            "0.5400474071502686 : Loss for Batch-> 450\n",
            "0.10005886852741241 : Loss for Batch-> 451\n",
            "0.005127177108079195 : Val Loss after training 451 batche(s)\n",
            "0.0010284860618412495 : Loss for Batch-> 452\n",
            "0.001037695212289691 : Loss for Batch-> 453\n",
            "0.0003949301317334175 : Loss for Batch-> 454\n",
            "0.0011121956631541252 : Loss for Batch-> 455\n",
            "0.0010037104366347194 : Loss for Batch-> 456\n",
            "0.0007972163148224354 : Loss for Batch-> 457\n",
            "0.0005211785319261253 : Loss for Batch-> 458\n",
            "0.00012892963422928005 : Loss for Batch-> 459\n",
            "0.0001298074930673465 : Loss for Batch-> 460\n",
            "0.00012747306027449667 : Loss for Batch-> 461\n",
            "0.006192519795149565 : Val Loss after training 461 batche(s)\n",
            "0.00012835419329348952 : Loss for Batch-> 462\n",
            "0.00016114614845719188 : Loss for Batch-> 463\n",
            "0.00016158298240043223 : Loss for Batch-> 464\n",
            "0.0001400134206051007 : Loss for Batch-> 465\n",
            "0.00014088883472140878 : Loss for Batch-> 466\n",
            "0.0001398155145579949 : Loss for Batch-> 467\n",
            "0.0016898317262530327 : Loss for Batch-> 468\n",
            "0.0006150412373244762 : Loss for Batch-> 469\n",
            "0.0005254666320979595 : Loss for Batch-> 470\n",
            "0.0013939981581643224 : Loss for Batch-> 471\n",
            "0.006177195347845554 : Val Loss after training 471 batche(s)\n",
            "8.394791802857071e-05 : Loss for Batch-> 472\n",
            "0.00045298185432329774 : Loss for Batch-> 473\n",
            "0.0005472918855957687 : Loss for Batch-> 474\n",
            "0.0009022977901622653 : Loss for Batch-> 475\n",
            "0.0004108638968318701 : Loss for Batch-> 476\n",
            "0.0011155287502333522 : Loss for Batch-> 477\n",
            "0.0006648145499639213 : Loss for Batch-> 478\n",
            "0.001256360555998981 : Loss for Batch-> 479\n",
            "0.00024698275956325233 : Loss for Batch-> 480\n",
            "0.0006149327964521945 : Loss for Batch-> 481\n",
            "0.006160068325698376 : Val Loss after training 481 batche(s)\n",
            "0.001740363659337163 : Loss for Batch-> 482\n",
            "0.003915791399776936 : Loss for Batch-> 483\n",
            "0.025594620034098625 : Loss for Batch-> 484\n",
            "0.004838308319449425 : Loss for Batch-> 485\n",
            "0.0033309501595795155 : Loss for Batch-> 486\n",
            "0.0034501226618885994 : Loss for Batch-> 487\n",
            "0.0036166191566735506 : Loss for Batch-> 488\n",
            "0.003509253030642867 : Loss for Batch-> 489\n",
            "0.003469795221462846 : Loss for Batch-> 490\n",
            "0.0034981558565050364 : Loss for Batch-> 491\n",
            "0.006212244741618633 : Val Loss after training 491 batche(s)\n",
            "0.0031841702293604612 : Loss for Batch-> 492\n",
            "0.002081828424707055 : Loss for Batch-> 493\n",
            "0.0020587346516549587 : Loss for Batch-> 494\n",
            "0.0019800052978098392 : Loss for Batch-> 495\n",
            "0.45632046461105347 : Loss for Batch-> 496\n",
            "1.3048831224441528 : Loss for Batch-> 497\n",
            "0.008574952371418476 : Loss for Batch-> 498\n",
            "0.015942955389618874 : Loss for Batch-> 499\n",
            "0.000947107095271349 : Loss for Batch-> 500\n",
            "0.0011208096984773874 : Loss for Batch-> 501\n",
            "0.0035175078082829714 : Val Loss after training 501 batche(s)\n",
            "0.0019697307143360376 : Loss for Batch-> 502\n",
            "0.0020926217548549175 : Loss for Batch-> 503\n",
            "0.005052903667092323 : Loss for Batch-> 504\n",
            "0.050473652780056 : Loss for Batch-> 505\n",
            "0.0058558364398777485 : Loss for Batch-> 506\n",
            "0.015595097094774246 : Loss for Batch-> 507\n",
            "0.02012246660888195 : Loss for Batch-> 508\n",
            "0.019371397793293 : Loss for Batch-> 509\n",
            "0.018965991213917732 : Loss for Batch-> 510\n",
            "29 Epoch\n",
            "0.01139562577009201 : Loss for Batch-> 1\n",
            "0.0017388520063832402 : Val Loss after training 1 batche(s)\n",
            "0.009051780216395855 : Loss for Batch-> 2\n",
            "0.008910703472793102 : Loss for Batch-> 3\n",
            "0.008892708458006382 : Loss for Batch-> 4\n",
            "0.008717392571270466 : Loss for Batch-> 5\n",
            "0.008535273373126984 : Loss for Batch-> 6\n",
            "0.007948057726025581 : Loss for Batch-> 7\n",
            "0.007876413874328136 : Loss for Batch-> 8\n",
            "0.007824069820344448 : Loss for Batch-> 9\n",
            "0.015450162813067436 : Loss for Batch-> 10\n",
            "0.017709052190184593 : Loss for Batch-> 11\n",
            "0.001624697120860219 : Val Loss after training 11 batche(s)\n",
            "0.01875186152756214 : Loss for Batch-> 12\n",
            "0.01737026311457157 : Loss for Batch-> 13\n",
            "3.4938008785247803 : Loss for Batch-> 14\n",
            "2.707965612411499 : Loss for Batch-> 15\n",
            "0.0015546611975878477 : Loss for Batch-> 16\n",
            "0.0011571466457098722 : Loss for Batch-> 17\n",
            "0.0016997121274471283 : Loss for Batch-> 18\n",
            "0.0015426750760525465 : Loss for Batch-> 19\n",
            "0.012274517677724361 : Loss for Batch-> 20\n",
            "0.0035187804605811834 : Loss for Batch-> 21\n",
            "0.0012298646615818143 : Val Loss after training 21 batche(s)\n",
            "0.0009225348476320505 : Loss for Batch-> 22\n",
            "0.020647384226322174 : Loss for Batch-> 23\n",
            "0.020359594374895096 : Loss for Batch-> 24\n",
            "0.021281668916344643 : Loss for Batch-> 25\n",
            "0.018294980749487877 : Loss for Batch-> 26\n",
            "0.018164632841944695 : Loss for Batch-> 27\n",
            "0.005958809982985258 : Loss for Batch-> 28\n",
            "0.0006355398800224066 : Loss for Batch-> 29\n",
            "0.00032644919701851904 : Loss for Batch-> 30\n",
            "0.0028909884858876467 : Loss for Batch-> 31\n",
            "0.0007689952617511153 : Val Loss after training 31 batche(s)\n",
            "0.003112096805125475 : Loss for Batch-> 32\n",
            "0.0020351321436464787 : Loss for Batch-> 33\n",
            "0.0007342151366174221 : Loss for Batch-> 34\n",
            "5.9007114032283425e-05 : Loss for Batch-> 35\n",
            "4.223194991936907e-05 : Loss for Batch-> 36\n",
            "2.025811409112066e-05 : Loss for Batch-> 37\n",
            "0.000881951826158911 : Loss for Batch-> 38\n",
            "0.0003895516274496913 : Loss for Batch-> 39\n",
            "0.0011614010436460376 : Loss for Batch-> 40\n",
            "0.014045175164937973 : Loss for Batch-> 41\n",
            "0.017813775688409805 : Val Loss after training 41 batche(s)\n",
            "0.0003469734510872513 : Loss for Batch-> 42\n",
            "0.00026943476404994726 : Loss for Batch-> 43\n",
            "0.0008743300568312407 : Loss for Batch-> 44\n",
            "0.0003366153687238693 : Loss for Batch-> 45\n",
            "0.013333377428352833 : Loss for Batch-> 46\n",
            "0.0709216520190239 : Loss for Batch-> 47\n",
            "0.06647167354822159 : Loss for Batch-> 48\n",
            "0.05451171472668648 : Loss for Batch-> 49\n",
            "0.0037173933815211058 : Loss for Batch-> 50\n",
            "0.09798658639192581 : Loss for Batch-> 51\n",
            "0.018845288082957268 : Val Loss after training 51 batche(s)\n",
            "0.24720868468284607 : Loss for Batch-> 52\n",
            "0.24656546115875244 : Loss for Batch-> 53\n",
            "0.03252290189266205 : Loss for Batch-> 54\n",
            "0.00119089859072119 : Loss for Batch-> 55\n",
            "0.0018133408157154918 : Loss for Batch-> 56\n",
            "0.018578166142106056 : Loss for Batch-> 57\n",
            "0.018082208931446075 : Loss for Batch-> 58\n",
            "0.0072059291414916515 : Loss for Batch-> 59\n",
            "0.0007348230574280024 : Loss for Batch-> 60\n",
            "0.007050119806081057 : Loss for Batch-> 61\n",
            "0.006167128682136536 : Val Loss after training 61 batche(s)\n",
            "0.037160724401474 : Loss for Batch-> 62\n",
            "0.2275441288948059 : Loss for Batch-> 63\n",
            "0.23324140906333923 : Loss for Batch-> 64\n",
            "0.06773512065410614 : Loss for Batch-> 65\n",
            "0.07990970462560654 : Loss for Batch-> 66\n",
            "0.0703645870089531 : Loss for Batch-> 67\n",
            "0.04746240749955177 : Loss for Batch-> 68\n",
            "0.10739506036043167 : Loss for Batch-> 69\n",
            "0.07210443168878555 : Loss for Batch-> 70\n",
            "0.052785273641347885 : Loss for Batch-> 71\n",
            "0.0028029857203364372 : Val Loss after training 71 batche(s)\n",
            "0.011587888933718204 : Loss for Batch-> 72\n",
            "0.01191655918955803 : Loss for Batch-> 73\n",
            "0.010793638415634632 : Loss for Batch-> 74\n",
            "0.010505425743758678 : Loss for Batch-> 75\n",
            "0.010916143655776978 : Loss for Batch-> 76\n",
            "0.016609998419880867 : Loss for Batch-> 77\n",
            "0.0455799400806427 : Loss for Batch-> 78\n",
            "2.5063326358795166 : Loss for Batch-> 79\n",
            "0.03862898796796799 : Loss for Batch-> 80\n",
            "0.08661570399999619 : Loss for Batch-> 81\n",
            "0.018928393721580505 : Val Loss after training 81 batche(s)\n",
            "0.08999460935592651 : Loss for Batch-> 82\n",
            "0.0435439869761467 : Loss for Batch-> 83\n",
            "0.0006285158451646566 : Loss for Batch-> 84\n",
            "0.01337609626352787 : Loss for Batch-> 85\n",
            "0.08446592092514038 : Loss for Batch-> 86\n",
            "0.08092691749334335 : Loss for Batch-> 87\n",
            "0.09357021003961563 : Loss for Batch-> 88\n",
            "0.03482597693800926 : Loss for Batch-> 89\n",
            "0.01677214913070202 : Loss for Batch-> 90\n",
            "0.028579754754900932 : Loss for Batch-> 91\n",
            "0.0015979327727109194 : Val Loss after training 91 batche(s)\n",
            "0.04543372616171837 : Loss for Batch-> 92\n",
            "0.0031324801966547966 : Loss for Batch-> 93\n",
            "0.02320137433707714 : Loss for Batch-> 94\n",
            "0.0412747897207737 : Loss for Batch-> 95\n",
            "0.018180571496486664 : Loss for Batch-> 96\n",
            "0.010096229612827301 : Loss for Batch-> 97\n",
            "0.010351440869271755 : Loss for Batch-> 98\n",
            "0.0015358371892943978 : Loss for Batch-> 99\n",
            "0.0008096668170765042 : Loss for Batch-> 100\n",
            "0.003700376721099019 : Loss for Batch-> 101\n",
            "0.0010137660428881645 : Val Loss after training 101 batche(s)\n",
            "0.005363307427614927 : Loss for Batch-> 102\n",
            "0.002594907768070698 : Loss for Batch-> 103\n",
            "0.002312934258952737 : Loss for Batch-> 104\n",
            "0.0021133276168257 : Loss for Batch-> 105\n",
            "0.021101508289575577 : Loss for Batch-> 106\n",
            "0.016996510326862335 : Loss for Batch-> 107\n",
            "0.015530476346611977 : Loss for Batch-> 108\n",
            "0.014565258286893368 : Loss for Batch-> 109\n",
            "0.0010968003189191222 : Loss for Batch-> 110\n",
            "0.004309433046728373 : Loss for Batch-> 111\n",
            "0.002919652732089162 : Val Loss after training 111 batche(s)\n",
            "0.03078891523182392 : Loss for Batch-> 112\n",
            "0.00606694258749485 : Loss for Batch-> 113\n",
            "0.005408548749983311 : Loss for Batch-> 114\n",
            "0.0010772093664854765 : Loss for Batch-> 115\n",
            "0.0249153021723032 : Loss for Batch-> 116\n",
            "0.0385085865855217 : Loss for Batch-> 117\n",
            "0.055405955761671066 : Loss for Batch-> 118\n",
            "0.0120961619541049 : Loss for Batch-> 119\n",
            "0.037376075983047485 : Loss for Batch-> 120\n",
            "0.04532516002655029 : Loss for Batch-> 121\n",
            "0.0030439323745667934 : Val Loss after training 121 batche(s)\n",
            "0.0015373267233371735 : Loss for Batch-> 122\n",
            "0.015392330475151539 : Loss for Batch-> 123\n",
            "0.011717472225427628 : Loss for Batch-> 124\n",
            "0.1057668998837471 : Loss for Batch-> 125\n",
            "0.07058850675821304 : Loss for Batch-> 126\n",
            "0.012779262848198414 : Loss for Batch-> 127\n",
            "0.0230655949562788 : Loss for Batch-> 128\n",
            "0.017442958429455757 : Loss for Batch-> 129\n",
            "0.003009398467838764 : Loss for Batch-> 130\n",
            "0.015992267057299614 : Loss for Batch-> 131\n",
            "0.0030324887484312057 : Val Loss after training 131 batche(s)\n",
            "0.03136473521590233 : Loss for Batch-> 132\n",
            "0.14023207128047943 : Loss for Batch-> 133\n",
            "0.012716344557702541 : Loss for Batch-> 134\n",
            "0.051812559366226196 : Loss for Batch-> 135\n",
            "0.01283723209053278 : Loss for Batch-> 136\n",
            "0.05160956084728241 : Loss for Batch-> 137\n",
            "0.06574974954128265 : Loss for Batch-> 138\n",
            "0.24512358009815216 : Loss for Batch-> 139\n",
            "0.016282785683870316 : Loss for Batch-> 140\n",
            "0.024813394993543625 : Loss for Batch-> 141\n",
            "0.0006624601664952934 : Val Loss after training 141 batche(s)\n",
            "0.0005104740848764777 : Loss for Batch-> 142\n",
            "0.0007780874730087817 : Loss for Batch-> 143\n",
            "0.0006389689515344799 : Loss for Batch-> 144\n",
            "0.006757425144314766 : Loss for Batch-> 145\n",
            "0.012714467011392117 : Loss for Batch-> 146\n",
            "0.016619961708784103 : Loss for Batch-> 147\n",
            "0.011138707399368286 : Loss for Batch-> 148\n",
            "0.007281143218278885 : Loss for Batch-> 149\n",
            "0.0018317404901608825 : Loss for Batch-> 150\n",
            "0.002472145948559046 : Loss for Batch-> 151\n",
            "0.0016953616868704557 : Val Loss after training 151 batche(s)\n",
            "0.00830728467553854 : Loss for Batch-> 152\n",
            "0.05896851047873497 : Loss for Batch-> 153\n",
            "0.04722554236650467 : Loss for Batch-> 154\n",
            "0.004298807121813297 : Loss for Batch-> 155\n",
            "0.043745703995227814 : Loss for Batch-> 156\n",
            "0.035785868763923645 : Loss for Batch-> 157\n",
            "0.012843422591686249 : Loss for Batch-> 158\n",
            "0.023789340630173683 : Loss for Batch-> 159\n",
            "0.04168763756752014 : Loss for Batch-> 160\n",
            "0.004650743678212166 : Loss for Batch-> 161\n",
            "0.003439938649535179 : Val Loss after training 161 batche(s)\n",
            "0.007290928158909082 : Loss for Batch-> 162\n",
            "0.03875026851892471 : Loss for Batch-> 163\n",
            "0.0692683607339859 : Loss for Batch-> 164\n",
            "0.022999688982963562 : Loss for Batch-> 165\n",
            "0.02136842906475067 : Loss for Batch-> 166\n",
            "0.046519991010427475 : Loss for Batch-> 167\n",
            "0.05184401944279671 : Loss for Batch-> 168\n",
            "0.04037492349743843 : Loss for Batch-> 169\n",
            "0.00890776515007019 : Loss for Batch-> 170\n",
            "0.03269721567630768 : Loss for Batch-> 171\n",
            "0.0051439665257930756 : Val Loss after training 171 batche(s)\n",
            "0.0011334755690768361 : Loss for Batch-> 172\n",
            "0.00035168108297511935 : Loss for Batch-> 173\n",
            "0.0013199939858168364 : Loss for Batch-> 174\n",
            "0.0010312024969607592 : Loss for Batch-> 175\n",
            "0.0003152491117361933 : Loss for Batch-> 176\n",
            "0.0005624749464914203 : Loss for Batch-> 177\n",
            "0.007519637234508991 : Loss for Batch-> 178\n",
            "0.022614598274230957 : Loss for Batch-> 179\n",
            "0.002660485915839672 : Loss for Batch-> 180\n",
            "0.0004422325873747468 : Loss for Batch-> 181\n",
            "0.005736192688345909 : Val Loss after training 181 batche(s)\n",
            "0.0003562620549928397 : Loss for Batch-> 182\n",
            "3.2142215786734596e-05 : Loss for Batch-> 183\n",
            "5.916239842918003e-06 : Loss for Batch-> 184\n",
            "0.0004957367200404406 : Loss for Batch-> 185\n",
            "0.0002718623145483434 : Loss for Batch-> 186\n",
            "0.0003324362332932651 : Loss for Batch-> 187\n",
            "0.000780076312366873 : Loss for Batch-> 188\n",
            "0.0002563321031630039 : Loss for Batch-> 189\n",
            "0.0005346565158106387 : Loss for Batch-> 190\n",
            "0.0005860714009031653 : Loss for Batch-> 191\n",
            "0.005039024166762829 : Val Loss after training 191 batche(s)\n",
            "0.007182755041867495 : Loss for Batch-> 192\n",
            "0.018492266535758972 : Loss for Batch-> 193\n",
            "0.02170369401574135 : Loss for Batch-> 194\n",
            "0.006065521389245987 : Loss for Batch-> 195\n",
            "0.0006580466288141906 : Loss for Batch-> 196\n",
            "0.000729513936676085 : Loss for Batch-> 197\n",
            "0.00019760560826398432 : Loss for Batch-> 198\n",
            "0.0017448568250983953 : Loss for Batch-> 199\n",
            "3.0390868186950684 : Loss for Batch-> 200\n",
            "0.09287545084953308 : Loss for Batch-> 201\n",
            "0.0029942255932837725 : Val Loss after training 201 batche(s)\n",
            "0.033173415809869766 : Loss for Batch-> 202\n",
            "0.05515013262629509 : Loss for Batch-> 203\n",
            "0.055865708738565445 : Loss for Batch-> 204\n",
            "0.015938572585582733 : Loss for Batch-> 205\n",
            "0.01872754655778408 : Loss for Batch-> 206\n",
            "0.011262957006692886 : Loss for Batch-> 207\n",
            "0.009562297724187374 : Loss for Batch-> 208\n",
            "0.006235263776034117 : Loss for Batch-> 209\n",
            "0.005914413835853338 : Loss for Batch-> 210\n",
            "0.011060137301683426 : Loss for Batch-> 211\n",
            "0.0005764490342698991 : Val Loss after training 211 batche(s)\n",
            "0.04161383584141731 : Loss for Batch-> 212\n",
            "0.0017003986285999417 : Loss for Batch-> 213\n",
            "0.027706975117325783 : Loss for Batch-> 214\n",
            "0.029090307652950287 : Loss for Batch-> 215\n",
            "0.011499196290969849 : Loss for Batch-> 216\n",
            "0.05055398866534233 : Loss for Batch-> 217\n",
            "0.00031062992638908327 : Loss for Batch-> 218\n",
            "0.0005628838553093374 : Loss for Batch-> 219\n",
            "0.0013156592613086104 : Loss for Batch-> 220\n",
            "0.061648715287446976 : Loss for Batch-> 221\n",
            "0.04524527117609978 : Val Loss after training 221 batche(s)\n",
            "0.0516766756772995 : Loss for Batch-> 222\n",
            "0.006754881702363491 : Loss for Batch-> 223\n",
            "0.16647456586360931 : Loss for Batch-> 224\n",
            "0.008871674537658691 : Loss for Batch-> 225\n",
            "0.0003891022934112698 : Loss for Batch-> 226\n",
            "0.0004325378977227956 : Loss for Batch-> 227\n",
            "0.00043127508251927793 : Loss for Batch-> 228\n",
            "0.675535261631012 : Loss for Batch-> 229\n",
            "1.788861870765686 : Loss for Batch-> 230\n",
            "5.377125263214111 : Loss for Batch-> 231\n",
            "0.04088388383388519 : Val Loss after training 231 batche(s)\n",
            "6.1527791023254395 : Loss for Batch-> 232\n",
            "0.13065524399280548 : Loss for Batch-> 233\n",
            "0.19616909325122833 : Loss for Batch-> 234\n",
            "0.008390283212065697 : Loss for Batch-> 235\n",
            "0.027673933655023575 : Loss for Batch-> 236\n",
            "0.041022635996341705 : Loss for Batch-> 237\n",
            "0.00217644264921546 : Loss for Batch-> 238\n",
            "0.0012168316170573235 : Loss for Batch-> 239\n",
            "0.0016951209399849176 : Loss for Batch-> 240\n",
            "0.09099750220775604 : Loss for Batch-> 241\n",
            "0.0003791371127590537 : Val Loss after training 241 batche(s)\n",
            "0.0967145562171936 : Loss for Batch-> 242\n",
            "0.1363024115562439 : Loss for Batch-> 243\n",
            "0.35775870084762573 : Loss for Batch-> 244\n",
            "0.7136126160621643 : Loss for Batch-> 245\n",
            "4.751981735229492 : Loss for Batch-> 246\n",
            "0.009675770998001099 : Loss for Batch-> 247\n",
            "0.028571680188179016 : Loss for Batch-> 248\n",
            "2.934217929840088 : Loss for Batch-> 249\n",
            "1.4942800998687744 : Loss for Batch-> 250\n",
            "0.8998715281486511 : Loss for Batch-> 251\n",
            "0.00028418080182746053 : Val Loss after training 251 batche(s)\n",
            "0.3317457139492035 : Loss for Batch-> 252\n",
            "0.05602816492319107 : Loss for Batch-> 253\n",
            "0.3853701055049896 : Loss for Batch-> 254\n",
            "0.5118734240531921 : Loss for Batch-> 255\n",
            "0.5580321550369263 : Loss for Batch-> 256\n",
            "4.025185585021973 : Loss for Batch-> 257\n",
            "5.175483226776123 : Loss for Batch-> 258\n",
            "0.003150244941934943 : Loss for Batch-> 259\n",
            "0.2611044645309448 : Loss for Batch-> 260\n",
            "0.421151340007782 : Loss for Batch-> 261\n",
            "0.006843962240964174 : Val Loss after training 261 batche(s)\n",
            "4.345717430114746 : Loss for Batch-> 262\n",
            "1.6764096021652222 : Loss for Batch-> 263\n",
            "0.4551999270915985 : Loss for Batch-> 264\n",
            "0.0015494206454604864 : Loss for Batch-> 265\n",
            "0.8254141211509705 : Loss for Batch-> 266\n",
            "1.3016695976257324 : Loss for Batch-> 267\n",
            "0.11202619224786758 : Loss for Batch-> 268\n",
            "0.038006674498319626 : Loss for Batch-> 269\n",
            "0.002398647367954254 : Loss for Batch-> 270\n",
            "3.6578328609466553 : Loss for Batch-> 271\n",
            "0.035649195313453674 : Val Loss after training 271 batche(s)\n",
            "2.3321328163146973 : Loss for Batch-> 272\n",
            "0.007552644237875938 : Loss for Batch-> 273\n",
            "0.00038370201946236193 : Loss for Batch-> 274\n",
            "0.0020361405331641436 : Loss for Batch-> 275\n",
            "0.0004887679824605584 : Loss for Batch-> 276\n",
            "0.0022238127421587706 : Loss for Batch-> 277\n",
            "0.003589517204090953 : Loss for Batch-> 278\n",
            "0.0006937708822079003 : Loss for Batch-> 279\n",
            "0.012819379568099976 : Loss for Batch-> 280\n",
            "0.008014150895178318 : Loss for Batch-> 281\n",
            "0.05418861284852028 : Val Loss after training 281 batche(s)\n",
            "0.041936494410037994 : Loss for Batch-> 282\n",
            "0.06599169969558716 : Loss for Batch-> 283\n",
            "0.06504780799150467 : Loss for Batch-> 284\n",
            "0.06489954888820648 : Loss for Batch-> 285\n",
            "0.02543497085571289 : Loss for Batch-> 286\n",
            "0.1920820027589798 : Loss for Batch-> 287\n",
            "11.803370475769043 : Loss for Batch-> 288\n",
            "2.389890670776367 : Loss for Batch-> 289\n",
            "0.01163127738982439 : Loss for Batch-> 290\n",
            "0.0017005379777401686 : Loss for Batch-> 291\n",
            "0.00834472756832838 : Val Loss after training 291 batche(s)\n",
            "0.06692533195018768 : Loss for Batch-> 292\n",
            "0.1138802021741867 : Loss for Batch-> 293\n",
            "0.0049380948767066 : Loss for Batch-> 294\n",
            "0.0003305270802229643 : Loss for Batch-> 295\n",
            "0.0016643746057525277 : Loss for Batch-> 296\n",
            "0.01690940372645855 : Loss for Batch-> 297\n",
            "0.05420159175992012 : Loss for Batch-> 298\n",
            "0.0017357569886371493 : Loss for Batch-> 299\n",
            "0.04010317847132683 : Loss for Batch-> 300\n",
            "0.017272843047976494 : Loss for Batch-> 301\n",
            "0.041409894824028015 : Val Loss after training 301 batche(s)\n",
            "1.0178166627883911 : Loss for Batch-> 302\n",
            "1.0179741382598877 : Loss for Batch-> 303\n",
            "0.6298153400421143 : Loss for Batch-> 304\n",
            "0.09070388972759247 : Loss for Batch-> 305\n",
            "0.010840360075235367 : Loss for Batch-> 306\n",
            "1.2161693572998047 : Loss for Batch-> 307\n",
            "1.800502896308899 : Loss for Batch-> 308\n",
            "0.9422466158866882 : Loss for Batch-> 309\n",
            "0.05328861251473427 : Loss for Batch-> 310\n",
            "0.037340037524700165 : Loss for Batch-> 311\n",
            "0.03733391687273979 : Val Loss after training 311 batche(s)\n",
            "0.321662038564682 : Loss for Batch-> 312\n",
            "1.65604829788208 : Loss for Batch-> 313\n",
            "1.0199795961380005 : Loss for Batch-> 314\n",
            "0.012793997302651405 : Loss for Batch-> 315\n",
            "0.010560702532529831 : Loss for Batch-> 316\n",
            "0.08631210029125214 : Loss for Batch-> 317\n",
            "0.5476741194725037 : Loss for Batch-> 318\n",
            "0.017930472269654274 : Loss for Batch-> 319\n",
            "0.02525676228106022 : Loss for Batch-> 320\n",
            "0.14907671511173248 : Loss for Batch-> 321\n",
            "0.010498764924705029 : Val Loss after training 321 batche(s)\n",
            "0.18304277956485748 : Loss for Batch-> 322\n",
            "0.17391924560070038 : Loss for Batch-> 323\n",
            "0.32418984174728394 : Loss for Batch-> 324\n",
            "0.04875186085700989 : Loss for Batch-> 325\n",
            "0.0740480050444603 : Loss for Batch-> 326\n",
            "0.2551795244216919 : Loss for Batch-> 327\n",
            "0.3198736906051636 : Loss for Batch-> 328\n",
            "0.47752800583839417 : Loss for Batch-> 329\n",
            "0.2998499870300293 : Loss for Batch-> 330\n",
            "0.05612720176577568 : Loss for Batch-> 331\n",
            "0.00844409130513668 : Val Loss after training 331 batche(s)\n",
            "0.08187494426965714 : Loss for Batch-> 332\n",
            "0.04789191856980324 : Loss for Batch-> 333\n",
            "0.008615216240286827 : Loss for Batch-> 334\n",
            "0.009090335108339787 : Loss for Batch-> 335\n",
            "0.00020333396969363093 : Loss for Batch-> 336\n",
            "0.0005852406029589474 : Loss for Batch-> 337\n",
            "0.0012585469521582127 : Loss for Batch-> 338\n",
            "0.0027467627078294754 : Loss for Batch-> 339\n",
            "0.00575575465336442 : Loss for Batch-> 340\n",
            "0.0623980388045311 : Loss for Batch-> 341\n",
            "0.008274015970528126 : Val Loss after training 341 batche(s)\n",
            "0.06092943251132965 : Loss for Batch-> 342\n",
            "0.05203017592430115 : Loss for Batch-> 343\n",
            "0.13848939538002014 : Loss for Batch-> 344\n",
            "0.4504851996898651 : Loss for Batch-> 345\n",
            "0.003349060658365488 : Loss for Batch-> 346\n",
            "0.016086848452687263 : Loss for Batch-> 347\n",
            "0.12185533344745636 : Loss for Batch-> 348\n",
            "0.35874637961387634 : Loss for Batch-> 349\n",
            "0.29544878005981445 : Loss for Batch-> 350\n",
            "0.003238614182919264 : Loss for Batch-> 351\n",
            "0.008075440302491188 : Val Loss after training 351 batche(s)\n",
            "0.40002501010894775 : Loss for Batch-> 352\n",
            "0.891975462436676 : Loss for Batch-> 353\n",
            "0.20772972702980042 : Loss for Batch-> 354\n",
            "0.03207113593816757 : Loss for Batch-> 355\n",
            "0.6070945858955383 : Loss for Batch-> 356\n",
            "1.3128516674041748 : Loss for Batch-> 357\n",
            "0.002283592475578189 : Loss for Batch-> 358\n",
            "0.01040647178888321 : Loss for Batch-> 359\n",
            "0.0031905171927064657 : Loss for Batch-> 360\n",
            "0.4993727505207062 : Loss for Batch-> 361\n",
            "0.0060296268202364445 : Val Loss after training 361 batche(s)\n",
            "0.09016596525907516 : Loss for Batch-> 362\n",
            "0.13186776638031006 : Loss for Batch-> 363\n",
            "0.12153009325265884 : Loss for Batch-> 364\n",
            "0.08497834205627441 : Loss for Batch-> 365\n",
            "0.20241963863372803 : Loss for Batch-> 366\n",
            "0.3462027609348297 : Loss for Batch-> 367\n",
            "0.10345397144556046 : Loss for Batch-> 368\n",
            "0.27855685353279114 : Loss for Batch-> 369\n",
            "0.030593009665608406 : Loss for Batch-> 370\n",
            "0.006220144685357809 : Loss for Batch-> 371\n",
            "0.017666559666395187 : Val Loss after training 371 batche(s)\n",
            "0.062236238270998 : Loss for Batch-> 372\n",
            "0.051698680967092514 : Loss for Batch-> 373\n",
            "0.7494406700134277 : Loss for Batch-> 374\n",
            "0.21078166365623474 : Loss for Batch-> 375\n",
            "0.7137978076934814 : Loss for Batch-> 376\n",
            "1.115615725517273 : Loss for Batch-> 377\n",
            "0.21043716371059418 : Loss for Batch-> 378\n",
            "0.5320059061050415 : Loss for Batch-> 379\n",
            "0.1467220038175583 : Loss for Batch-> 380\n",
            "0.013236451894044876 : Loss for Batch-> 381\n",
            "0.01795414462685585 : Val Loss after training 381 batche(s)\n",
            "0.03912825882434845 : Loss for Batch-> 382\n",
            "0.17146718502044678 : Loss for Batch-> 383\n",
            "0.25590425729751587 : Loss for Batch-> 384\n",
            "0.7041983604431152 : Loss for Batch-> 385\n",
            "1.0345275402069092 : Loss for Batch-> 386\n",
            "1.5349938869476318 : Loss for Batch-> 387\n",
            "0.7688983082771301 : Loss for Batch-> 388\n",
            "0.35490188002586365 : Loss for Batch-> 389\n",
            "0.0027013535145670176 : Loss for Batch-> 390\n",
            "0.042145486921072006 : Loss for Batch-> 391\n",
            "0.004453424364328384 : Val Loss after training 391 batche(s)\n",
            "0.8355247378349304 : Loss for Batch-> 392\n",
            "1.3782821893692017 : Loss for Batch-> 393\n",
            "0.4734489917755127 : Loss for Batch-> 394\n",
            "0.06410985440015793 : Loss for Batch-> 395\n",
            "0.062068309634923935 : Loss for Batch-> 396\n",
            "0.029240652918815613 : Loss for Batch-> 397\n",
            "0.3898589313030243 : Loss for Batch-> 398\n",
            "0.13508504629135132 : Loss for Batch-> 399\n",
            "0.9212637543678284 : Loss for Batch-> 400\n",
            "0.6320046782493591 : Loss for Batch-> 401\n",
            "0.0005333020817488432 : Val Loss after training 401 batche(s)\n",
            "0.008438779041171074 : Loss for Batch-> 402\n",
            "0.007400361821055412 : Loss for Batch-> 403\n",
            "0.01666674204170704 : Loss for Batch-> 404\n",
            "0.014004670083522797 : Loss for Batch-> 405\n",
            "0.31712639331817627 : Loss for Batch-> 406\n",
            "0.5610029697418213 : Loss for Batch-> 407\n",
            "0.2531448304653168 : Loss for Batch-> 408\n",
            "0.1007821187376976 : Loss for Batch-> 409\n",
            "0.04840327799320221 : Loss for Batch-> 410\n",
            "0.0073252166621387005 : Loss for Batch-> 411\n",
            "0.013182749971747398 : Val Loss after training 411 batche(s)\n",
            "0.06866869330406189 : Loss for Batch-> 412\n",
            "0.6879622936248779 : Loss for Batch-> 413\n",
            "1.1836097240447998 : Loss for Batch-> 414\n",
            "0.18058151006698608 : Loss for Batch-> 415\n",
            "0.143413707613945 : Loss for Batch-> 416\n",
            "0.0013810390373691916 : Loss for Batch-> 417\n",
            "0.0015587919624522328 : Loss for Batch-> 418\n",
            "0.0017528554890304804 : Loss for Batch-> 419\n",
            "0.020124901086091995 : Loss for Batch-> 420\n",
            "0.029732108116149902 : Loss for Batch-> 421\n",
            "0.08479442447423935 : Val Loss after training 421 batche(s)\n",
            "0.001745977671816945 : Loss for Batch-> 422\n",
            "0.18301723897457123 : Loss for Batch-> 423\n",
            "0.08107282966375351 : Loss for Batch-> 424\n",
            "0.02743365801870823 : Loss for Batch-> 425\n",
            "0.022984104230999947 : Loss for Batch-> 426\n",
            "0.02202538773417473 : Loss for Batch-> 427\n",
            "0.02267691120505333 : Loss for Batch-> 428\n",
            "0.023121917620301247 : Loss for Batch-> 429\n",
            "0.02418743073940277 : Loss for Batch-> 430\n",
            "0.02365935780107975 : Loss for Batch-> 431\n",
            "0.09048564732074738 : Val Loss after training 431 batche(s)\n",
            "0.02325834520161152 : Loss for Batch-> 432\n",
            "0.022543394938111305 : Loss for Batch-> 433\n",
            "0.055584877729415894 : Loss for Batch-> 434\n",
            "0.1152941957116127 : Loss for Batch-> 435\n",
            "0.024045128375291824 : Loss for Batch-> 436\n",
            "0.004426003433763981 : Loss for Batch-> 437\n",
            "0.0012699178187176585 : Loss for Batch-> 438\n",
            "0.0012421434512361884 : Loss for Batch-> 439\n",
            "0.002522681839764118 : Loss for Batch-> 440\n",
            "0.001209518639370799 : Loss for Batch-> 441\n",
            "0.02688649483025074 : Val Loss after training 441 batche(s)\n",
            "0.0415586456656456 : Loss for Batch-> 442\n",
            "0.062153205275535583 : Loss for Batch-> 443\n",
            "0.016344275325536728 : Loss for Batch-> 444\n",
            "0.013644469901919365 : Loss for Batch-> 445\n",
            "0.023555554449558258 : Loss for Batch-> 446\n",
            "0.04195743426680565 : Loss for Batch-> 447\n",
            "0.012849463149905205 : Loss for Batch-> 448\n",
            "0.13330374658107758 : Loss for Batch-> 449\n",
            "0.33289361000061035 : Loss for Batch-> 450\n",
            "0.6065815687179565 : Loss for Batch-> 451\n",
            "0.07128848880529404 : Val Loss after training 451 batche(s)\n",
            "0.006422966253012419 : Loss for Batch-> 452\n",
            "0.0015139833558350801 : Loss for Batch-> 453\n",
            "0.0005188502254895866 : Loss for Batch-> 454\n",
            "0.0008256701403297484 : Loss for Batch-> 455\n",
            "0.0006878192070871592 : Loss for Batch-> 456\n",
            "0.0007859689649194479 : Loss for Batch-> 457\n",
            "0.001182560808956623 : Loss for Batch-> 458\n",
            "0.0001274850801564753 : Loss for Batch-> 459\n",
            "0.0001341564056929201 : Loss for Batch-> 460\n",
            "0.00013304053572937846 : Loss for Batch-> 461\n",
            "0.06278782337903976 : Val Loss after training 461 batche(s)\n",
            "0.00013286522880662233 : Loss for Batch-> 462\n",
            "0.00013075560855213553 : Loss for Batch-> 463\n",
            "0.00018156517762690783 : Loss for Batch-> 464\n",
            "0.00013831559044774622 : Loss for Batch-> 465\n",
            "0.00013489233970176429 : Loss for Batch-> 466\n",
            "0.00013665176811628044 : Loss for Batch-> 467\n",
            "0.0006291895406320691 : Loss for Batch-> 468\n",
            "0.0014283217024058104 : Loss for Batch-> 469\n",
            "0.00038411509012803435 : Loss for Batch-> 470\n",
            "0.0005140979774296284 : Loss for Batch-> 471\n",
            "0.003655084641650319 : Val Loss after training 471 batche(s)\n",
            "0.001387062482535839 : Loss for Batch-> 472\n",
            "0.00026855934993363917 : Loss for Batch-> 473\n",
            "0.00035787749220617115 : Loss for Batch-> 474\n",
            "0.000744450488127768 : Loss for Batch-> 475\n",
            "0.0008251824765466154 : Loss for Batch-> 476\n",
            "0.00044009785051457584 : Loss for Batch-> 477\n",
            "0.0009231550502590835 : Loss for Batch-> 478\n",
            "0.0011648728977888823 : Loss for Batch-> 479\n",
            "0.0007662264397367835 : Loss for Batch-> 480\n",
            "0.00041110411984845996 : Loss for Batch-> 481\n",
            "0.02385210245847702 : Val Loss after training 481 batche(s)\n",
            "0.00043447193456813693 : Loss for Batch-> 482\n",
            "0.0019534146413207054 : Loss for Batch-> 483\n",
            "0.014388066716492176 : Loss for Batch-> 484\n",
            "0.015295010060071945 : Loss for Batch-> 485\n",
            "0.00565014174208045 : Loss for Batch-> 486\n",
            "0.003548270557075739 : Loss for Batch-> 487\n",
            "0.003509216010570526 : Loss for Batch-> 488\n",
            "0.003642098279669881 : Loss for Batch-> 489\n",
            "0.0034987377002835274 : Loss for Batch-> 490\n",
            "0.003458616090938449 : Loss for Batch-> 491\n",
            "0.005168762058019638 : Val Loss after training 491 batche(s)\n",
            "0.0036240979097783566 : Loss for Batch-> 492\n",
            "0.0025838769506663084 : Loss for Batch-> 493\n",
            "0.0020808144472539425 : Loss for Batch-> 494\n",
            "0.0020575588569045067 : Loss for Batch-> 495\n",
            "0.0019167590653523803 : Loss for Batch-> 496\n",
            "1.549150824546814 : Loss for Batch-> 497\n",
            "0.21316002309322357 : Loss for Batch-> 498\n",
            "0.01796352118253708 : Loss for Batch-> 499\n",
            "0.004587662406265736 : Loss for Batch-> 500\n",
            "0.0013436663430184126 : Loss for Batch-> 501\n",
            "0.019900821149349213 : Val Loss after training 501 batche(s)\n",
            "0.0010424199281260371 : Loss for Batch-> 502\n",
            "0.002013090532273054 : Loss for Batch-> 503\n",
            "0.0032360635232180357 : Loss for Batch-> 504\n",
            "0.007903745397925377 : Loss for Batch-> 505\n",
            "0.04980801045894623 : Loss for Batch-> 506\n",
            "0.0050167543813586235 : Loss for Batch-> 507\n",
            "0.02059127576649189 : Loss for Batch-> 508\n",
            "0.019862927496433258 : Loss for Batch-> 509\n",
            "0.019120439887046814 : Loss for Batch-> 510\n",
            "30 Epoch\n",
            "0.01834031753242016 : Loss for Batch-> 1\n",
            "0.011983249336481094 : Val Loss after training 1 batche(s)\n",
            "0.007741207256913185 : Loss for Batch-> 2\n",
            "0.008884046226739883 : Loss for Batch-> 3\n",
            "0.009047886356711388 : Loss for Batch-> 4\n",
            "0.008676617406308651 : Loss for Batch-> 5\n",
            "0.008553971536457539 : Loss for Batch-> 6\n",
            "0.008388764224946499 : Loss for Batch-> 7\n",
            "0.007856291718780994 : Loss for Batch-> 8\n",
            "0.007762487046420574 : Loss for Batch-> 9\n",
            "0.00937440525740385 : Loss for Batch-> 10\n",
            "0.017895054072141647 : Loss for Batch-> 11\n",
            "0.001567311235703528 : Val Loss after training 11 batche(s)\n",
            "0.01767384260892868 : Loss for Batch-> 12\n",
            "0.018860209733247757 : Loss for Batch-> 13\n",
            "0.18736526370048523 : Loss for Batch-> 14\n",
            "5.8676323890686035 : Loss for Batch-> 15\n",
            "0.157307967543602 : Loss for Batch-> 16\n",
            "0.0013612459879368544 : Loss for Batch-> 17\n",
            "0.0014868383295834064 : Loss for Batch-> 18\n",
            "0.0010830642422661185 : Loss for Batch-> 19\n",
            "0.0037739770486950874 : Loss for Batch-> 20\n",
            "0.013036787509918213 : Loss for Batch-> 21\n",
            "0.13090106844902039 : Val Loss after training 21 batche(s)\n",
            "0.0010617688531056046 : Loss for Batch-> 22\n",
            "0.0035393033176660538 : Loss for Batch-> 23\n",
            "0.024748079478740692 : Loss for Batch-> 24\n",
            "0.01995369978249073 : Loss for Batch-> 25\n",
            "0.018807651475071907 : Loss for Batch-> 26\n",
            "0.02088611200451851 : Loss for Batch-> 27\n",
            "0.016130436211824417 : Loss for Batch-> 28\n",
            "0.0009452827507629991 : Loss for Batch-> 29\n",
            "0.0003797135432250798 : Loss for Batch-> 30\n",
            "0.0003444257890805602 : Loss for Batch-> 31\n",
            "0.11681436747312546 : Val Loss after training 31 batche(s)\n",
            "0.0040795630775392056 : Loss for Batch-> 32\n",
            "0.002645309781655669 : Loss for Batch-> 33\n",
            "0.0016628856537863612 : Loss for Batch-> 34\n",
            "0.0001707327173789963 : Loss for Batch-> 35\n",
            "7.783603359712288e-05 : Loss for Batch-> 36\n",
            "2.589817449916154e-05 : Loss for Batch-> 37\n",
            "1.5552597687928937e-05 : Loss for Batch-> 38\n",
            "0.001222546910867095 : Loss for Batch-> 39\n",
            "0.0002653049596119672 : Loss for Batch-> 40\n",
            "0.008687030524015427 : Loss for Batch-> 41\n",
            "0.053798168897628784 : Val Loss after training 41 batche(s)\n",
            "0.006536728236824274 : Loss for Batch-> 42\n",
            "0.0001759106817189604 : Loss for Batch-> 43\n",
            "0.0005223933840170503 : Loss for Batch-> 44\n",
            "0.0006276662461459637 : Loss for Batch-> 45\n",
            "0.0004794170963577926 : Loss for Batch-> 46\n",
            "0.04495048522949219 : Loss for Batch-> 47\n",
            "0.06523176282644272 : Loss for Batch-> 48\n",
            "0.0703825056552887 : Loss for Batch-> 49\n",
            "0.0253874771296978 : Loss for Batch-> 50\n",
            "0.006375100463628769 : Loss for Batch-> 51\n",
            "0.11944358050823212 : Val Loss after training 51 batche(s)\n",
            "0.1880994439125061 : Loss for Batch-> 52\n",
            "0.2542143762111664 : Loss for Batch-> 53\n",
            "0.17668505012989044 : Loss for Batch-> 54\n",
            "0.0020323446951806545 : Loss for Batch-> 55\n",
            "0.000884458189830184 : Loss for Batch-> 56\n",
            "0.008102755062282085 : Loss for Batch-> 57\n",
            "0.017967822030186653 : Loss for Batch-> 58\n",
            "0.01664319820702076 : Loss for Batch-> 59\n",
            "0.0033136666752398014 : Loss for Batch-> 60\n",
            "0.0006213242304511368 : Loss for Batch-> 61\n",
            "0.03060160204768181 : Val Loss after training 61 batche(s)\n",
            "0.012465378269553185 : Loss for Batch-> 62\n",
            "0.11171389371156693 : Loss for Batch-> 63\n",
            "0.23299065232276917 : Loss for Batch-> 64\n",
            "0.20626209676265717 : Loss for Batch-> 65\n",
            "0.03779106214642525 : Loss for Batch-> 66\n",
            "0.09043864905834198 : Loss for Batch-> 67\n",
            "0.033399973064661026 : Loss for Batch-> 68\n",
            "0.1381228119134903 : Loss for Batch-> 69\n",
            "0.042862340807914734 : Loss for Batch-> 70\n",
            "0.08996754884719849 : Loss for Batch-> 71\n",
            "0.038234688341617584 : Val Loss after training 71 batche(s)\n",
            "0.010776286013424397 : Loss for Batch-> 72\n",
            "0.011708958074450493 : Loss for Batch-> 73\n",
            "0.011681552976369858 : Loss for Batch-> 74\n",
            "0.010373156517744064 : Loss for Batch-> 75\n",
            "0.01083914004266262 : Loss for Batch-> 76\n",
            "0.01070766244083643 : Loss for Batch-> 77\n",
            "0.024600619450211525 : Loss for Batch-> 78\n",
            "0.6926130652427673 : Loss for Batch-> 79\n",
            "1.8777284622192383 : Loss for Batch-> 80\n",
            "0.04234764724969864 : Loss for Batch-> 81\n",
            "0.05057825893163681 : Val Loss after training 81 batche(s)\n",
            "0.07847689837217331 : Loss for Batch-> 82\n",
            "0.093235082924366 : Loss for Batch-> 83\n",
            "0.014168559573590755 : Loss for Batch-> 84\n",
            "0.0016593612963333726 : Loss for Batch-> 85\n",
            "0.04472055286169052 : Loss for Batch-> 86\n",
            "0.08672928810119629 : Loss for Batch-> 87\n",
            "0.08694145083427429 : Loss for Batch-> 88\n",
            "0.07509012520313263 : Loss for Batch-> 89\n",
            "0.022749872878193855 : Loss for Batch-> 90\n",
            "0.011232055723667145 : Loss for Batch-> 91\n",
            "0.043158192187547684 : Val Loss after training 91 batche(s)\n",
            "0.042948585003614426 : Loss for Batch-> 92\n",
            "0.028611769899725914 : Loss for Batch-> 93\n",
            "0.0015967462677508593 : Loss for Batch-> 94\n",
            "0.044791772961616516 : Loss for Batch-> 95\n",
            "0.03623194620013237 : Loss for Batch-> 96\n",
            "0.0015271060401573777 : Loss for Batch-> 97\n",
            "0.019623450934886932 : Loss for Batch-> 98\n",
            "0.0007299759308807552 : Loss for Batch-> 99\n",
            "0.001708352123387158 : Loss for Batch-> 100\n",
            "0.0017956570954993367 : Loss for Batch-> 101\n",
            "0.040065065026283264 : Val Loss after training 101 batche(s)\n",
            "0.006748869549483061 : Loss for Batch-> 102\n",
            "0.001529222121462226 : Loss for Batch-> 103\n",
            "0.0031046310905367136 : Loss for Batch-> 104\n",
            "0.0020140735432505608 : Loss for Batch-> 105\n",
            "0.009748891927301884 : Loss for Batch-> 106\n",
            "0.01915828138589859 : Loss for Batch-> 107\n",
            "0.01893124170601368 : Loss for Batch-> 108\n",
            "0.01734115742146969 : Loss for Batch-> 109\n",
            "0.004512455314397812 : Loss for Batch-> 110\n",
            "0.0017508985474705696 : Loss for Batch-> 111\n",
            "0.03920523822307587 : Val Loss after training 111 batche(s)\n",
            "0.015423646196722984 : Loss for Batch-> 112\n",
            "0.023813974112272263 : Loss for Batch-> 113\n",
            "0.0027728471904993057 : Loss for Batch-> 114\n",
            "0.003937374334782362 : Loss for Batch-> 115\n",
            "0.0010805530473589897 : Loss for Batch-> 116\n",
            "0.04672258719801903 : Loss for Batch-> 117\n",
            "0.03588368743658066 : Loss for Batch-> 118\n",
            "0.046313270926475525 : Loss for Batch-> 119\n",
            "0.002131174085661769 : Loss for Batch-> 120\n",
            "0.06781312078237534 : Loss for Batch-> 121\n",
            "0.03799090534448624 : Val Loss after training 121 batche(s)\n",
            "0.014142774045467377 : Loss for Batch-> 122\n",
            "0.011077722534537315 : Loss for Batch-> 123\n",
            "0.012114438228309155 : Loss for Batch-> 124\n",
            "0.029154613614082336 : Loss for Batch-> 125\n",
            "0.10985228419303894 : Loss for Batch-> 126\n",
            "0.049278806895017624 : Loss for Batch-> 127\n",
            "0.009866960346698761 : Loss for Batch-> 128\n",
            "0.036037627607584 : Loss for Batch-> 129\n",
            "0.0026473465841263533 : Loss for Batch-> 130\n",
            "0.002457608003169298 : Loss for Batch-> 131\n",
            "0.02890351042151451 : Val Loss after training 131 batche(s)\n",
            "0.028341807425022125 : Loss for Batch-> 132\n",
            "0.03990190103650093 : Loss for Batch-> 133\n",
            "0.12805549800395966 : Loss for Batch-> 134\n",
            "0.01433993224054575 : Loss for Batch-> 135\n",
            "0.040660347789525986 : Loss for Batch-> 136\n",
            "0.0429963544011116 : Loss for Batch-> 137\n",
            "0.02544635348021984 : Loss for Batch-> 138\n",
            "0.20728878676891327 : Loss for Batch-> 139\n",
            "0.10902746021747589 : Loss for Batch-> 140\n",
            "0.030654361471533775 : Loss for Batch-> 141\n",
            "0.03422602266073227 : Val Loss after training 141 batche(s)\n",
            "0.000417420087615028 : Loss for Batch-> 142\n",
            "0.0007647352176718414 : Loss for Batch-> 143\n",
            "0.0005098330439068377 : Loss for Batch-> 144\n",
            "0.0011794472811743617 : Loss for Batch-> 145\n",
            "0.010608059354126453 : Loss for Batch-> 146\n",
            "0.017543187364935875 : Loss for Batch-> 147\n",
            "0.012030555866658688 : Loss for Batch-> 148\n",
            "0.011564628221094608 : Loss for Batch-> 149\n",
            "0.0037240281235426664 : Loss for Batch-> 150\n",
            "0.002033533062785864 : Loss for Batch-> 151\n",
            "0.040151700377464294 : Val Loss after training 151 batche(s)\n",
            "0.0014978934777900577 : Loss for Batch-> 152\n",
            "0.02665948122739792 : Loss for Batch-> 153\n",
            "0.06807555258274078 : Loss for Batch-> 154\n",
            "0.018918385729193687 : Loss for Batch-> 155\n",
            "0.01977992057800293 : Loss for Batch-> 156\n",
            "0.04476309195160866 : Loss for Batch-> 157\n",
            "0.03146272152662277 : Loss for Batch-> 158\n",
            "0.0011503811692819 : Loss for Batch-> 159\n",
            "0.05084843933582306 : Loss for Batch-> 160\n",
            "0.014192775823175907 : Loss for Batch-> 161\n",
            "0.025947868824005127 : Val Loss after training 161 batche(s)\n",
            "0.011479182168841362 : Loss for Batch-> 162\n",
            "0.00108873110730201 : Loss for Batch-> 163\n",
            "0.06373734027147293 : Loss for Batch-> 164\n",
            "0.05882183462381363 : Loss for Batch-> 165\n",
            "0.008479508571326733 : Loss for Batch-> 166\n",
            "0.03940229117870331 : Loss for Batch-> 167\n",
            "0.04678810015320778 : Loss for Batch-> 168\n",
            "0.04820379242300987 : Loss for Batch-> 169\n",
            "0.028724204748868942 : Loss for Batch-> 170\n",
            "0.021976938471198082 : Loss for Batch-> 171\n",
            "0.000495102081913501 : Val Loss after training 171 batche(s)\n",
            "0.016831640154123306 : Loss for Batch-> 172\n",
            "0.00032048928551375866 : Loss for Batch-> 173\n",
            "0.000979845761321485 : Loss for Batch-> 174\n",
            "0.001322246971540153 : Loss for Batch-> 175\n",
            "0.00037526394589804113 : Loss for Batch-> 176\n",
            "0.0003301527467556298 : Loss for Batch-> 177\n",
            "0.0007007452077232301 : Loss for Batch-> 178\n",
            "0.015794888138771057 : Loss for Batch-> 179\n",
            "0.0164173636585474 : Loss for Batch-> 180\n",
            "0.0004253812076058239 : Loss for Batch-> 181\n",
            "0.0697443038225174 : Val Loss after training 181 batche(s)\n",
            "0.00044630561023950577 : Loss for Batch-> 182\n",
            "0.0001432023709639907 : Loss for Batch-> 183\n",
            "9.281713573727757e-06 : Loss for Batch-> 184\n",
            "3.304429992567748e-05 : Loss for Batch-> 185\n",
            "0.0006486502825282514 : Loss for Batch-> 186\n",
            "0.00015551237447652966 : Loss for Batch-> 187\n",
            "0.0007660469273105264 : Loss for Batch-> 188\n",
            "0.0003135707229375839 : Loss for Batch-> 189\n",
            "0.0004037028120364994 : Loss for Batch-> 190\n",
            "0.0003405536408536136 : Loss for Batch-> 191\n",
            "0.08542802929878235 : Val Loss after training 191 batche(s)\n",
            "0.0014914155472069979 : Loss for Batch-> 192\n",
            "0.013651846908032894 : Loss for Batch-> 193\n",
            "0.02772883139550686 : Loss for Batch-> 194\n",
            "0.007526874542236328 : Loss for Batch-> 195\n",
            "0.003899727715179324 : Loss for Batch-> 196\n",
            "0.0008153122034855187 : Loss for Batch-> 197\n",
            "0.0001874071022029966 : Loss for Batch-> 198\n",
            "0.00018906737386714667 : Loss for Batch-> 199\n",
            "0.2508571147918701 : Loss for Batch-> 200\n",
            "2.8750250339508057 : Loss for Batch-> 201\n",
            "0.018473805859684944 : Val Loss after training 201 batche(s)\n",
            "0.035432275384664536 : Loss for Batch-> 202\n",
            "0.011034157127141953 : Loss for Batch-> 203\n",
            "0.07650833576917648 : Loss for Batch-> 204\n",
            "0.03285229578614235 : Loss for Batch-> 205\n",
            "0.013866492547094822 : Loss for Batch-> 206\n",
            "0.024556085467338562 : Loss for Batch-> 207\n",
            "0.00995632167905569 : Loss for Batch-> 208\n",
            "0.007126391399651766 : Loss for Batch-> 209\n",
            "0.005752437748014927 : Loss for Batch-> 210\n",
            "0.00650535523891449 : Loss for Batch-> 211\n",
            "0.04171404987573624 : Val Loss after training 211 batche(s)\n",
            "0.0311236884444952 : Loss for Batch-> 212\n",
            "0.019680902361869812 : Loss for Batch-> 213\n",
            "0.00683706346899271 : Loss for Batch-> 214\n",
            "0.03345971181988716 : Loss for Batch-> 215\n",
            "0.02275344915688038 : Loss for Batch-> 216\n",
            "0.03592166677117348 : Loss for Batch-> 217\n",
            "0.021195752546191216 : Loss for Batch-> 218\n",
            "0.0006288739969022572 : Loss for Batch-> 219\n",
            "0.0005075244116596878 : Loss for Batch-> 220\n",
            "0.014953839592635632 : Loss for Batch-> 221\n",
            "0.11906197667121887 : Val Loss after training 221 batche(s)\n",
            "0.07459685206413269 : Loss for Batch-> 222\n",
            "0.0254212636500597 : Loss for Batch-> 223\n",
            "0.039245523512363434 : Loss for Batch-> 224\n",
            "0.14116279780864716 : Loss for Batch-> 225\n",
            "0.0008036222425289452 : Loss for Batch-> 226\n",
            "0.0004092463059350848 : Loss for Batch-> 227\n",
            "0.0004145167476963252 : Loss for Batch-> 228\n",
            "0.0017895351629704237 : Loss for Batch-> 229\n",
            "2.186399221420288 : Loss for Batch-> 230\n",
            "0.46710529923439026 : Loss for Batch-> 231\n",
            "5.360124588012695 : Val Loss after training 231 batche(s)\n",
            "10.88705062866211 : Loss for Batch-> 232\n",
            "0.46413829922676086 : Loss for Batch-> 233\n",
            "0.26288315653800964 : Loss for Batch-> 234\n",
            "0.06347475945949554 : Loss for Batch-> 235\n",
            "0.0006358586251735687 : Loss for Batch-> 236\n",
            "0.05662403628230095 : Loss for Batch-> 237\n",
            "0.012920009903609753 : Loss for Batch-> 238\n",
            "0.0016153522301465273 : Loss for Batch-> 239\n",
            "0.0009740230161696672 : Loss for Batch-> 240\n",
            "0.025815889239311218 : Loss for Batch-> 241\n",
            "0.032300401479005814 : Val Loss after training 241 batche(s)\n",
            "0.10391579568386078 : Loss for Batch-> 242\n",
            "0.10309051722288132 : Loss for Batch-> 243\n",
            "0.13355307281017303 : Loss for Batch-> 244\n",
            "0.6583959460258484 : Loss for Batch-> 245\n",
            "1.0087521076202393 : Loss for Batch-> 246\n",
            "4.117212295532227 : Loss for Batch-> 247\n",
            "0.015568902716040611 : Loss for Batch-> 248\n",
            "0.03671928495168686 : Loss for Batch-> 249\n",
            "4.406689167022705 : Loss for Batch-> 250\n",
            "0.07399997115135193 : Loss for Batch-> 251\n",
            "0.07645577192306519 : Val Loss after training 251 batche(s)\n",
            "1.1556026935577393 : Loss for Batch-> 252\n",
            "0.015929730609059334 : Loss for Batch-> 253\n",
            "0.22137323021888733 : Loss for Batch-> 254\n",
            "0.42070692777633667 : Loss for Batch-> 255\n",
            "0.49578702449798584 : Loss for Batch-> 256\n",
            "2.269778251647949 : Loss for Batch-> 257\n",
            "6.98264217376709 : Loss for Batch-> 258\n",
            "0.3147340714931488 : Loss for Batch-> 259\n",
            "0.004252465441823006 : Loss for Batch-> 260\n",
            "0.6290202140808105 : Loss for Batch-> 261\n",
            "0.001264940481632948 : Val Loss after training 261 batche(s)\n",
            "0.22985893487930298 : Loss for Batch-> 262\n",
            "4.659860134124756 : Loss for Batch-> 263\n",
            "1.6059309244155884 : Loss for Batch-> 264\n",
            "0.03346291184425354 : Loss for Batch-> 265\n",
            "0.021514546126127243 : Loss for Batch-> 266\n",
            "1.4615343809127808 : Loss for Batch-> 267\n",
            "0.6596336364746094 : Loss for Batch-> 268\n",
            "0.13379450142383575 : Loss for Batch-> 269\n",
            "0.0008150461362674832 : Loss for Batch-> 270\n",
            "0.34310340881347656 : Loss for Batch-> 271\n",
            "5.983361916150898e-05 : Val Loss after training 271 batche(s)\n",
            "5.55999755859375 : Loss for Batch-> 272\n",
            "0.09629091620445251 : Loss for Batch-> 273\n",
            "0.0013433153508231044 : Loss for Batch-> 274\n",
            "0.000732719898223877 : Loss for Batch-> 275\n",
            "0.0018090010853484273 : Loss for Batch-> 276\n",
            "0.0022715262603014708 : Loss for Batch-> 277\n",
            "0.0010839304886758327 : Loss for Batch-> 278\n",
            "0.002893078373745084 : Loss for Batch-> 279\n",
            "0.0008480799151584506 : Loss for Batch-> 280\n",
            "0.01949671283364296 : Loss for Batch-> 281\n",
            "0.003184687811881304 : Val Loss after training 281 batche(s)\n",
            "0.010844486765563488 : Loss for Batch-> 282\n",
            "0.05428497493267059 : Loss for Batch-> 283\n",
            "0.06879890710115433 : Loss for Batch-> 284\n",
            "0.06933366507291794 : Loss for Batch-> 285\n",
            "0.058066967874765396 : Loss for Batch-> 286\n",
            "0.006500080693513155 : Loss for Batch-> 287\n",
            "3.8753890991210938 : Loss for Batch-> 288\n",
            "10.501300811767578 : Loss for Batch-> 289\n",
            "0.010497041046619415 : Loss for Batch-> 290\n",
            "0.0077004460617899895 : Loss for Batch-> 291\n",
            "0.0006499998271465302 : Val Loss after training 291 batche(s)\n",
            "0.0030056408140808344 : Loss for Batch-> 292\n",
            "0.13260042667388916 : Loss for Batch-> 293\n",
            "0.04691871255636215 : Loss for Batch-> 294\n",
            "0.004058002959936857 : Loss for Batch-> 295\n",
            "0.0013557961210608482 : Loss for Batch-> 296\n",
            "0.0012491546804085374 : Loss for Batch-> 297\n",
            "0.05343933030962944 : Loss for Batch-> 298\n",
            "0.018522752448916435 : Loss for Batch-> 299\n",
            "0.005214937496930361 : Loss for Batch-> 300\n",
            "0.03781971335411072 : Loss for Batch-> 301\n",
            "0.001081556547433138 : Val Loss after training 301 batche(s)\n",
            "0.26682528853416443 : Loss for Batch-> 302\n",
            "1.1845660209655762 : Loss for Batch-> 303\n",
            "1.0641601085662842 : Loss for Batch-> 304\n",
            "0.17115409672260284 : Loss for Batch-> 305\n",
            "0.09060198813676834 : Loss for Batch-> 306\n",
            "0.1791841685771942 : Loss for Batch-> 307\n",
            "1.912459135055542 : Loss for Batch-> 308\n",
            "1.700123906135559 : Loss for Batch-> 309\n",
            "0.184982031583786 : Loss for Batch-> 310\n",
            "0.05902731046080589 : Loss for Batch-> 311\n",
            "0.002552411053329706 : Val Loss after training 311 batche(s)\n",
            "0.02324853092432022 : Loss for Batch-> 312\n",
            "1.0677680969238281 : Loss for Batch-> 313\n",
            "1.4505902528762817 : Loss for Batch-> 314\n",
            "0.47441405057907104 : Loss for Batch-> 315\n",
            "0.020859284326434135 : Loss for Batch-> 316\n",
            "0.0021966248750686646 : Loss for Batch-> 317\n",
            "0.38808077573776245 : Loss for Batch-> 318\n",
            "0.24998821318149567 : Loss for Batch-> 319\n",
            "0.019385620951652527 : Loss for Batch-> 320\n",
            "0.028651228174567223 : Loss for Batch-> 321\n",
            "0.02308972366154194 : Val Loss after training 321 batche(s)\n",
            "0.21469922363758087 : Loss for Batch-> 322\n",
            "0.12118349224328995 : Loss for Batch-> 323\n",
            "0.2996349632740021 : Loss for Batch-> 324\n",
            "0.20243987441062927 : Loss for Batch-> 325\n",
            "0.04111459106206894 : Loss for Batch-> 326\n",
            "0.16143307089805603 : Loss for Batch-> 327\n",
            "0.24729634821414948 : Loss for Batch-> 328\n",
            "0.482230007648468 : Loss for Batch-> 329\n",
            "0.4616831839084625 : Loss for Batch-> 330\n",
            "0.06668664515018463 : Loss for Batch-> 331\n",
            "0.00017007923452183604 : Val Loss after training 331 batche(s)\n",
            "0.09137003868818283 : Loss for Batch-> 332\n",
            "0.08259405940771103 : Loss for Batch-> 333\n",
            "0.014717546291649342 : Loss for Batch-> 334\n",
            "0.006500884424895048 : Loss for Batch-> 335\n",
            "0.007090847939252853 : Loss for Batch-> 336\n",
            "0.0003976149018853903 : Loss for Batch-> 337\n",
            "0.0006002169684506953 : Loss for Batch-> 338\n",
            "0.0018904154421761632 : Loss for Batch-> 339\n",
            "0.002032617572695017 : Loss for Batch-> 340\n",
            "0.034105800092220306 : Loss for Batch-> 341\n",
            "0.0029363895300775766 : Val Loss after training 341 batche(s)\n",
            "0.054345596581697464 : Loss for Batch-> 342\n",
            "0.06188264489173889 : Loss for Batch-> 343\n",
            "0.03482744097709656 : Loss for Batch-> 344\n",
            "0.37366974353790283 : Loss for Batch-> 345\n",
            "0.2122795283794403 : Loss for Batch-> 346\n",
            "0.013315040618181229 : Loss for Batch-> 347\n",
            "0.029770517721772194 : Loss for Batch-> 348\n",
            "0.20219434797763824 : Loss for Batch-> 349\n",
            "0.45248642563819885 : Loss for Batch-> 350\n",
            "0.09772081673145294 : Loss for Batch-> 351\n",
            "0.0016752112424001098 : Val Loss after training 351 batche(s)\n",
            "0.0028935917653143406 : Loss for Batch-> 352\n",
            "0.703061580657959 : Loss for Batch-> 353\n",
            "0.7594211101531982 : Loss for Batch-> 354\n",
            "0.049119919538497925 : Loss for Batch-> 355\n",
            "0.03111155331134796 : Loss for Batch-> 356\n",
            "1.366528034210205 : Loss for Batch-> 357\n",
            "0.5430078506469727 : Loss for Batch-> 358\n",
            "0.008263976313173771 : Loss for Batch-> 359\n",
            "0.005554405506700277 : Loss for Batch-> 360\n",
            "0.018241383135318756 : Loss for Batch-> 361\n",
            "0.05101064592599869 : Val Loss after training 361 batche(s)\n",
            "0.5466639995574951 : Loss for Batch-> 362\n",
            "0.038180895149707794 : Loss for Batch-> 363\n",
            "0.23047605156898499 : Loss for Batch-> 364\n",
            "0.020182549953460693 : Loss for Batch-> 365\n",
            "0.13813388347625732 : Loss for Batch-> 366\n",
            "0.383281946182251 : Loss for Batch-> 367\n",
            "0.10849414765834808 : Loss for Batch-> 368\n",
            "0.2058737874031067 : Loss for Batch-> 369\n",
            "0.19221818447113037 : Loss for Batch-> 370\n",
            "0.008265362121164799 : Loss for Batch-> 371\n",
            "0.023174097761511803 : Val Loss after training 371 batche(s)\n",
            "0.024386731907725334 : Loss for Batch-> 372\n",
            "0.054607175290584564 : Loss for Batch-> 373\n",
            "0.33335766196250916 : Loss for Batch-> 374\n",
            "0.6528010368347168 : Loss for Batch-> 375\n",
            "0.11594735085964203 : Loss for Batch-> 376\n",
            "1.024746298789978 : Loss for Batch-> 377\n",
            "0.8589847087860107 : Loss for Batch-> 378\n",
            "0.3014494776725769 : Loss for Batch-> 379\n",
            "0.40648844838142395 : Loss for Batch-> 380\n",
            "0.030454689636826515 : Loss for Batch-> 381\n",
            "0.0011361503275111318 : Val Loss after training 381 batche(s)\n",
            "0.02882838435471058 : Loss for Batch-> 382\n",
            "0.06932682543992996 : Loss for Batch-> 383\n",
            "0.18396668136119843 : Loss for Batch-> 384\n",
            "0.6465920805931091 : Loss for Batch-> 385\n",
            "0.2707236707210541 : Loss for Batch-> 386\n",
            "2.1207005977630615 : Loss for Batch-> 387\n",
            "0.5674248337745667 : Loss for Batch-> 388\n",
            "0.908423900604248 : Loss for Batch-> 389\n",
            "0.07930788397789001 : Loss for Batch-> 390\n",
            "0.005671000573784113 : Loss for Batch-> 391\n",
            "0.0002504714939277619 : Val Loss after training 391 batche(s)\n",
            "0.19343340396881104 : Loss for Batch-> 392\n",
            "1.2138910293579102 : Loss for Batch-> 393\n",
            "1.245751976966858 : Loss for Batch-> 394\n",
            "0.07516798377037048 : Loss for Batch-> 395\n",
            "0.1087866872549057 : Loss for Batch-> 396\n",
            "0.016924722120165825 : Loss for Batch-> 397\n",
            "0.0926586240530014 : Loss for Batch-> 398\n",
            "0.3811374604701996 : Loss for Batch-> 399\n",
            "0.27445700764656067 : Loss for Batch-> 400\n",
            "1.0959818363189697 : Loss for Batch-> 401\n",
            "0.10785529017448425 : Val Loss after training 401 batche(s)\n",
            "0.26360130310058594 : Loss for Batch-> 402\n",
            "0.00785338506102562 : Loss for Batch-> 403\n",
            "0.00798970740288496 : Loss for Batch-> 404\n",
            "0.017092997208237648 : Loss for Batch-> 405\n",
            "0.021855976432561874 : Loss for Batch-> 406\n",
            "0.526365339756012 : Loss for Batch-> 407\n",
            "0.4924905300140381 : Loss for Batch-> 408\n",
            "0.17319375276565552 : Loss for Batch-> 409\n",
            "0.047858260571956635 : Loss for Batch-> 410\n",
            "0.030843017622828484 : Loss for Batch-> 411\n",
            "0.2593051791191101 : Val Loss after training 411 batche(s)\n",
            "0.01522279903292656 : Loss for Batch-> 412\n",
            "0.15776847302913666 : Loss for Batch-> 413\n",
            "1.1423996686935425 : Loss for Batch-> 414\n",
            "0.8070526123046875 : Loss for Batch-> 415\n",
            "0.07563851028680801 : Loss for Batch-> 416\n",
            "0.07144477963447571 : Loss for Batch-> 417\n",
            "0.000841445114929229 : Loss for Batch-> 418\n",
            "0.0016726432368159294 : Loss for Batch-> 419\n",
            "0.006575964856892824 : Loss for Batch-> 420\n",
            "0.026752090081572533 : Loss for Batch-> 421\n",
            "0.09701075404882431 : Val Loss after training 421 batche(s)\n",
            "0.018763352185487747 : Loss for Batch-> 422\n",
            "0.05311986804008484 : Loss for Batch-> 423\n",
            "0.17372673749923706 : Loss for Batch-> 424\n",
            "0.05182366818189621 : Loss for Batch-> 425\n",
            "0.022629447281360626 : Loss for Batch-> 426\n",
            "0.022173061966896057 : Loss for Batch-> 427\n",
            "0.022135518491268158 : Loss for Batch-> 428\n",
            "0.022557631134986877 : Loss for Batch-> 429\n",
            "0.02385694719851017 : Loss for Batch-> 430\n",
            "0.02406196855008602 : Loss for Batch-> 431\n",
            "0.0019475228618830442 : Val Loss after training 431 batche(s)\n",
            "0.023325538262724876 : Loss for Batch-> 432\n",
            "0.02292574942111969 : Loss for Batch-> 433\n",
            "0.021962806582450867 : Loss for Batch-> 434\n",
            "0.1377938836812973 : Loss for Batch-> 435\n",
            "0.04164900630712509 : Loss for Batch-> 436\n",
            "0.008677089586853981 : Loss for Batch-> 437\n",
            "0.0030928479973226786 : Loss for Batch-> 438\n",
            "0.0013657595263794065 : Loss for Batch-> 439\n",
            "0.0005528039764612913 : Loss for Batch-> 440\n",
            "0.0025264485739171505 : Loss for Batch-> 441\n",
            "0.005332074593752623 : Val Loss after training 441 batche(s)\n",
            "0.004536563530564308 : Loss for Batch-> 442\n",
            "0.06045400723814964 : Loss for Batch-> 443\n",
            "0.05011468753218651 : Loss for Batch-> 444\n",
            "0.008183821104466915 : Loss for Batch-> 445\n",
            "0.033757053315639496 : Loss for Batch-> 446\n",
            "0.0074117593467235565 : Loss for Batch-> 447\n",
            "0.04539068043231964 : Loss for Batch-> 448\n",
            "0.006573594640940428 : Loss for Batch-> 449\n",
            "0.32219842076301575 : Loss for Batch-> 450\n",
            "0.42785170674324036 : Loss for Batch-> 451\n",
            "0.004621265921741724 : Val Loss after training 451 batche(s)\n",
            "0.3252876400947571 : Loss for Batch-> 452\n",
            "0.0006675620097666979 : Loss for Batch-> 453\n",
            "0.0013295543612912297 : Loss for Batch-> 454\n",
            "0.00045916615636087954 : Loss for Batch-> 455\n",
            "0.0011090260231867433 : Loss for Batch-> 456\n",
            "0.00093701237346977 : Loss for Batch-> 457\n",
            "0.00034988217521458864 : Loss for Batch-> 458\n",
            "0.0010118589270859957 : Loss for Batch-> 459\n",
            "0.0001256485265912488 : Loss for Batch-> 460\n",
            "0.00012542320473585278 : Loss for Batch-> 461\n",
            "0.004871078301221132 : Val Loss after training 461 batche(s)\n",
            "0.00012402338325046003 : Loss for Batch-> 462\n",
            "0.00012143121421104297 : Loss for Batch-> 463\n",
            "0.00014832902525085956 : Loss for Batch-> 464\n",
            "0.00017596133693587035 : Loss for Batch-> 465\n",
            "0.00014643705799244344 : Loss for Batch-> 466\n",
            "0.00014541253040079027 : Loss for Batch-> 467\n",
            "0.0001462385116610676 : Loss for Batch-> 468\n",
            "0.0012607856187969446 : Loss for Batch-> 469\n",
            "0.0010751588270068169 : Loss for Batch-> 470\n",
            "0.0005178760038688779 : Loss for Batch-> 471\n",
            "0.004872270859777927 : Val Loss after training 471 batche(s)\n",
            "0.0012262878008186817 : Loss for Batch-> 472\n",
            "0.00022611732129007578 : Loss for Batch-> 473\n",
            "0.00045180448796600103 : Loss for Batch-> 474\n",
            "0.00039094703970476985 : Loss for Batch-> 475\n",
            "0.0008130520000122488 : Loss for Batch-> 476\n",
            "0.0005819792277179658 : Loss for Batch-> 477\n",
            "0.0011459480738267303 : Loss for Batch-> 478\n",
            "0.0005054578650742769 : Loss for Batch-> 479\n",
            "0.0012146803783252835 : Loss for Batch-> 480\n",
            "0.00046632715384475887 : Loss for Batch-> 481\n",
            "0.07815501093864441 : Val Loss after training 481 batche(s)\n",
            "0.0005732359713874757 : Loss for Batch-> 482\n",
            "0.0008520713308826089 : Loss for Batch-> 483\n",
            "0.0014142153086140752 : Loss for Batch-> 484\n",
            "0.02869732305407524 : Loss for Batch-> 485\n",
            "0.0046782284043729305 : Loss for Batch-> 486\n",
            "0.0031056092120707035 : Loss for Batch-> 487\n",
            "0.0033916179090738297 : Loss for Batch-> 488\n",
            "0.0035906073171645403 : Loss for Batch-> 489\n",
            "0.00351815321482718 : Loss for Batch-> 490\n",
            "0.0034117356408387423 : Loss for Batch-> 491\n",
            "3.0769829750061035 : Val Loss after training 491 batche(s)\n",
            "0.0035071494057774544 : Loss for Batch-> 492\n",
            "0.0034539494663476944 : Loss for Batch-> 493\n",
            "0.0020347756799310446 : Loss for Batch-> 494\n",
            "0.002058172132819891 : Loss for Batch-> 495\n",
            "0.0019988506101071835 : Loss for Batch-> 496\n",
            "0.18549884855747223 : Loss for Batch-> 497\n",
            "1.5763531923294067 : Loss for Batch-> 498\n",
            "0.003768975380808115 : Loss for Batch-> 499\n",
            "0.020718509331345558 : Loss for Batch-> 500\n",
            "0.0011679977178573608 : Loss for Batch-> 501\n",
            "0.0005760463536716998 : Val Loss after training 501 batche(s)\n",
            "0.0009832564974203706 : Loss for Batch-> 502\n",
            "0.0012938390718773007 : Loss for Batch-> 503\n",
            "0.002318080747500062 : Loss for Batch-> 504\n",
            "0.003986137919127941 : Loss for Batch-> 505\n",
            "0.03623345494270325 : Loss for Batch-> 506\n",
            "0.020313847810029984 : Loss for Batch-> 507\n",
            "0.013207140378654003 : Loss for Batch-> 508\n",
            "0.020235667005181313 : Loss for Batch-> 509\n",
            "0.01957930624485016 : Loss for Batch-> 510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"saved_model.h5\")"
      ],
      "metadata": {
        "id": "p6cFdh25k4Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_x,val_y = LoadValBatch(batch_size)\n",
        "val_x = np.array(val_x)\n",
        "val_y = np.array(val_y)"
      ],
      "metadata": {
        "id": "4I72DV5VfTFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(val_x)"
      ],
      "metadata": {
        "id": "83oTmE9KfBwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.array(y_pred)"
      ],
      "metadata": {
        "id": "X2SuLO1NffrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DGBgeyUfoO_",
        "outputId": "1be271d2-d4e9-4e31-ea11-5c53817b9573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = y_pred.reshape((1,100))"
      ],
      "metadata": {
        "id": "vZwBjRtufpo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_y = val_y.reshape((1,100))"
      ],
      "metadata": {
        "id": "RC9Rchr5frlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_eE-J20f_GK",
        "outputId": "f11332ca-d727-451f-9601-d56e52ca9c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504,\n",
              "        0.01506504, 0.01506504, 0.01506504, 0.01506504, 0.01506504]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djgoI0WhfxX",
        "outputId": "acfd9f25-971f-40fb-ca8e-fa49c816f6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00873016,  0.01065079,  0.01065079,  0.01065079,  0.01239683,\n",
              "         0.01239683,  0.01239683,  0.01239683,  0.01239683,  0.01239683,\n",
              "         0.01239683,  0.01239683,  0.01239683,  0.01239683,  0.01239683,\n",
              "         0.01239683,  0.01239683,  0.01414286,  0.01763492,  0.02461905,\n",
              "         0.03526984,  0.04225397,  0.04574603,  0.04574603,  0.        ,\n",
              "         0.0387619 ,  0.0387619 ,  0.0387619 ,  0.0387619 ,  0.0387619 ,\n",
              "         0.04050794,  0.044     ,  0.04574603,  0.04574603,  0.044     ,\n",
              "         0.04225397,  0.0387619 ,  0.03352381,  0.03177778,  0.02636508,\n",
              "         0.02112698,  0.01414286,  0.01239683,  0.00873016,  0.00698413,\n",
              "         0.00698413,  0.00349206,  0.00349206,  0.00174603,  0.        ,\n",
              "        -0.00698413, -0.01414286, -0.02112698, -0.02985714, -0.0387619 ,\n",
              "        -0.04050794, -0.04050794, -0.04050794, -0.04050794, -0.04050794,\n",
              "        -0.04050794, -0.04050794, -0.04050794, -0.04050794, -0.04050794,\n",
              "        -0.04050794, -0.04050794, -0.04050794, -0.04050794, -0.04050794,\n",
              "        -0.04050794, -0.0387619 , -0.02112698, -0.01414286, -0.00349206,\n",
              "         0.        ,  0.01065079,  0.01414286,  0.01938095,  0.02636508,\n",
              "         0.03352381,  0.        ,  0.0387619 ,  0.0387619 ,  0.0387619 ,\n",
              "         0.03701587,  0.03352381,  0.02985714,  0.02811111,  0.02636508,\n",
              "         0.02636508,  0.02287302,  0.01938095,  0.01763492,  0.01414286,\n",
              "         0.00873016,  0.0052381 ,  0.        , -0.00174603, -0.0052381 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}